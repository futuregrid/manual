<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>USER MANUAL &mdash; Cloud Computing Book 0.1 documentation</title>
    
    <link rel="stylesheet" href="_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/bootstrap-2.3.1/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootswatch/2.3.1/united/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="_static/bootstrap-2.3.1/css/bootstrap-responsive.min.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="_static/js/jquery-1.9.1.min.js"></script>
    <script type="text/javascript" src="_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="_static/bootstrap-2.3.1/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="_static/bootstrap-sphinx.js"></script>
    <link rel="top" title="Cloud Computing Book 0.1 documentation" href="index.html" />
    <link rel="next" title="Part 2 - OTHER STUFF" href="part-2.html" />
    <link rel="prev" title="Preface" href="preface.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body>

  <div id="navbar" class="navbar navbar-inverse navbar-fixed-top">
    <div class="navbar-inner">
      <div class="container">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

        <a class="brand" href="index.html">Contents</a>
        <span class="navbar-text pull-left"><b>0.1</b></span>

        <div class="nav-collapse">
          <ul class="nav">
            <li class="divider-vertical"></li>
            
              <li class="dropdown globaltoc-container">
  <a href="index.html"
     class="dropdown-toggle"
     data-toggle="dropdown">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
    ><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="part-1-fg.html">Part 1 - FutureGrid Manual</a></li>
<li class="toctree-l1"><a class="reference internal" href="preface.html">Preface</a><ul>
<li class="toctree-l2"><a class="reference internal" href="preface.html#citation">Citation</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface.html#acknowledgement">Acknowledgement</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface.html#about-this-manual">About this Manual</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="">USER MANUAL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#futuregrid-user-manual">FutureGrid User Manual</a></li>
<li class="toctree-l2"><a class="reference internal" href="#preface">Preface</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sponsors">Sponsors</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Citation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id2">Sponsors</a></li>
<li class="toctree-l2"><a class="reference internal" href="#about">About</a></li>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#getting-started">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="#accessing-futuregrid">Accessing FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#account-creation">Account Creation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#help-and-support">Help and Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="#guide-to-using-the-futuregrid-portal">Guide to Using the FutureGrid Portal</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id5">Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id6">Compute Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="#alamo">Alamo</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bravo">Bravo</a></li>
<li class="toctree-l2"><a class="reference internal" href="#delta">Delta</a></li>
<li class="toctree-l2"><a class="reference internal" href="#foxtrot">Foxtrot</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hotel">Hotel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#india">India</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sierra">Sierra</a></li>
<li class="toctree-l2"><a class="reference internal" href="#xray">Xray</a></li>
<li class="toctree-l2"><a class="reference internal" href="#network">Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="#futuregrid-network-impairments-device-nid">FutureGrid Network Impairments Device (NID)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id7">Storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-hpss-from-futuregrid">Using HPSS from FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id12">Status</a></li>
<li class="toctree-l2"><a class="reference internal" href="#accessing-futuregrid-resources-via-ssh">Accessing FutureGrid resources via SSH</a></li>
<li class="toctree-l2"><a class="reference internal" href="#requirement-for-windows-users">Requirement for Windows Users</a></li>
<li class="toctree-l2"><a class="reference internal" href="#instructions-for-both-windows-and-unix-users">Instructions for both Windows and Unix users</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hpc-services">HPC Services</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-hpc-services-on-futuregrid">Using HPC Services on FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#log-in-to-hpc-services">Log in to HPC services</a></li>
<li class="toctree-l2"><a class="reference internal" href="#generating-ssh-keys-for-futuregrid-access">Generating SSH Keys for FutureGrid Access</a></li>
<li class="toctree-l2"><a class="reference internal" href="#key-generation">Key Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#working-with-hpc-job-services">Working with HPC Job Services</a></li>
<li class="toctree-l2"><a class="reference internal" href="#analyzing-code-performance">Analyzing Code Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#papi">PAPI</a></li>
<li class="toctree-l2"><a class="reference internal" href="#vampir">Vampir</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id13"><strong>Getting Started</strong></a></li>
<li class="toctree-l2"><a class="reference internal" href="#basics"><strong>Basics</strong></a></li>
<li class="toctree-l2"><a class="reference internal" href="#performance-data-visualization"><strong>Performance Data Visualization</strong></a></li>
<li class="toctree-l2"><a class="reference internal" href="#customization"><strong>Customization</strong></a></li>
<li class="toctree-l2"><a class="reference internal" href="#footnotes"><strong>Footnotes</strong></a></li>
<li class="toctree-l2"><a class="reference internal" href="#vampirtrace">VampirTrace</a></li>
<li class="toctree-l2"><a class="reference internal" href="#official-hpcc-results-from-the-acceptance-tests">Official HPCC Results from the Acceptance Tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hpcc-results">HPCC Results</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hpcc-configuration">HPCC Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id29">Foxtrot</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id32">India</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id33">Sierra</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id34">Xray</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id35">Alamo</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id36">Delta</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-mpi-gpu-program-on-the-delta-cluster">Running MPI/GPU program on the Delta cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-programs-on-a-single-gpu">Running programs on a single GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="#c-means-clustering-using-cuda-on-gpu">C-means clustering using CUDA on GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="#scalemp-vsmp">ScaleMP vSMP</a></li>
<li class="toctree-l2"><a class="reference internal" href="#accessing-scalemp">Accessing ScaleMP:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#developing-a-job-script">Developing a job script:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#iaas-infrastructure-as-a-service">IaaS - Infrastructure as a Service</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-iaas-clouds-on-futuregrid">Using IaaS Clouds on FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-nimbus-on-futuregrid">Using Nimbus on FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cloud-quick-start-launch-a-vm-with-1-command">Cloud Quick Start : Launch a VM with 1 command</a></li>
<li class="toctree-l2"><a class="reference internal" href="#launch-a-vm-via-nimbus">Launch A VM via Nimbus</a></li>
<li class="toctree-l2"><a class="reference internal" href="#options">Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-happens">What Happens</a></li>
<li class="toctree-l2"><a class="reference internal" href="#futuregrid-tutorial-nm2-nimbus-one-click-cluster-guide">FutureGrid Tutorial NM2 - Nimbus One-Click Cluster Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-eucalyptus-on-futuregrid">Using Eucalyptus on FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-openstack-on-futuregrid">Using OpenStack on FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#note">NOTE:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id41">Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id42">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="#log-into-india">Log into India</a></li>
<li class="toctree-l2"><a class="reference internal" href="#account-and-credentials">Account and Credentials</a></li>
<li class="toctree-l2"><a class="reference internal" href="#euca2ools-ec2-client-tools">Euca2ools (EC2 client tools)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id43">Testing Your Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#list-of-common-images">List of Common Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="#vm-types">VM Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id44">Key Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="#image-instantiation">Image Instantiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#better-server-names">Better Server Names</a></li>
<li class="toctree-l2"><a class="reference internal" href="#monitoring-instances">Monitoring Instances</a></li>
<li class="toctree-l2"><a class="reference internal" href="#log-into-your-vm">Log into your VM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#nova-volumes-not-available">Nova Volumes (Not available)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#volume-snapshots">Volume Snapshots</a></li>
<li class="toctree-l2"><a class="reference internal" href="#image-registration">Image Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#delete-your-images">Delete your images</a></li>
<li class="toctree-l2"><a class="reference internal" href="#terminate-your-vms">Terminate your VMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#limitations">Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#troubleshooting">Troubleshooting:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#image-management-and-rain-on-futuregrid">Image Management and Rain on FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#generate-and-register-an-os-image-on-futuregrid-using-the-fg-shell">Generate and Register an OS Image on FutureGrid using the FG Shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="#futuregrid-standalone-image-repository">FutureGrid Standalone Image Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="#manual-image-customization">Manual Image Customization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rain-manual-pages">RAIN Manual Pages</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fg-repo">fg-repo</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fg-rain">fg-rain</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fg-generate">fg-generate</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fg-register">fg-register</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fg-shell">fg-shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fg-portal-manage">fg-portal-manage</a></li>
<li class="toctree-l2"><a class="reference internal" href="#vine">ViNe</a></li>
<li class="toctree-l2"><a class="reference internal" href="#opennebula-2-0-tutorial">OpenNebula 2.0 Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="#create-a-vm-image-contextualized-for-opennebula">Create a VM Image Contextualized for OpenNebula</a></li>
<li class="toctree-l2"><a class="reference internal" href="#managing-virtual-machines">Managing Virtual Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="#managing-the-image-repository">Managing the Image Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="#managing-physical-hosts-and-clusters">Managing Physical Hosts and Clusters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#managing-virtual-networks">Managing Virtual Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#accounting">Accounting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#user-management">User Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="#paas-platform-as-a-service">PaaS - Platform as a Service</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-map-reduce-in-futuregrid">Using Map/Reduce in FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-hadoop-as-a-batch-job-using-myhadoop">Running Hadoop as a Batch Job using MyHadoop</a></li>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hadoop-on-futuregrid">Hadoop on FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-is-myhadoop">What is myHadoop?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-myhadoop-on-futuregrid">Running myHadoop on FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#persistent-mode">Persistent Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="#customizing-hadoop-settings">Customizing Hadoop&nbsp;Settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-a-different-installation-of-hadoop">Using a Different Installation of Hadoop</a></li>
<li class="toctree-l2"><a class="reference internal" href="#more-information">More Information</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-salsahadoop-on-futuregrid">Using SalsaHadoop on FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hadoop-blast">Hadoop Blast</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hadoop-wordcount">Hadoop WordCount</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-twister-on-futuregrid">Using Twister on FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#twister-blast">Twister Blast</a></li>
<li class="toctree-l2"><a class="reference internal" href="#eucalyptus-and-twister-on-futuregrid">Eucalyptus and Twister on FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-futuregrid-twister-tutorial">The FutureGrid Twister Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-pegasus-on-futuregrid">Using Pegasus on FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#management-services">Management Services</a></li>
<li class="toctree-l2"><a class="reference internal" href="#nimbus-phantom">Nimbus Phantom</a></li>
<li class="toctree-l2"><a class="reference internal" href="#precip-pegasus-repeatable-experiments-for-the-cloud-in-python">Precip - Pegasus Repeatable Experiments for the Cloud in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cloudinit-d">cloudinit.d</a></li>
<li class="toctree-l2"><a class="reference internal" href="#grid-services">Grid Services</a></li>
<li class="toctree-l2"><a class="reference internal" href="#unicore">Unicore</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id69">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-is-unicore">What is UNICORE?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#connecting-to-the-unicore-bes-endpoints-from-other-grid-middleware-clients">Connecting to the UNICORE BES Endpoints From Other Grid Middleware Clients</a></li>
<li class="toctree-l2"><a class="reference internal" href="#connecting-to-the-unicore-bes-endpoints-using-a-unicore-commandline-client">Connecting to the UNICORE BES Endpoints Using a UNICORE Commandline Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-jobs-on-unicore-sites">Running Jobs on UNICORE Sites</a></li>
<li class="toctree-l2"><a class="reference internal" href="#deploying-a-new-unicore-6-grid">Deploying a New UNICORE 6 Grid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#genesis-ii">Genesis II</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-is-genesisii">What is GenesisII</a></li>
<li class="toctree-l2"><a class="reference internal" href="#connecting-to-the-genesisii-bes-endpoints">Connecting to the GenesisII BES Endpoints</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-the-futuregrid-genesisii-endpoints-as-a-client">Using the Futuregrid GenesisII Endpoints as a Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="#saga-supporting-distributed-applications-on-grids-clouds-on-futuregrid">SAGA supporting Distributed Applications on Grids, Clouds on FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#emi-unicore-tutorial">EMI Unicore Tutorial</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tutorials">Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="#futuregrid-grid-appliance-for-nimbus-and-eucalyptus">FutureGrid Grid Appliance for Nimbus and Eucalyptus</a></li>
<li class="toctree-l2"><a class="reference internal" href="#one-click-hadoop-wordcount-on-eucalyptus-futuregrid">One-click Hadoop WordCount on Eucalyptus FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#one-click-twister-k-means-on-eucalyptus-futuregrid">One-click Twister K-means on Eucalyptus FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="#register-virtual-box-image-on-openstack">Register Virtual Box Image on OpenStack</a></li>
<li class="toctree-l2"><a class="reference internal" href="#virtual-appliances">Virtual Appliances</a></li>
<li class="toctree-l2"><a class="reference internal" href="#development-projects">Development Projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="#appendix">Appendix</a></li>
<li class="toctree-l2"><a class="reference internal" href="#upgrading-nimbus-on-fg-clusters">Upgrading Nimbus On FG clusters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#faq">FAQ</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="part-2.html">Part 2 - OTHER STUFF</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Building the Book</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#virtualenv">Virtualenv</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#theme">Theme</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#autorun">Autorun</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="python.html">Just Enough Python for the Cloud</a><ul>
<li class="toctree-l2"><a class="reference internal" href="python.html#terminal">Terminal</a></li>
<li class="toctree-l2"><a class="reference internal" href="python.html#os-specific">OS Specific</a></li>
<li class="toctree-l2"><a class="reference internal" href="python.html#lanaguage-features">Lanaguage Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="python.html#local">Local</a></li>
<li class="toctree-l2"><a class="reference internal" href="python.html#ecosystem">Ecosystem</a></li>
<li class="toctree-l2"><a class="reference internal" href="python.html#excersises">Excersises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="openstack.html">OpenStack</a><ul>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#get-a-cloud-account">Get a cloud account</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#commandline-clients">commandline clients</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="eucalyptus.html">Eucalyptus on FutureGrid</a><ul>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#summary">Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#account-creation">Account Creation</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#resources-overview">Resources Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#testing-your-setup">Testing Your Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#image-deployment">Image Deployment</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#logging-into-the-vm">Logging Into the VM</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#vm-network-info">VM Network Info</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#image-management">Image Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#status-of-deployments">Status of Deployments</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="india.html">India on  FutureGrid</a></li>
</ul>
</ul>
</li>
              <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"><ul>
<li><a class="reference internal" href="#">USER MANUAL</a><ul>
<li><a class="reference internal" href="#futuregrid-user-manual">FutureGrid User Manual</a></li>
<li><a class="reference internal" href="#preface">Preface</a><ul>
<li><a class="reference internal" href="#citation">Citation</a></li>
<li><a class="reference internal" href="#acknowledgement">Acknowledgement</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sponsors">Sponsors</a></li>
<li><a class="reference internal" href="#id1">Citation</a></li>
<li><a class="reference internal" href="#id2">Sponsors</a></li>
<li><a class="reference internal" href="#about">About</a></li>
<li><a class="reference internal" href="#introduction">Introduction</a><ul>
<li><a class="reference internal" href="#project-account-application">Project &amp; Account Application</a></li>
<li><a class="reference internal" href="#cloud-services">Cloud Services</a></li>
<li><a class="reference internal" href="#high-performance-computing">High Performance Computing</a></li>
<li><a class="reference internal" href="#storage">Storage</a></li>
<li><a class="reference internal" href="#information-services">Information Services</a></li>
<li><a class="reference internal" href="#hardware">Hardware</a></li>
<li><a class="reference internal" href="#support">Support</a></li>
<li><a class="reference internal" href="#id3">Acknowledgement</a></li>
</ul>
</li>
<li><a class="reference internal" href="#getting-started">Getting Started</a><ul>
<li><a class="reference internal" href="#step-1-create-a-futuregrid-portal-account">Step 1: Create a FutureGrid Portal Account</a></li>
<li><a class="reference internal" href="#step-2-create-a-new-project-or-join-an-existing-one">Step 2: Create a new Project or join an existing one</a></li>
<li><a class="reference internal" href="#step-3-upload-your-ssh-public-key-s">Step 3: Upload Your SSH Public Key(s)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#accessing-futuregrid">Accessing FutureGrid</a><ul>
<li><a class="reference internal" href="#account-management-service">Account Management Service</a><ul>
<li><a class="reference internal" href="#apply-for-a-project-or-request-an-account">Apply for a Project or Request an Account</a><ul>
<li><a class="reference internal" href="#implicit-project-responsibilities-for-project-members-and-pi-agreement-for-reporting">Implicit Project Responsibilities for Project Members and PI Agreement for Reporting</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cloud-accounts">Cloud Accounts</a></li>
</ul>
</li>
<li><a class="reference internal" href="#key-reset-or-adding-new-keys">Key Reset or Adding new Keys</a></li>
</ul>
</li>
<li><a class="reference internal" href="#account-creation">Account Creation</a><ul>
<li><a class="reference internal" href="#creating-an-hpc-account">Creating an HPC account</a></li>
<li><a class="reference internal" href="#resetting-a-ssh-key">Resetting a ssh-key</a></li>
<li><a class="reference internal" href="#nimbus-eucalyptus-openstack">Nimbus, Eucalyptus, OpenStack</a></li>
</ul>
</li>
<li><a class="reference internal" href="#help-and-support">Help and Support</a><ul>
<li><a class="reference internal" href="#id4">Help and Support</a></li>
<li><a class="reference internal" href="#manual">Manual</a></li>
<li><a class="reference internal" href="#user-forum">User Forum</a></li>
<li><a class="reference internal" href="#help-ticketing-system">Help Ticketing System</a></li>
<li><a class="reference internal" href="#training-and-education">Training and Education</a></li>
</ul>
</li>
<li><a class="reference internal" href="#guide-to-using-the-futuregrid-portal">Guide to Using the FutureGrid Portal</a><ul>
<li><a class="reference internal" href="#functions-of-the-futuregrid-portal">Functions of the FutureGrid Portal</a></li>
<li><a class="reference internal" href="#a-futuregrid-user-dashboard">A FutureGrid&nbsp;User Dashboard</a></li>
<li><a class="reference internal" href="#update-project-information-and-add-results">Update Project Information and Add Results</a></li>
<li><a class="reference internal" href="#contribute-to-the-futuregrid-community">Contribute to the FutureGrid Community</a></li>
<li><a class="reference internal" href="#file-upload-and-attachment-to-a-page">File Upload and Attachment to a Page</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id5">Hardware</a><ul>
<li><a class="reference internal" href="#compute-resources">Compute Resources</a></li>
<li><a class="reference internal" href="#storage-resources">Storage Resources</a></li>
<li><a class="reference internal" href="#status">Status</a></li>
<li><a class="reference internal" href="#maintenance">Maintenance</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id6">Compute Resources</a><ul>
<li><a class="reference internal" href="#compute-networks">Compute Networks</a></li>
</ul>
</li>
<li><a class="reference internal" href="#alamo">Alamo</a></li>
<li><a class="reference internal" href="#bravo">Bravo</a></li>
<li><a class="reference internal" href="#delta">Delta</a></li>
<li><a class="reference internal" href="#foxtrot">Foxtrot</a></li>
<li><a class="reference internal" href="#hotel">Hotel</a></li>
<li><a class="reference internal" href="#india">India</a></li>
<li><a class="reference internal" href="#sierra">Sierra</a></li>
<li><a class="reference internal" href="#xray">Xray</a></li>
<li><a class="reference internal" href="#network">Network</a></li>
<li><a class="reference internal" href="#futuregrid-network-impairments-device-nid">FutureGrid Network Impairments Device (NID)</a></li>
<li><a class="reference internal" href="#id7">Storage</a></li>
<li><a class="reference internal" href="#using-hpss-from-futuregrid">Using HPSS from FutureGrid</a></li>
<li><a class="reference internal" href="#id12">Status</a></li>
<li><a class="reference internal" href="#accessing-futuregrid-resources-via-ssh">Accessing FutureGrid resources via SSH</a></li>
<li><a class="reference internal" href="#requirement-for-windows-users">Requirement for Windows Users</a></li>
<li><a class="reference internal" href="#instructions-for-both-windows-and-unix-users">Instructions for both Windows and Unix users</a><ul>
<li><a class="reference internal" href="#generate-ssh-key">Generate SSH key</a></li>
<li><a class="reference internal" href="#check-your-ssh-key">Check your ssh key</a></li>
<li><a class="reference internal" href="#copy-the-content-of-your-public-key">Copy the content of your public key</a></li>
<li><a class="reference internal" href="#upload-the-key-to-the-futuregrid-portal">Upload the key to the FutureGrid Portal</a></li>
<li><a class="reference internal" href="#testing-your-ssh-key">Testing your ssh key</a></li>
<li><a class="reference internal" href="#testing-your-ssh-key-on-hotel">Testing your ssh key on Hotel</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hpc-services">HPC Services</a></li>
<li><a class="reference internal" href="#using-hpc-services-on-futuregrid">Using HPC Services on FutureGrid</a><ul>
<li><a class="reference internal" href="#accessing-systems">Accessing Systems</a></li>
<li><a class="reference internal" href="#filesystem-layout">Filesystem Layout</a></li>
<li><a class="reference internal" href="#modules">Modules</a></li>
<li><a class="reference internal" href="#managing-applications-with-torque">Managing Applications with Torque</a></li>
<li><a class="reference internal" href="#message-passing-interface-mpi">Message Passing Interface (MPI)</a></li>
<li><a class="reference internal" href="#mpi-libraries">MPI Libraries</a></li>
<li><a class="reference internal" href="#compiling-mpi-applications">Compiling MPI&nbsp;Applications</a></li>
<li><a class="reference internal" href="#running-mpi-applications">Running MPI&nbsp;Applications</a></li>
</ul>
</li>
<li><a class="reference internal" href="#log-in-to-hpc-services">Log in to HPC services</a></li>
<li><a class="reference internal" href="#generating-ssh-keys-for-futuregrid-access">Generating SSH Keys for FutureGrid Access</a></li>
<li><a class="reference internal" href="#key-generation">Key Generation</a><ul>
<li><a class="reference internal" href="#generate-public-private-key-pair"><strong>1.&nbsp;</strong><strong>Generate Public/Private Key Pair</strong></a></li>
<li><a class="reference internal" href="#list-the-result"><strong>2.&nbsp;</strong><strong>List the result</strong></a></li>
<li><a class="reference internal" href="#add-or-replace-passphrase-for-an-already-generated-key"><strong>3.&nbsp;</strong><strong>Add or Replace Passphrase for an Already Generated Key</strong></a></li>
<li><a class="reference internal" href="#capture-the-public-key-for-futuregrid"><strong>4.</strong>&nbsp;&nbsp;<strong>Capture the Public Key for FutureGrid</strong></a></li>
<li><a class="reference internal" href="#key-management">5. Key Management</a></li>
<li><a class="reference internal" href="#resetting-the-ssh-key">6. Resetting the SSH key</a></li>
<li><a class="reference internal" href="#i-still-can-not-access-fg-resources">7. I still can not access FG resources</a></li>
</ul>
</li>
<li><a class="reference internal" href="#working-with-hpc-job-services">Working with HPC Job Services</a><ul>
<li><a class="reference internal" href="#running-queued-jobs-as-part-of-the-hpc-services">Running Queued Jobs as Part of the HPC Services</a><ul>
<li><a class="reference internal" href="#a-simple-job-script">&nbsp;A Simple Job Script</a></li>
<li><a class="reference internal" href="#submitting-your-job">&nbsp; Submitting Your Job</a></li>
</ul>
</li>
<li><a class="reference internal" href="#monitoring-your-job">Monitoring Your Job</a></li>
<li><a class="reference internal" href="#examining-your-job-output">Examining Your Job Output</a></li>
</ul>
</li>
<li><a class="reference internal" href="#analyzing-code-performance">Analyzing Code Performance</a></li>
<li><a class="reference internal" href="#papi">PAPI</a></li>
<li><a class="reference internal" href="#vampir">Vampir</a><ul>
<li><a class="reference internal" href="#vampir-on-futuregrid"><strong>Vampir on FutureGrid</strong></a></li>
<li><a class="reference internal" href="#event-based-performance-tracing-and-profiling"><strong>Event-based Performance Tracing and Profiling</strong></a></li>
<li><a class="reference internal" href="#the-open-trace-format-otf"><strong>The Open Trace Format (OTF)</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#id13"><strong>Getting Started</strong></a><ul>
<li><a class="reference internal" href="#generation-of-trace-data"><strong>Generation of Trace Data</strong></a></li>
<li><a class="reference internal" href="#enabling-performance-tracing"><strong>Enabling Performance Tracing</strong></a></li>
<li><a class="reference internal" href="#tracing-an-application"><strong>Tracing an Application</strong></a></li>
<li><a class="reference internal" href="#starting-vampir-and-loading-a-trace-file"><strong>Starting Vampir and Loading a Trace File</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#basics"><strong>Basics</strong></a><ul>
<li><a class="reference internal" href="#chart-arrangement"><strong>Chart Arrangement</strong></a></li>
<li><a class="reference internal" href="#context-menus"><strong>Context Menus</strong></a></li>
<li><a class="reference internal" href="#zooming"><strong>Zooming</strong></a></li>
<li><a class="reference internal" href="#the-zoom-toolbar"><strong>The Zoom Toolbar</strong></a></li>
<li><a class="reference internal" href="#the-charts-toolbar"><strong>The Charts Toolbar</strong></a></li>
<li><a class="reference internal" href="#properties-of-the-tracefile"><strong>Properties of the Tracefile</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance-data-visualization"><strong>Performance Data Visualization</strong></a><ul>
<li><a class="reference internal" href="#timeline-charts"><strong>Timeline Charts</strong></a><ul>
<li><a class="reference internal" href="#master-timeline-and-process-timeline"><strong>Master Timeline and Process Timeline</strong></a></li>
<li><a class="reference internal" href="#counter-data-timeline"><strong>Counter Data Timeline</strong></a></li>
<li><a class="reference internal" href="#performance-radar"><strong>Performance Radar</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#statistical-charts"><strong>Statistical Charts</strong></a><ul>
<li><a class="reference internal" href="#call-tree"><strong>Call Tree</strong></a></li>
<li><a class="reference internal" href="#function-summary"><strong>Function Summary</strong></a></li>
<li><a class="reference internal" href="#process-summary"><strong>Process Summary</strong></a></li>
<li><a class="reference internal" href="#message-summary"><strong>Message Summary</strong></a></li>
<li><a class="reference internal" href="#communication-matrix-view"><strong>Communication Matrix View</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#informational-charts"><strong>Informational Charts</strong></a></li>
<li><a class="reference internal" href="#function-legend"><strong>Function Legend</strong></a><ul>
<li><a class="reference internal" href="#context-view"><strong>Context View</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#information-filtering-and-reduction"><strong>Information Filtering and Reduction</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#customization"><strong>Customization</strong></a><ul>
<li><a class="reference internal" href="#general-preferences"><strong>General Preferences</strong></a></li>
<li><a class="reference internal" href="#appearance"><strong>Appearance</strong></a></li>
<li><a class="reference internal" href="#saving-policy"><strong>Saving Policy</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#footnotes"><strong>Footnotes</strong></a></li>
<li><a class="reference internal" href="#vampirtrace">VampirTrace</a></li>
<li><a class="reference internal" href="#official-hpcc-results-from-the-acceptance-tests">Official HPCC Results from the Acceptance Tests</a></li>
<li><a class="reference internal" href="#hpcc-results">HPCC Results</a><ul>
<li><a class="reference internal" href="#id22">Official HPCC Results from the Acceptance Tests</a><ul>
<li><a class="reference internal" href="#alamo-view-machine-details-manual-alamo">Alamo&nbsp;<strong>(`view machine details &lt;/manual/alamo&gt;`__)</strong></a></li>
<li><a class="reference internal" href="#india-view-machine-details-manual-india">India&nbsp;<strong>(`view machine details &lt;/manual/india&gt;`__)</strong></a></li>
<li><a class="reference internal" href="#hotel-view-machine-details-manual-hotel">Hotel&nbsp;<strong>(`view machine details &lt;/manual/hotel&gt;`__)</strong></a></li>
<li><a class="reference internal" href="#xray-view-machine-details-manual-xray">XRay&nbsp;<strong>(`view machine details &lt;/manual/xray&gt;`__)</strong></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#hpcc-configuration">HPCC Configuration</a><ul>
<li><a class="reference internal" href="#general-information">General Information</a><ul>
<li><a class="reference internal" href="#id23">India</a></li>
<li><a class="reference internal" href="#id24">Hotel</a></li>
<li><a class="reference internal" href="#id25">Xray</a></li>
</ul>
</li>
<li><a class="reference internal" href="#configuration-files">Configuration Files</a><ul>
<li><a class="reference internal" href="#id26">India</a></li>
<li><a class="reference internal" href="#id27">Hotel</a></li>
<li><a class="reference internal" href="#id28">XRay</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#id29">Foxtrot</a></li>
<li><a class="reference internal" href="#id32">India</a><ul>
<li><a class="reference internal" href="#iu-idataplex-user-manual">IU iDataplex User Manual</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id33">Sierra</a><ul>
<li><a class="reference internal" href="#ucsd-idataplex-user-manual">UCSD iDataPlex User Manual</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id34">Xray</a><ul>
<li><a class="reference internal" href="#this-section-is-maintained-by-greg-pike-please-contact-https-portal-futuregrid-org-help-for-more-information">This section is maintained by Greg Pike; please contact &nbsp;https://portal.futuregrid.org/help&nbsp;for more information.</a></li>
<li><a class="reference internal" href="#iu-cray-user-manual">IU Cray User Manual</a><ul>
<li><a class="reference internal" href="#submitting-a-job">&nbsp;Submitting a job</a></li>
<li><a class="reference internal" href="#how-do-i-submit-a-job-to-the-cray-xt5m-on-futuregrid">How Do I Submit a Job to the Cray XT5m on FutureGrid?</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#id35">Alamo</a></li>
<li><a class="reference internal" href="#id36">Delta</a><ul>
<li><a class="reference internal" href="#gpu-user-manual">GPU&nbsp;User Manual</a></li>
</ul>
</li>
<li><a class="reference internal" href="#running-mpi-gpu-program-on-the-delta-cluster">Running MPI/GPU program on the Delta cluster</a></li>
<li><a class="reference internal" href="#running-programs-on-a-single-gpu">Running programs on a single GPU</a></li>
<li><a class="reference internal" href="#c-means-clustering-using-cuda-on-gpu">C-means clustering using CUDA on GPU</a></li>
<li><a class="reference internal" href="#scalemp-vsmp">ScaleMP vSMP</a></li>
<li><a class="reference internal" href="#accessing-scalemp">Accessing ScaleMP:</a><ul>
<li><a class="reference internal" href="#id37">Submitting a job:</a></li>
</ul>
</li>
<li><a class="reference internal" href="#developing-a-job-script">Developing a job script:</a><ul>
<li><a class="reference internal" href="#mpi">MPI:</a></li>
<li><a class="reference internal" href="#openmp">OpenMP:</a></li>
<li><a class="reference internal" href="#threaded">Threaded:</a></li>
<li><a class="reference internal" href="#throughput">Throughput:</a></li>
</ul>
</li>
<li><a class="reference internal" href="#iaas-infrastructure-as-a-service">IaaS - Infrastructure as a Service</a></li>
<li><a class="reference internal" href="#using-iaas-clouds-on-futuregrid">Using IaaS Clouds on FutureGrid</a><ul>
<li><a class="reference internal" href="#openstack-clouds">OpenStack Clouds</a></li>
<li><a class="reference internal" href="#eucalyptus-clouds">Eucalyptus Clouds</a></li>
<li><a class="reference internal" href="#virtual-appliances-for-training-and-education">Virtual Appliances for Training and Education</a></li>
</ul>
</li>
<li><a class="reference internal" href="#using-nimbus-on-futuregrid">Using Nimbus on FutureGrid</a><ul>
<li><a class="reference internal" href="#image87"></a></li>
<li><a class="reference internal" href="#what-is-nimbus">What is Nimbus?</a></li>
<li><a class="reference internal" href="#nimbus-on-futuregrid">Nimbus on FutureGrid</a></li>
<li><a class="reference internal" href="#id38">Getting Started</a><ul>
<li><a class="reference internal" href="#log-into-hotel">Log into hotel</a></li>
<li><a class="reference internal" href="#download-and-install-cloud-client">Download and install cloud-client</a></li>
<li><a class="reference internal" href="#obtain-your-nimbus-credentials-and-configuration-files">Obtain Your Nimbus Credentials and Configuration Files</a></li>
<li><a class="reference internal" href="#id39">Check Your ssh Key</a></li>
</ul>
</li>
<li><a class="reference internal" href="#using-the-cloud-client">Using the Cloud Client</a><ul>
<li><a class="reference internal" href="#check-out-the-various-futuregrid-clouds">Check out the various FutureGrid clouds</a></li>
<li><a class="reference internal" href="#run-a-virtual-machine">Run a Virtual Machine</a></li>
<li><a class="reference internal" href="#create-a-new-vm-image">Create a New VM Image</a></li>
<li><a class="reference internal" href="#save-the-changes-to-a-new-vm">Save the Changes to a New VM</a></li>
<li><a class="reference internal" href="#launch-your-new-vm">Launch Your New VM</a></li>
<li><a class="reference internal" href="#terminate-the-vm">Terminate the VM</a></li>
</ul>
</li>
<li><a class="reference internal" href="#virtual-clusters">Virtual Clusters</a><ul>
<li><a class="reference internal" href="#cluster-definition-file">Cluster Definition File</a></li>
<li><a class="reference internal" href="#start-the-cluster">Start the Cluster</a></li>
<li><a class="reference internal" href="#check-out-the-virtual-cluster">Check Out the Virtual Cluster</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#cloud-quick-start-launch-a-vm-with-1-command">Cloud Quick Start : Launch a VM with 1 command</a></li>
<li><a class="reference internal" href="#launch-a-vm-via-nimbus">Launch A VM via Nimbus</a></li>
<li><a class="reference internal" href="#options">Options</a><ul>
<li><a class="reference internal" href="#using-other-nimbus-clouds-on-futuregrid">Using Other Nimbus Clouds on FutureGrid</a></li>
<li><a class="reference internal" href="#launching-multiple-vms">Launching Multiple VMs</a></li>
<li><a class="reference internal" href="#using-with-eucalyptus">Using with Eucalyptus</a></li>
</ul>
</li>
<li><a class="reference internal" href="#what-happens">What Happens</a></li>
<li><a class="reference internal" href="#futuregrid-tutorial-nm2-nimbus-one-click-cluster-guide">FutureGrid Tutorial NM2 - Nimbus One-Click Cluster Guide</a></li>
<li><a class="reference internal" href="#using-eucalyptus-on-futuregrid">Using Eucalyptus on FutureGrid</a><ul>
<li><a class="reference internal" href="#summary"><strong>Summary</strong></a></li>
<li><a class="reference internal" href="#requirements"><strong>Requirements</strong></a></li>
<li><a class="reference internal" href="#id40"><strong>Account Creation</strong></a></li>
<li><a class="reference internal" href="#resources-overview"><strong>Resources Overview</strong></a></li>
<li><a class="reference internal" href="#testing-your-setup"><strong>Testing Your Setup</strong></a></li>
<li><a class="reference internal" href="#image-deployment"><strong>Image Deployment</strong></a></li>
<li><a class="reference internal" href="#logging-into-the-vm"><strong>Logging Into the VM</strong></a></li>
<li><a class="reference internal" href="#vm-network-info"><strong>VM Network Info</strong></a></li>
<li><a class="reference internal" href="#image-management"><strong>Image Management</strong></a></li>
<li><a class="reference internal" href="#status-of-deployments">Status of Deployments</a></li>
</ul>
</li>
<li><a class="reference internal" href="#using-openstack-on-futuregrid">Using OpenStack on FutureGrid</a></li>
<li><a class="reference internal" href="#note">NOTE:</a></li>
<li><a class="reference internal" href="#id41">Summary</a></li>
<li><a class="reference internal" href="#id42">Getting Started</a></li>
<li><a class="reference internal" href="#log-into-india">Log into India</a></li>
<li><a class="reference internal" href="#account-and-credentials">Account and Credentials</a></li>
<li><a class="reference internal" href="#euca2ools-ec2-client-tools">Euca2ools (EC2 client tools)</a></li>
<li><a class="reference internal" href="#id43">Testing Your Setup</a></li>
<li><a class="reference internal" href="#list-of-common-images">List of Common Images</a></li>
<li><a class="reference internal" href="#vm-types">VM Types</a></li>
<li><a class="reference internal" href="#id44">Key Management</a><ul>
<li><a class="reference internal" href="#key-pair-verification">Key pair verification</a></li>
</ul>
</li>
<li><a class="reference internal" href="#image-instantiation">Image Instantiation</a></li>
<li><a class="reference internal" href="#better-server-names">Better Server Names</a></li>
<li><a class="reference internal" href="#monitoring-instances">Monitoring Instances</a></li>
<li><a class="reference internal" href="#log-into-your-vm">Log into your VM</a><ul>
<li><a class="reference internal" href="#making-a-snapshot-with-nova-client">Making a snapshot with nova client</a></li>
</ul>
</li>
<li><a class="reference internal" href="#nova-volumes-not-available">Nova Volumes (Not available)</a><ul>
<li><a class="reference internal" href="#list-available-volumes">List available Volumes</a></li>
<li><a class="reference internal" href="#create-a-volume">Create a Volume</a></li>
<li><a class="reference internal" href="#attach-volume">Attach Volume</a></li>
<li><a class="reference internal" href="#using-the-new-disk">Using the new Disk</a></li>
<li><a class="reference internal" href="#detach-volumes">Detach Volumes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#volume-snapshots">Volume Snapshots</a><ul>
<li><a class="reference internal" href="#create-snapshot">Create Snapshot</a></li>
<li><a class="reference internal" href="#list-snapshot">List Snapshot</a></li>
<li><a class="reference internal" href="#create-volume-from-snapshot-not-yet-functional-in-openstack-essex">Create Volume from Snapshot (not yet functional in OpenStack Essex)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#image-registration">Image Registration</a></li>
<li><a class="reference internal" href="#delete-your-images">Delete your images</a></li>
<li><a class="reference internal" href="#terminate-your-vms">Terminate your VMs</a></li>
<li><a class="reference internal" href="#limitations">Limitations</a></li>
<li><a class="reference internal" href="#troubleshooting">Troubleshooting:</a></li>
<li><a class="reference internal" href="#image-management-and-rain-on-futuregrid">Image Management and Rain on FutureGrid</a></li>
<li><a class="reference internal" href="#generate-and-register-an-os-image-on-futuregrid-using-the-fg-shell">Generate and Register an OS Image on FutureGrid using the FG Shell</a><ul>
<li><a class="reference internal" href="#id45">Summary</a></li>
<li><a class="reference internal" href="#request-access">Request access</a></li>
<li><a class="reference internal" href="#id46">Log into India</a></li>
<li><a class="reference internal" href="#start-the-shell">Start the Shell</a></li>
<li><a class="reference internal" href="#generate-the-image">Generate the Image</a></li>
<li><a class="reference internal" href="#image-repository">Image Repository</a></li>
<li><a class="reference internal" href="#register-image">Register Image</a></li>
<li><a class="reference internal" href="#start-image">Start Image</a></li>
</ul>
</li>
<li><a class="reference internal" href="#futuregrid-standalone-image-repository">FutureGrid Standalone Image Repository</a><ul>
<li><a class="reference internal" href="#id47">Introduction</a></li>
<li><a class="reference internal" href="#requirement">Requirement</a></li>
<li><a class="reference internal" href="#software">Software</a></li>
<li><a class="reference internal" href="#documentation">Documentation</a><ul>
<li><a class="reference internal" href="#id48">User Manual</a></li>
<li><a class="reference internal" href="#installation">Installation</a></li>
<li><a class="reference internal" href="#configuration">Configuration</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#manual-image-customization">Manual Image Customization</a><ul>
<li><a class="reference internal" href="#logging-into-india">Logging into India</a></li>
<li><a class="reference internal" href="#requesting-access">Requesting access</a></li>
<li><a class="reference internal" href="#obtaining-the-image">Obtaining the image</a><ul>
<li><a class="reference internal" href="#getting-an-image-from-the-repository">Getting an image from the repository</a></li>
<li><a class="reference internal" href="#generating-a-new-image">Generating a new image</a></li>
</ul>
</li>
<li><a class="reference internal" href="#customizing-the-image">Customizing the image</a></li>
<li><a class="reference internal" href="#transfer-the-image-back-to-india">Transfer the image back to India</a></li>
<li><a class="reference internal" href="#id49">Log into India</a></li>
<li><a class="reference internal" href="#upload-the-image-to-the-repository">Upload the image to the repository</a></li>
<li><a class="reference internal" href="#register-your-image-in-different-infrastructures">Register your image in different infrastructures</a><ul>
<li><a class="reference internal" href="#register-the-image-in-openstack">Register the image in Openstack</a></li>
<li><a class="reference internal" href="#register-the-image-in-eucalyptus">Register the image in Eucalyptus</a></li>
<li><a class="reference internal" href="#register-the-image-in-hpc">Register the image in HPC</a></li>
</ul>
</li>
<li><a class="reference internal" href="#using-your-registered-image">Using your Registered Image</a><ul>
<li><a class="reference internal" href="#openstack-more-info-in-https-portal-futuregrid-org-tutorials-openstack">OpenStack (more info in&nbsp;https://portal.futuregrid.org/tutorials/openstack)</a></li>
<li><a class="reference internal" href="#eucalyptus-more-info-in-https-portal-futuregrid-org-tutorials-eucalyptus3">Eucalyptus (more info in&nbsp;https://portal.futuregrid.org/tutorials/eucalyptus3)</a></li>
<li><a class="reference internal" href="#hpc-more-info-in-https-portal-futuregrid-org-tutorials-hpc">HPC (more info in&nbsp;https://portal.futuregrid.org/tutorials/hpc)</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#rain-manual-pages">RAIN Manual Pages</a></li>
<li><a class="reference internal" href="#fg-repo">fg-repo</a></li>
<li><a class="reference internal" href="#fg-rain">fg-rain</a></li>
<li><a class="reference internal" href="#fg-generate">fg-generate</a></li>
<li><a class="reference internal" href="#fg-register">fg-register</a></li>
<li><a class="reference internal" href="#fg-shell">fg-shell</a></li>
<li><a class="reference internal" href="#fg-portal-manage">fg-portal-manage</a><ul>
<li><a class="reference internal" href="#name">NAME</a></li>
<li><a class="reference internal" href="#synopsis">SYNOPSIS</a></li>
<li><a class="reference internal" href="#description">DESCRIPTION</a></li>
<li><a class="reference internal" href="#examples">EXAMPLES</a></li>
<li><a class="reference internal" href="#see-also"><strong>SEE ALSO</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#vine">ViNe</a></li>
<li><a class="reference internal" href="#opennebula-2-0-tutorial">OpenNebula 2.0 Tutorial</a></li>
<li><a class="reference internal" href="#create-a-vm-image-contextualized-for-opennebula">Create a VM Image Contextualized for OpenNebula</a><ul>
<li><a class="reference internal" href="#create-vm-image-using-kvm">Create VM Image Using KVM</a></li>
<li><a class="reference internal" href="#contextualize-vm">Contextualize VM</a></li>
</ul>
</li>
<li><a class="reference internal" href="#managing-virtual-machines">Managing Virtual Machines</a><ul>
<li><a class="reference internal" href="#virtual-machine-template">Virtual Machine Template</a></li>
</ul>
</li>
<li><a class="reference internal" href="#managing-the-image-repository">Managing the Image Repository</a></li>
<li><a class="reference internal" href="#managing-physical-hosts-and-clusters">Managing Physical Hosts and Clusters</a><ul>
<li><a class="reference internal" href="#managing-hosts">Managing Hosts</a></li>
</ul>
</li>
<li><a class="reference internal" href="#managing-virtual-networks">Managing Virtual Networks</a><ul>
<li><a class="reference internal" href="#defining-a-virtual-network">Defining a Virtual Network</a><ul>
<li><a class="reference internal" href="#fixed-virtual-network">Fixed Virtual Network</a></li>
<li><a class="reference internal" href="#ranged-virtual-network">Ranged Virtual Network</a></li>
</ul>
</li>
<li><a class="reference internal" href="#adding-and-deleting-virtual-networks">Adding and Deleting Virtual Networks</a></li>
<li><a class="reference internal" href="#using-the-leases-within-the-virtual-machine">Using the Leases Within the Virtual Machine</a><ul>
<li><a class="reference internal" href="#configuring-the-virtual-machine-to-use-the-leases">Configuring the Virtual Machine to Use the Leases</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#accounting">Accounting</a><ul>
<li><a class="reference internal" href="#physical-resources">Physical Resources</a></li>
<li><a class="reference internal" href="#virtual-resources">&nbsp;&nbsp;Virtual Resources</a></li>
</ul>
</li>
<li><a class="reference internal" href="#user-management">User Management</a><ul>
<li><a class="reference internal" href="#adding-new-users">Adding New Users</a><ul>
<li><a class="reference internal" href="#public-key-extraction-done-by-users">Public Key Extraction (Done by Users)</a></li>
<li><a class="reference internal" href="#user-creation-done-by-the-administrator">User Creation (done by the administrator)</a></li>
<li><a class="reference internal" href="#user-login-done-by-users">User Login (Done by Users)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#quota">Quota</a><ul>
<li><a class="reference internal" href="#quotas-database-security-done-by-the-administrator">Quotas Database Security (Done by the Administrator)</a></li>
<li><a class="reference internal" href="#default-quotas-done-by-the-administrator">Default Quotas (Done by the Administrator)</a></li>
<li><a class="reference internal" href="#explicit-user-quotas-done-by-the-administrator">Explicit User Quotas (Done by the Administrator)</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#paas-platform-as-a-service">PaaS - Platform as a Service</a></li>
<li><a class="reference internal" href="#using-map-reduce-in-futuregrid">Using Map/Reduce in FutureGrid</a><ul>
<li><a class="reference internal" href="#education-training-with-mapreduce">Education / Training with MapReduce</a></li>
</ul>
</li>
<li><a class="reference internal" href="#running-hadoop-as-a-batch-job-using-myhadoop">Running Hadoop as a Batch Job using MyHadoop</a></li>
<li><a class="reference internal" href="#overview">Overview</a></li>
<li><a class="reference internal" href="#hadoop-on-futuregrid">Hadoop on FutureGrid</a></li>
<li><a class="reference internal" href="#what-is-myhadoop">What is myHadoop?</a></li>
<li><a class="reference internal" href="#running-myhadoop-on-futuregrid">Running myHadoop on FutureGrid</a></li>
<li><a class="reference internal" href="#persistent-mode">Persistent Mode</a></li>
<li><a class="reference internal" href="#customizing-hadoop-settings">Customizing Hadoop&nbsp;Settings</a></li>
<li><a class="reference internal" href="#using-a-different-installation-of-hadoop">Using a Different Installation of Hadoop</a></li>
<li><a class="reference internal" href="#more-information">More Information</a></li>
<li><a class="reference internal" href="#using-salsahadoop-on-futuregrid">Using SalsaHadoop on FutureGrid</a><ul>
<li><a class="reference internal" href="#salsahadoop-introduction">SalsaHadoop Introduction</a></li>
<li><a class="reference internal" href="#running-salsahadoop-on-futuregrid">Running SalsaHadoop on FutureGrid</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hadoop-blast">Hadoop Blast</a><ul>
<li><a class="reference internal" href="#id50">Hadoop Blast</a></li>
<li><a class="reference internal" href="#acknowledge">Acknowledge</a></li>
<li><a class="reference internal" href="#id51">Requirement</a></li>
<li><a class="reference internal" href="#download-hadoop-blast-under">1. Download Hadoop Blast under $</a></li>
<li><a class="reference internal" href="#hadoop-home">HADOOP_HOME</a></li>
<li><a class="reference internal" href="#prepare-hadoop-blast">2. Prepare Hadoop Blast</a></li>
<li><a class="reference internal" href="#execute-hadoop-blast">3. Execute Hadoop-Blast</a></li>
<li><a class="reference internal" href="#monitoring-hadoop">3. Monitoring Hadoop</a></li>
<li><a class="reference internal" href="#finishing-the-map-reduce-process">5.&nbsp;Finishing the Map-Reduce process</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hadoop-wordcount">Hadoop WordCount</a><ul>
<li><a class="reference internal" href="#id52">Hadoop WordCount</a></li>
<li><a class="reference internal" href="#id53">Acknowledge</a></li>
<li><a class="reference internal" href="#id54">Requirement</a></li>
<li><a class="reference internal" href="#download-and-unzip-wordcount-under-hadoop-home">1. Download and unzip WordCount under $HADOOP_HOME</a></li>
<li><a class="reference internal" href="#execute">2. Execute</a></li>
<li><a class="reference internal" href="#id55">Hadoop-WordCount</a></li>
<li><a class="reference internal" href="#id56">3. Monitoring Hadoop</a></li>
<li><a class="reference internal" href="#check-the-result">4.&nbsp;Check the result</a></li>
<li><a class="reference internal" href="#id57">5.&nbsp;Finishing the Map-Reduce process</a></li>
</ul>
</li>
<li><a class="reference internal" href="#using-twister-on-futuregrid">Using Twister on FutureGrid</a><ul>
<li><a class="reference internal" href="#what-is-twister">What is Twister?</a></li>
<li><a class="reference internal" href="#image114"></a></li>
<li><a class="reference internal" href="#running-twister-on-futuregrid">Running Twister on FutureGrid</a></li>
<li><a class="reference internal" href="#papers-and-presentations">Papers and Presentations</a></li>
</ul>
</li>
<li><a class="reference internal" href="#twister-blast">Twister Blast</a><ul>
<li><a class="reference internal" href="#id60">Twister Blast</a></li>
<li><a class="reference internal" href="#id61">Acknowledge</a></li>
<li><a class="reference internal" href="#id62">Requirement</a></li>
<li><a class="reference internal" href="#download-and-prepare-the">1. Download and prepare the</a></li>
<li><a class="reference internal" href="#id63">Twister-Blast</a></li>
<li><a class="reference internal" href="#prepare-twister-blast-input">2. Prepare Twister-Blast input</a></li>
<li><a class="reference internal" href="#execute-twister-blast">3. Execute Twister-Blast</a></li>
<li><a class="reference internal" href="#id64">4.&nbsp;Finishing the Map-Reduce process</a></li>
</ul>
</li>
<li><a class="reference internal" href="#eucalyptus-and-twister-on-futuregrid">Eucalyptus and Twister on FutureGrid</a></li>
<li><a class="reference internal" href="#the-futuregrid-twister-tutorial">The FutureGrid Twister Tutorial</a></li>
<li><a class="reference internal" href="#using-pegasus-on-futuregrid">Using Pegasus on FutureGrid</a><ul>
<li><a class="reference internal" href="#about-pegasus">About Pegasus</a></li>
<li><a class="reference internal" href="#the-pegasus-run-time-cloud-architecture">The Pegasus Run-Time Cloud Architecture</a></li>
<li><a class="reference internal" href="#id65">Using Pegasus on FutureGrid</a></li>
</ul>
</li>
<li><a class="reference internal" href="#management-services">Management Services</a></li>
<li><a class="reference internal" href="#nimbus-phantom">Nimbus Phantom</a></li>
<li><a class="reference internal" href="#precip-pegasus-repeatable-experiments-for-the-cloud-in-python">Precip - Pegasus Repeatable Experiments for the Cloud in Python</a><ul>
<li><a class="reference internal" href="#id66">Overview</a></li>
<li><a class="reference internal" href="#id67">Installation</a></li>
<li><a class="reference internal" href="#api">API</a></li>
<li><a class="reference internal" href="#id68">Examples</a><ul>
<li><a class="reference internal" href="#hello-world">Hello World</a></li>
<li><a class="reference internal" href="#resources-from-mulitple-infrastructures">Resources from mulitple infrastructures</a></li>
<li><a class="reference internal" href="#setting-up-a-condor-pool-and-run-a-pegasus-workflow">Setting up a Condor pool and run a Pegasus workflow</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#cloudinit-d">cloudinit.d</a></li>
<li><a class="reference internal" href="#grid-services">Grid Services</a></li>
<li><a class="reference internal" href="#unicore">Unicore</a></li>
<li><a class="reference internal" href="#id69">Introduction</a></li>
<li><a class="reference internal" href="#what-is-unicore">What is UNICORE?</a></li>
<li><a class="reference internal" href="#connecting-to-the-unicore-bes-endpoints-from-other-grid-middleware-clients">Connecting to the UNICORE BES Endpoints From Other Grid Middleware Clients</a><ul>
<li><a class="reference internal" href="#india-endpoint-info-currently-unavailable">India Endpoint Info &lt;currently unavailable&gt;</a></li>
<li><a class="reference internal" href="#sierra-endpoint-info">Sierra Endpoint Info</a></li>
</ul>
</li>
<li><a class="reference internal" href="#connecting-to-the-unicore-bes-endpoints-using-a-unicore-commandline-client">Connecting to the UNICORE BES Endpoints Using a UNICORE Commandline Client</a><ul>
<li><a class="reference internal" href="#installing-the-unicore6-commandline-client-ucc">Installing the UNICORE6 Commandline Client (UCC)</a><ul>
<li><a class="reference internal" href="#acquire-client-bundle">Acquire Client Bundle</a></li>
<li><a class="reference internal" href="#unpack-ucc">Unpack UCC</a></li>
<li><a class="reference internal" href="#examine-ucc-files">Examine UCC Files</a></li>
<li><a class="reference internal" href="#run-commandline">Run Commandline</a></li>
<li><a class="reference internal" href="#client">Client</a></li>
<li><a class="reference internal" href="#installation-conclusion">Installation Conclusion</a></li>
</ul>
</li>
<li><a class="reference internal" href="#configuring-client-to-connect-to-futuregrid-u6-endpoints">Configuring Client to Connect to FutureGrid U6 Endpoints</a><ul>
<li><a class="reference internal" href="#configuration-overview">Configuration Overview</a></li>
</ul>
</li>
<li><a class="reference internal" href="#setting-up-security-in-unicore">Setting Up Security in UNICORE</a></li>
<li><a class="reference internal" href="#setting-up-a">Setting Up a</a></li>
<li><a class="reference internal" href="#keystore">Keystore</a><ul>
<li><a class="reference internal" href="#creating-a-truststore">Creating a Truststore</a></li>
<li><a class="reference internal" href="#acquiring-the-registry">Acquiring the Registry</a></li>
<li><a class="reference internal" href="#address">Address</a></li>
<li><a class="reference internal" href="#connecting-to-endpoints-w-o-registries">Connecting to Endpoints w/o Registries</a></li>
<li><a class="reference internal" href="#preferences-file-modifications">Preferences File Modifications</a></li>
<li><a class="reference internal" href="#example-preferences-file">Example Preferences File</a></li>
<li><a class="reference internal" href="#locating-the-preferences">Locating the Preferences</a></li>
<li><a class="reference internal" href="#file">File</a></li>
<li><a class="reference internal" href="#validate-client">Validate Client</a></li>
<li><a class="reference internal" href="#setup">Setup</a></li>
<li><a class="reference internal" href="#id70">Configuration</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li><a class="reference internal" href="#submitting-jobs-to-futuregrid-u6-endpoints">Submitting Jobs to FutureGrid U6 Endpoints</a><ul>
<li><a class="reference internal" href="#getting">Getting</a></li>
<li><a class="reference internal" href="#started">Started</a></li>
<li><a class="reference internal" href="#submit-jobs-to">Submit Jobs to</a></li>
<li><a class="reference internal" href="#bes">BES</a></li>
<li><a class="reference internal" href="#other-bes-related">Other BES Related</a></li>
<li><a class="reference internal" href="#commands">Commands</a></li>
<li><a class="reference internal" href="#job-submission-conclusion">Job Submission Conclusion</a></li>
</ul>
</li>
<li><a class="reference internal" href="#references">References</a></li>
<li><a class="reference internal" href="#questions-comments">&nbsp; Questions/Comments</a></li>
</ul>
</li>
<li><a class="reference internal" href="#running-jobs-on-unicore-sites">Running Jobs on UNICORE Sites</a></li>
<li><a class="reference internal" href="#deploying-a-new-unicore-6-grid">Deploying a New UNICORE 6 Grid</a><ul>
<li><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li><a class="reference internal" href="#installing-the-core-server-bundle">Installing the Core Server Bundle</a></li>
<li><a class="reference internal" href="#starting-stopping-the-unicore-servers">Starting/Stopping the UNICORE Servers</a></li>
</ul>
</li>
<li><a class="reference internal" href="#genesis-ii">Genesis II</a></li>
<li><a class="reference internal" href="#what-is-genesisii">What is GenesisII</a></li>
<li><a class="reference internal" href="#connecting-to-the-genesisii-bes-endpoints">Connecting to the GenesisII BES Endpoints</a><ul>
<li><a class="reference internal" href="#supported-data-staging">Supported Data Staging</a></li>
<li><a class="reference internal" href="#protocols">Protocols</a><ul>
<li><a class="reference internal" href="#stage-in">Stage in:</a></li>
<li><a class="reference internal" href="#stage-out">Stage out:</a></li>
</ul>
</li>
<li><a class="reference internal" href="#endpoint-connection-information">Endpoint Connection Information</a><ul>
<li><a class="reference internal" href="#id71">India</a></li>
<li><a class="reference internal" href="#id72">Sierra</a></li>
<li><a class="reference internal" href="#id73">Alamo</a></li>
<li><a class="reference internal" href="#id74">Hotel</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#using-the-futuregrid-genesisii-endpoints-as-a-client">Using the Futuregrid GenesisII Endpoints as a Client</a><ul>
<li><a class="reference internal" href="#non-genesisiiusing-a-standards-compliant-client">Non-GenesisIIUsing a Standards-Compliant Client</a></li>
<li><a class="reference internal" href="#using-the-genesisii-client">Using The GenesisII Client</a><ul>
<li><a class="reference internal" href="#acquiring-genesisii-client-package">Acquiring GenesisII Client Package</a><ul>
<li><a class="reference internal" href="#linux">Linux</a></li>
<li><a class="reference internal" href="#windows">Windows</a></li>
<li><a class="reference internal" href="#macos">MacOS</a></li>
</ul>
</li>
<li><a class="reference internal" href="#installing-the-genesisii-xcg-client-package">Installing the GenesisII/XCG Client Package</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id75">Getting Started</a><ul>
<li><a class="reference internal" href="#start-grid-shell">Start Grid Shell</a><ul>
<li><a class="reference internal" href="#id76">Windows</a></li>
<li><a class="reference internal" href="#linux-or-macos">Linux or MacOS</a></li>
</ul>
</li>
<li><a class="reference internal" href="#running-jobs">Running Jobs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#getting-help">Getting Help</a></li>
</ul>
</li>
<li><a class="reference internal" href="#saga-supporting-distributed-applications-on-grids-clouds-on-futuregrid">SAGA supporting Distributed Applications on Grids, Clouds on FutureGrid</a></li>
<li><a class="reference internal" href="#emi-unicore-tutorial">EMI Unicore Tutorial</a><ul>
<li><a class="reference internal" href="#what-is-emi">What is EMI?</a></li>
<li><a class="reference internal" href="#emi-on-futuregrid">EMI on FutureGrid</a></li>
<li><a class="reference internal" href="#launching-the-unicore-appliance">Launching the UNICORE appliance</a></li>
<li><a class="reference internal" href="#using-your-unicore-server">Using your UNICORE Server</a></li>
<li><a class="reference internal" href="#querying-resources-on-emi-unicore">Querying Resources on EMI UNICORE</a></li>
<li><a class="reference internal" href="#runing-a-job-and-the-job-description-file">Runing a job and the job description file</a></li>
<li><a class="reference internal" href="#running-your-own-scripts">Running your own scripts</a></li>
<li><a class="reference internal" href="#handling-data">Handling Data</a></li>
<li><a class="reference internal" href="#resources">Resources</a></li>
<li><a class="reference internal" href="#running-job-on-a-set-of-files">Running job on a set of files</a></li>
</ul>
</li>
<li><a class="reference internal" href="#tutorials">Tutorials</a><ul>
<li><a class="reference internal" href="#tutorial-topic-0-accessing-futuregrid-resources">Tutorial Topic 0: Accessing FutureGrid Resources</a></li>
<li><a class="reference internal" href="#tutorial-topic-1-cloud-provisioning-platforms">Tutorial Topic 1: Cloud Provisioning Platforms</a></li>
<li><a class="reference internal" href="#tutorial-topic-2-cloud-run-time-map-reduce-platforms">Tutorial Topic 2: Cloud Run-time Map/Reduce Platforms</a></li>
<li><a class="reference internal" href="#tutorial-topic-3-grid-appliances-for-training-education-and-outreach">Tutorial Topic 3: Grid Appliances for Training, Education, and Outreach</a></li>
<li><a class="reference internal" href="#tutorial-topic-4-high-performance-computing">Tutorial Topic 4: High Performance Computing</a></li>
<li><a class="reference internal" href="#tutorial-topic-5-experiment-management">Tutorial Topic 5: Experiment Management</a></li>
<li><a class="reference internal" href="#tutorial-topic-6-image-management-and-rain">Tutorial Topic 6: Image Management and Rain</a></li>
<li><a class="reference internal" href="#tutorial-topic-7-storage">Tutorial Topic 7: &nbsp;Storage</a></li>
<li><a class="reference internal" href="#other-tutorials-and-educational-materials">Other Tutorials and Educational Materials</a></li>
</ul>
</li>
<li><a class="reference internal" href="#futuregrid-grid-appliance-for-nimbus-and-eucalyptus">FutureGrid Grid Appliance for Nimbus and Eucalyptus</a></li>
<li><a class="reference internal" href="#one-click-hadoop-wordcount-on-eucalyptus-futuregrid">One-click Hadoop WordCount on Eucalyptus FutureGrid</a></li>
<li><a class="reference internal" href="#one-click-twister-k-means-on-eucalyptus-futuregrid">One-click Twister K-means on Eucalyptus FutureGrid</a></li>
<li><a class="reference internal" href="#register-virtual-box-image-on-openstack">Register Virtual Box Image on OpenStack</a><ul>
<li><a class="reference internal" href="#id77">Prerequisites</a><ul>
<li><a class="reference internal" href="#disable-selinux-only-for-redhat-based-linux-like-centos">Disable SELinux (Only for RedHat-based Linux like CentOS)</a></li>
<li><a class="reference internal" href="#configuring-the-image-network-interface-eth0-for-dhcp">Configuring the image network interface (eth0) for DHCP</a></li>
<li><a class="reference internal" href="#configure-the-image-to-allow-openstack-to-inject-the-ssh-key">Configure the image to allow OpenStack to inject the ssh key</a></li>
<li><a class="reference internal" href="#configure-udev-persistent-rules-only-centos">Configure udev persistent rules (only CentOS)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#convert-your-virtual-box-image-to-raw-format">Convert your virtual box image to raw format</a></li>
<li><a class="reference internal" href="#convert-the-image-to-qcow2-format-optional">Convert the image to qcow2 format (optional)</a></li>
<li><a class="reference internal" href="#test-your-image">Test your image</a></li>
<li><a class="reference internal" href="#transfer-your-image-to-india">Transfer your Image to India</a></li>
<li><a class="reference internal" href="#id78">Log into India</a></li>
<li><a class="reference internal" href="#upload-your-image-to-openstack">Upload your image to OpenStack</a></li>
<li><a class="reference internal" href="#checking-status-image">Checking Status Image</a></li>
<li><a class="reference internal" href="#test-image-in-openstack">Test Image in OpenStack</a></li>
<li><a class="reference internal" href="#id79">Troubleshooting</a></li>
<li><a class="reference internal" href="#notes">Notes:</a></li>
</ul>
</li>
<li><a class="reference internal" href="#virtual-appliances">Virtual Appliances</a></li>
<li><a class="reference internal" href="#development-projects">Development Projects</a></li>
<li><a class="reference internal" href="#appendix">Appendix</a></li>
<li><a class="reference internal" href="#upgrading-nimbus-on-fg-clusters">Upgrading Nimbus On FG clusters</a><ul>
<li><a class="reference internal" href="#disclaimer">Disclaimer</a></li>
<li><a class="reference internal" href="#upgrading-workspace-service-and-cumulus">Upgrading Workspace Service and Cumulus</a></li>
<li><a class="reference internal" href="#upgrading-workspace-control">Upgrading Workspace Control</a></li>
</ul>
</li>
<li><a class="reference internal" href="#faq">FAQ</a></li>
</ul>
</li>
</ul>
</ul>
</li>
            
            
              
  <li><a href="preface.html"
         title="previous chapter">&laquo; Preface</a></li>
  <li><a href="part-2.html"
         title="next chapter">Part 2 - OTHER STUFF &raquo;</a></li>
            
            
              <li>
  <a href="_sources/fgmanual.txt"
     rel="nofollow">Source</a></li>
            
          </ul>

          
            
<form class="navbar-search pull-right" style="margin-bottom:-3px;" action="search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
      </div>
    </div>
  </div>

<div class="container">
  
  <div class="section" id="user-manual">
<h1>USER MANUAL<a class="headerlink" href="#user-manual" title="Permalink to this headline"></a></h1>
<p><img alt="image0" src="https://portal.futuregrid.org/sites/default/files/u30/fg-logo-md.gif" /></p>
<hr class="docutils" />
<div class="section" id="futuregrid-user-manual">
<h2>FutureGrid User Manual<a class="headerlink" href="#futuregrid-user-manual" title="Permalink to this headline"></a></h2>
<p>Gregor von Laszewski, Fugang Wang, Javier Diaz, Archit Kulshrestha,
Jonathan Bolte, and many more</p>
<p>Pervasive Technology Institute, Indiana University, Bloomington, IN
47408, U.S.A.</p>
<dl class="docutils">
<dt>Please</dt>
<dd>If your name is missing in the author list, please add it, or
send mail to <a class="reference external" href="mailto:laszewski&#37;&#52;&#48;gmail&#46;com">laszewski<span>&#64;</span>gmail<span>&#46;</span>com</a>
Many other&nbsp;<a class="reference external" href="https://portal.futuregrid.org/staff">staff</a>&nbsp;members
have contributed to the FutureGrid project.</dd>
</dl>
<p>This material is based upon work supported in part by the National
Science Foundation under Grant No. 0910812 to Indiana University for
&#8220;FutureGrid: An Experimental, High-Performance Grid Test-bed.&#8221; Partners
in the FutureGrid project include U. Chicago, U. Florida, San Diego
Supercomputer Center - UC San Diego, U. Southern California, U. Texas at
Austin, U. Tennessee at Knoxville, U. of Virginia, Purdue I., and T-U.
Dresden. -Or the shorter- This material is based upon work supported in
part by the National Science Foundation under Grant No. 0910812.</p>
</div>
<div class="section" id="preface">
<h2>Preface<a class="headerlink" href="#preface" title="Permalink to this headline"></a></h2>
<blockquote>
<div>FutureGrid is a project led by Indiana University and funded by the</div></blockquote>
<p>National Science Foundation (NSF) to develop a high-performance grid
test bed that will allow scientists to collaboratively develop and test
innovative approaches to parallel, grid, and cloud computing.
&nbsp;FutureGrid will provide the infrastructure to researchers that allows
them to perform their own computational experiments using distributed
systems. The goal is to make it easier for scientists to conduct such
experiments in a transparent manner.</p>
<p>FutureGrid users will be able to deploy their own hardware and software
configurations on a public/private cloud, and run their experiments.
They will be able to save their configurations and execute their
experiments using the provided tools. The FutureGrid test bed is
composed of a high-speed network connecting distributed clusters of high
performance computers. FutureGrid will employ virtualization technology
that will allow the test bed to support a wide range of operating
systems.</p>
<p>The NSF awarded $10.1 million to enable joint development of
FutureGrid through a partnership of Indiana University, Purdue
University, University of California - San Diego, University of
Chicago/Argonne National Labs, University of Florida, University of
Southern California, University of Texas, and the Center for Information
Services and High Performance Computing at Technische Universitt
Dresden. The principal investigator is Dr. Geoffrey C. Fox, Director of
the&nbsp;<a class="reference external" href="http://pti.iu.edu/dsc">Digital Science Center</a>&nbsp;at the
IU&nbsp;<a class="reference external" href="http://pti.iu.edu/">Pervasive Technology Institute</a>.</p>
<div class="section" id="citation">
<h3>Citation<a class="headerlink" href="#citation" title="Permalink to this headline"></a></h3>
<p>If you use FutureGrid, we ask you kindly to include the following
reference in your papers:</p>
<p><a class="reference external" href="https://portal.futuregrid.org/biblio/author/11">Fox, G.</a>,&nbsp;<a class="reference external" href="https://portal.futuregrid.org/biblio/author/1">G. von
Laszewski</a>,&nbsp;<a class="reference external" href="https://portal.futuregrid.org/biblio/author/61">J.
Diaz</a>,&nbsp;<a class="reference external" href="https://portal.futuregrid.org/biblio/author/3">K.
Keahey</a>,&nbsp;<a class="reference external" href="https://portal.futuregrid.org/biblio/author/18">J.
Fortes</a>,&nbsp;<a class="reference external" href="https://portal.futuregrid.org/biblio/author/43">R.
Figueiredo</a>,&nbsp;<a class="reference external" href="https://portal.futuregrid.org/biblio/author/24">S.
Smallen</a>,&nbsp;<a class="reference external" href="https://portal.futuregrid.org/biblio/author/16">W.
Smith</a>, and&nbsp;<a class="reference external" href="https://portal.futuregrid.org/biblio/author/17">A.
Grimshaw</a>,&nbsp;&#8221;<a class="reference external" href="https://portal.futuregrid.org/references/futuregrid-recon-figurable-testbed-cloud-hpc-and-grid-computing">FutureGrid
- a recon figurable testbed for Cloud, HPC and Grid
Computing</a>&#8221;,&nbsp;Contemporary
High Performance Computing: From Petascale toward Exascale, April, 2013.
Editor J.
Vetter.&nbsp;Download:&nbsp;<a class="reference external" href="https://portal.futuregrid.org/biblio/view/2034">vonLaszewski-fg-bookchapter.pdf</a></p>
</div>
<div class="section" id="acknowledgement">
<h3>Acknowledgement<a class="headerlink" href="#acknowledgement" title="Permalink to this headline"></a></h3>
<p>The FutureGrid project is funded by the National Science Foundation
(NSF) and is led by&nbsp;<a class="reference external" href="http://www.iub.edu/">Indiana
University</a>&nbsp;with&nbsp;<a class="reference external" href="http://www.uchicago.edu/index.shtml">University of
Chicago</a>,&nbsp;<a class="reference external" href="http://www.ufl.edu/">University of
Florida</a>,&nbsp;<a class="reference external" href="http://www.sdsc.edu/">San Diego Supercomputing
Center</a>,&nbsp;<a class="reference external" href="http://www.tacc.utexas.edu/">Texas Advanced Computing
Center</a>,&nbsp;<a class="reference external" href="http://www.virginia.edu/">University of
Virginia</a>,<a class="reference external" href="http://www.utk.edu/">University of
Tennessee</a>,&nbsp;<a class="reference external" href="http://www3.isi.edu/home">University of Southern
California</a>,&nbsp;<a class="reference external" href="http://tu-dresden.de/">Dresden</a>,&nbsp;<a class="reference external" href="http://www.purdue.edu/">Purdue
University</a>, and&nbsp;<a class="reference external" href="https://www.grid5000.fr/mediawiki/index.php/Grid5000:Home">Grid
5000</a>&nbsp;as
partner sites. This material is based upon work supported in part by the
National Science Foundation under Grant No. 0910812.</p>
<p>About this Manual</p>
<p>This manual is the primary location of&nbsp; information about FutureGrid
software and services. All FG team members update this manual regularly.</p>
<blockquote>
<div>We would also like to<strong>*ask the community to*</strong><strong>*help*</strong>.&nbsp; If</div></blockquote>
<p>you would enjoy contributing sections and chapters as part of your
community activities, please contact Gregor
at&nbsp;<a class="reference external" href="mailto:laszewski&#37;&#52;&#48;gmail&#46;com">laszewski<span>&#64;</span>gmail<span>&#46;</span>com</a>. We welcome
additional contributors and editors to this manual.</p>
<p>Enjoy using and expanding the FutureGrid User Manual!</p>
<p>Gregor von Laszewski</p>
</div>
</div>
<div class="section" id="sponsors">
<h2>Sponsors<a class="headerlink" href="#sponsors" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="id1">
<h2>Citation<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h2>
<p>If you use FutureGrid, we ask you kindly to include the following
reference in your papers:</p>
<ol class="arabic simple">
<li>Fox, G., G. von Laszewski, J. Diaz, K. Keahey, J. Fortes, R.
Figueiredo, S. Smallen, W. Smith, and A. Grimshaw, &#8220;FutureGrid - a
recon figurable testbed for Cloud, HPC and Grid Computing&#8221;,
Contemporary High Performance Computing: From Petascale toward
Exascale, April, 2013. Editor J. Vetter.
&nbsp;<a class="reference external" href="https://portal.futuregrid.org/sites/default/files/vonLaszewski-fg-bookchapter.pdf">[pdf]</a></li>
</ol>
</div>
<div class="section" id="id2">
<h2>Sponsors<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<p>FutureGrid has received funding from a number of agencies. The current
sponsors of FutureGrid include:</p>
<p><img alt="image1" src="https://portal.futuregrid.org/sites/default/files/u30/nsf.jpg" /></p>
<p>NSF</p>
<p>You must include the following in all papers and oral presentations if
you use FG:</p>
<p><em>This material is based upon work supported in part by the National
Science Foundation under Grant No. 0910812 to Indiana University for
&#8220;FutureGrid: An Experimental, High-Performance Grid Test-bed.&#8221; Partners
in the&nbsp;FutureGrid&nbsp;project include U. Chicago, U. Florida, San Diego
Supercomputer Center -&nbsp;UC&nbsp;San Diego, U. Southern California, U. Texas at
Austin, U. Tennessee at Knoxville, U. of Virginia, and T-U. Dresden.</em></p>
<p>-Or the shorter version-</p>
<p><em>This material is based upon work supported in part by the National
Science Foundation under Grant No. 0910812.</em></p>
<p><img alt="image2" src="https://portal.futuregrid.org/sites/default/files/images/lilly_endowment.jpg" /></p>
<p><strong>Lilly Endowment</strong></p>
<p>Provided a $15 million grant to Indiana University to establish
Pervasive Technology Institute (PTI).</p>
<p><img alt="image3" src="https://portal.futuregrid.org/sites/default/files/resize/u23/iu-logo-50x64.jpeg" /></p>
<p><strong>Indiana University Pervasive Technology Institute</strong></p>
<p>Indiana University has sponsored this project with $5M&nbsp;in matching
funding.</p>
</div>
<div class="section" id="about">
<h2>About<a class="headerlink" href="#about" title="Permalink to this headline"></a></h2>
<p>The&nbsp;<a class="reference external" href="https://portal.futuregrid.org/award-abstract-091081-futuregrid-experimental-high-performance-grid-test-bed-0">FutureGrid
Project</a>&nbsp;makes
it possible for researchers to tackle complex research challenges in
computer science related to the use and security of grids and clouds.
These include topics ranging from authentication, authorization,
scheduling, virtualization, middleware design, interface design, and
cybersecurity, to the optimization of grid-enabled and cloud-enabled
computational schemes for researchers in astronomy, chemistry, biology,
engineering, atmospheric science and epidemiology. The project team
provides a significant new experimental computing grid and cloud
test-bed, named FutureGrid, to the research community, together with
user support for third-party researchers conducting experiments on
FutureGrid.</p>
<p>The test-bed makes it possible for researchers to conduct experiments
by submitting an experiment plan that is then executed via a
sophisticated workflow engine, preserving the provenance and state
information necessary to allow reproducibility.</p>
<blockquote>
<div>The test-bed also includes a geographically distributed set of</div></blockquote>
<p>heterogeneous computing systems, a data management system that holds
both metadata and a growing library of software images, and a dedicated
network allowing isolatable, secure experiments. The test-bed supports
virtual machine-based environments, as well as native operating systems
for experiments aimed at minimizing overhead and maximizing performance.
The project partners integrate existing open-source software packages to
create an easy-to-use software environment that supports the
instantiation, execution, and recording of grid and cloud computing
experiments.</p>
<p>One of the goals of the project is to understand the behavior and
utility of cloud computing approaches. Researchers are able to measure
the overhead of cloud technology by requesting linked experiments on
both virtual and bare-metal systems. FutureGrid enables US scientists to
develop and test new approaches to parallel, grid, and cloud computing,
and compare and collaborate with international efforts in this area. The
FutureGrid project provides an experimental platform that accommodates
batch, grid, and cloud computing, allowing researchers to attack a range
of research questions associated with optimizing, integrating and
scheduling the different service models. The FutureGrid also provides a
test-bed for middleware development and, because of its private network,
allows middleware researchers to do controlled experiments under
different network conditions and to test approaches to middleware that
include direct interaction with the network control layer. Another
component of the project is the development of benchmarks appropriate
for grid computing, including workflow-based benchmarks derived from
applications in astronomy, bioinformatics, seismology, and physics.</p>
<blockquote>
<div>The FutureGrid forms part of NSF&#8217;s high-performance</div></blockquote>
<p>cyberinfrastructure. It increases the capability of the XSEDE to support
innovative computer science research requiring access to lower levels of
the grid software stack, the networking software stack, and to
virtualization and workflow orchestration tools.</p>
<blockquote>
<div>Education and broader outreach activities include the dissemination</div></blockquote>
<p>of curricular materials on the use of FutureGrid, pre-packaged
FutureGrid virtual machines configured for particular course modules,
and educational modules based on virtual appliance networks and social
networking technologies that focus on education in networking, parallel
computing, virtualization and distributed computing. The project
advances education and training in distributed computing at academic
institutions with less diverse computational resources. It does this
through the development of instructional resources that include
preconfigured environments that provide students with sandboxed virtual
clusters. These can be used for teaching courses in parallel, cloud, and
grid computing. Such resources also provide academic institutions with a
simple opportunity to experiment with cloud technology to see if such
technology can enhance their campus resources. The FutureGrid project
leverages the fruits of several software development projects funded by
the National Science Foundation and the Department of Energy.</p>
<p>Some useful resources are a poster
in<a class="reference external" href="/sites/default/files/future%20grid.jpg">&nbsp;low</a>&nbsp;and&nbsp;<a class="reference external" href="/sites/default/files/FutureGrid%20poster%20JPEG_0.jpg">high</a>&nbsp;resolution
as well as a generic presentation as of&nbsp;<a class="reference external" href="/sites/default/files/talk-FutureGrid-overview_1.pptx">October 11
2009</a>&nbsp;with an
update on&nbsp;<a class="reference external" href="/sites/default/files/FutureGridOverviewNov06-09_0.ppt">November 6
2009</a>&nbsp;and&nbsp;<a class="reference external" href="http://grids.ucs.indiana.edu/ptliupages/presentations/FG-NSF-Sept15-10.pptx">September
15
2010</a>.</p>
<p>This material is based upon work supported in part by the National
Science Foundation under Grant No. 0910812 to Indiana University for
&#8220;FutureGrid: An Experimental, High-Performance Grid Test-bed.&#8221; Partners
in the FutureGrid project include U. Chicago, U. Florida, San Diego
Supercomputer Center - UC San Diego, U. Southern California, U. Texas at
Austin, U. Tennessee at Knoxville, U. of Virginia, Purdue I., and T-U.
Dresden.</p>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p>The&nbsp;<a class="reference external" href="https://portal.futuregrid.org/award-abstract-091081-futuregrid-experimental-high-performance-grid-test-bed-0">FutureGrid
Project</a>&nbsp;allows
researchers to tackle complex research challenges in computer science
related to the use and security of grids and clouds. We provide a
significant new experimental computing grid and cloud test-bed, named
FutureGrid, to the research community, together with user support for
third-party researchers conducting experiments on FutureGrid.</p>
<div class="section" id="project-account-application">
<h3>Project &amp; Account Application<a class="headerlink" href="#project-account-application" title="Permalink to this headline"></a></h3>
<p>FutureGrid allows you to easily create a project and use FG resources.
All you need to do is to create a portal account and apply for a
project. Detailed information is provided in the
<a class="reference external" href="https://portal.futuregrid.org/manual/access">manual</a>.</p>
</div>
<div class="section" id="cloud-services">
<h3>Cloud Services<a class="headerlink" href="#cloud-services" title="Permalink to this headline"></a></h3>
<p>Currently, FutureGrid provides three cloud services on a variety of
resources: Nimbus, Eucalyptus, and OpenStack. However, not all services
on the different resources are integrated, and an additional account
request is necessary for accessing Eucalyptus and OpenStack&nbsp;. To find
out how the machines are&nbsp;divided, we provide a convenient&nbsp;<a class="reference external" href="http://inca.futuregrid.org:8080/inca/jsp/partitionTable.jsp">status
monitor</a>.</p>
<dl class="docutils">
<dt>Nimbus:</dt>
<dd>Nimbus is an open-source service package that allows users to run
virtual machines on FutureGrid hardware. You can easily upload
your own VM image or customize an image provided by us. When you
boot a VM, it is assigned a public IP address (and/or an optional
private address); you are authorized to log in as root via
SSH. You can then run services, perform computations, and
configure the system as desired. After using and configuring the
VM, you can save the modified VM image back to the Nimbus image
repository. Users can find more details in the&nbsp;<a class="reference external" href="https://portal.futuregrid.org/tutorials/nimbus">Nimbus tutorial</a>.</dd>
<dt>Eucalyptus:</dt>
<dd>Eucalyptus is an open-source software platform that implements
IaaS-style cloud computing. Eucalyptus provides a Amazon Web
Services (AWS) compliant EC2-based web service interface for
interacting with the Cloud service. Eucalyptus also provides
services such as the AWS-compliant Walrus and a user interface for
managing users and images.  Users can find more details in the&nbsp;<a class="reference external" href="https://portal.futuregrid.org/tutorials/eucalyptus">Eucalyptus tutorial</a>.</dd>
<dt>OpenStack:</dt>
<dd>OpenStack is a collection of open source components to deliver
public and private clouds. These components currently include
OpenStack Compute (called Nova), OpenStack Object Storage (called
Swift), and OpenStack Image Service (called Glance). OpenStack is
a new effort and has received considerable momentum due to its
openness and the support of companies. Users can find more details
in the <a class="reference external" href="https://portal.futuregrid.org/tutorials/openstack">OpenStack&nbsp;Tutorial</a>.</dd>
<dt>OpenNebula:</dt>
<dd><strong>Not yet available to the public</strong>, OpenNebula is an open and
flexible tool that fits into existing data center environments to
build any type of IaaS Cloud deployment. OpenNebula can be
primarily used as a virtualization tool to manage your virtual
infrastructure in the data center or cluster, which is usually
referred to as Private Cloud.  OpenNebula supports Hybrid Cloud to
combine local infrastructure with public cloud-based
infrastructure, enabling highly scalable hosting
environments. OpenNebula also supports Public Clouds by providing
Cloud interfaces to expose its functionality for virtual machine,
storage, and network management. Users can find how to use this
software in the <a class="reference external" href="https://portal.futuregrid.org/tutorials/opennebula">OpenNebula tutorial</a>.</dd>
</dl>
</div>
<div class="section" id="high-performance-computing">
<h3>High Performance Computing<a class="headerlink" href="#high-performance-computing" title="Permalink to this headline"></a></h3>
<p>High Performance Computing can be defined as the application of
supercomputing techniques to solve computational problems that are too
large for standard computers or would take too much time. This is one of
the more important features that the scientific community needs&nbsp;to
achieve their projects. Thus, FutureGrid provides users the possibility
of executing their parallel applications or using scientific software. A
guide to accessing HPC services can be found in the&nbsp;<a class="reference external" href="https://portal.futuregrid.org/manual/access">Accessing
FutureGrid</a>&nbsp;and&nbsp;<a class="reference external" href="https://portal.futuregrid.org/manual/hpcservices">HPC
services</a>&nbsp;sections&nbsp;of
the&nbsp;<a class="reference external" href="https://portal.futuregrid.org/manual">User Manual</a>.</p>
</div>
<div class="section" id="storage">
<h3>Storage<a class="headerlink" href="#storage" title="Permalink to this headline"></a></h3>
<p>Users can find different storage systems to cover a wide number of
purposes. In the&nbsp;<a class="reference external" href="https://portal.futuregrid.org/manual/services/storage">Storage
Manual</a>, users
can find the information needed to get access and learn the usage.&nbsp;A
summary of the external storage systems available can be found at:
<a class="reference external" href="http://portal.futuregrid.org/manual/hardware">http://portal.futuregrid.org/manual/hardware</a>.</p>
</div>
<div class="section" id="information-services">
<h3>Information Services<a class="headerlink" href="#information-services" title="Permalink to this headline"></a></h3>
<p>These services gather the information of the different elements that
make up FutureGrid to&nbsp;provide accurate and complete knowledge of the
computational environment.&nbsp;This information is presented using different
web portals:&nbsp;<a class="reference external" href="https://portal.futuregrid.org/status">General System
Status</a>,&nbsp;<a class="reference external" href="http://inca.futuregrid.org/">Cloud, Cluster,
Services Status</a>&nbsp;and&nbsp;<a class="reference external" href="http://noc.futuregrid.org/">Network
Status</a>.</p>
</div>
<div class="section" id="hardware">
<h3>Hardware<a class="headerlink" href="#hardware" title="Permalink to this headline"></a></h3>
<p>FutureGrid includes a geographically distributed set of heterogeneous
computing systems, data management systems, and dedicated networks.
These resources are provided by different institutions across the United
States. Detailed information on the different sites can be
found&nbsp;<a class="reference external" href="https://portal.futuregrid.org/hardware">here</a>&nbsp;and in
the&nbsp;<a class="reference external" href="https://portal.futuregrid.org/manual">User Manual</a>.</p>
</div>
<div class="section" id="support">
<h3>Support<a class="headerlink" href="#support" title="Permalink to this headline"></a></h3>
<p>The best place to start obtaining information about FutureGrid is to
visit our expanding&nbsp;<a class="reference external" href="https://portal.futuregrid.org/manual">FutureGrid User
Manual</a>, of which this page is
a part. To obtain support for FutureGrid, you can visit the&nbsp;<a class="reference external" href="http://kb.iu.edu/index.cgi?searchOptionBtn=KB&amp;search=futuregrid&amp;Search=Search&amp;maxdocs=300">Knowledge
Base</a>,
which includes a collection of FAQs about FutureGrid. We also provide
some user&nbsp;<a class="reference external" href="https://portal.futuregrid.org/forum">Forums</a>. Please
note that some information may already be outdated. We try our best,
however, to keep the manual and the Knowledge Base as up to date as
possible.</p>
<p>If you need help or observe things that are not correct, see
<a class="reference external" href="https://portal.futuregrid.org/help">https://portal.futuregrid.org/help</a>.</p>
</div>
<div class="section" id="id3">
<h3>Acknowledgement<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h3>
<p>You must include in all papers and oral presentations the following
acknowledgment:</p>
<p><em>&#8220;This material is based upon work supported in part by the National
Science Foundation under Grant No. 0910812 to Indiana University for
&#8220;FutureGrid: An Experimental, High-Performance Grid Test-bed.&#8221; Partners
in the FutureGrid project include U. Chicago, U. Florida, San Diego
Supercomputer Center - UC San Diego, U. Southern California, U. Texas at
Austin, U. Tennessee at Knoxville, U. of Virginia, Purdue U., and T-U.
Dresden.&#8221;</em></p>
<p>If you run out of space you can use a shorter version:</p>
<p><em>&#8220;This material is based upon work supported in part by the National
Science Foundation under Grant No. 0910812.&#8221;</em></p>
<p>Additionally, you should forward a copy of each publication or
presentation
to&nbsp;<a class="reference external" href="mailto:help&#37;&#52;&#48;futuregrid&#46;org">https://portal<span>&#46;</span>futuregrid<span>&#46;</span>org/help</a>.
Please use this instead of sending e-mail. This will allow faster
assignment to our support staff.</p>
</div>
</div>
<div class="section" id="getting-started">
<h2>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this headline"></a></h2>
<p>In order for you to gain access to the FutureGrid resources, you need to
follow real simple steps:</p>
<ol class="arabic simple">
<li>Create a FutureGrid portal account</li>
<li>Create or join a project</li>
<li>Upload your SSH key</li>
<li>Explore our Manual</li>
</ol>
<p>We will introduce these steps in more detail next.</p>
<div class="section" id="step-1-create-a-futuregrid-portal-account">
<h3>Step 1: Create a FutureGrid Portal Account<a class="headerlink" href="#step-1-create-a-futuregrid-portal-account" title="Permalink to this headline"></a></h3>
<p>In order to utilize&nbsp;<strong>any</strong>&nbsp;FutureGrid resource, you must posses a
FutureGrid&nbsp;<strong>portal account</strong>. Thus,&nbsp;<em>apply for your&nbsp;**portal
account&nbsp;**</em>before you attempt anything else.&nbsp;This account is used to
gather some information that we will use in the next steps. You must
make sure that the information is complete before you proceed to the
second step. &nbsp;FutureGrid performs basic verification of the information
you provide when creating an account, so it may take a little while
before your account is approved. Once you have a portal account, please
proceed.</p>
<p>Please note that you cannot access FutureGrid resources until you
complete the next steps.</p>
<p>Here are a few tips that make it easy for you</p>
<ul class="simple">
<li>On the portal&#8217;s main page at
<a class="reference external" href="https://portal.futuregrid.org">https://portal.futuregrid.org</a>
appears
a&nbsp;<a class="reference external" href="https://portal.futuregrid.org/user/register%20">**Register**</a>&nbsp;Link.</li>
<li>Following you will be able to&nbsp;<strong>Create a new account</strong>&nbsp;on the
portal.</li>
<li>Fill in <strong>ALL&nbsp;</strong>fields as much as you can.</li>
<li>Note that fields with *&nbsp;are mandatory</li>
<li>It is important that you specify your address information completely.</li>
<li>If you are a graduate or undergraduate student please fill out your
advisors contact information in the field specially dedicated for it.
If he has a FutureGrid Portal name, please also add his portal name
if you know it in that field.</li>
<li>If you have an e-mail address from your institution, we ask that you
use this address instead of one from gmail, hotmail, or other e-mail
services that we cannot trace back to your name or institution.</li>
<li>Usage of all non institutional addresses will prolong the application
process.</li>
<li>Please note that creating a portal account does not give you access
to any FutureGrid resources, for that you have to complete step 2 and
3.</li>
<li>Please remember that checking your information will take time. Thus
we recommend that you wait&nbsp;till you get a message that tells you that
your portal account has been approved. Then continue to The next
step. We are not conducting any portal approval outside of 10am-4pm
EST. If you are easily to be identified your approval will take 1-2
days, if not, we have either problems verifying your data or
something else is not right. In case you appear to be a spammer we
will not notify you.</li>
<li>In case you are teaching a class class we have some special
instructions for you and after you apply for a portal account you may
want to get in contact with us via the <a class="reference external" href="https://portal.futuregrid.org/help">help
form</a></li>
</ul>
</div>
<div class="section" id="step-2-create-a-new-project-or-join-an-existing-one">
<h3>Step 2: Create a new Project or join an existing one<a class="headerlink" href="#step-2-create-a-new-project-or-join-an-existing-one" title="Permalink to this headline"></a></h3>
<p>You need to either apply for a new FutureGrid project or join an
existing project to use FutureGrid resources. To apply for a new
project, fill out the <a class="reference external" href="https://portal.futuregrid.org/node/add/fg-projects">project creation
form</a>. To join an
existing project, ask the project lead or project manager for that
project to add you to their project using that same form. If the project
is set to &#8220;accept public join request&#8221;, you may also send a request in
the portal. To do this, first view the <a class="reference external" href="https://portal.futuregrid.org/projects">project
list</a>and go to the project
detail page by clicking the project title. If the project is set by the
project lead to &#8220;accept join request&#8221;, then you&#8217;ll see a large gray
&#8216;Join this project&#8217; button in the upper right corner of the page. Click
the button to send the join request to the project lead and manager so
they can process your request.</p>
<p>Once you have been approved to work on a project, you will be able to
access the resources and services that your project has requested and
been authorized to use. See the <a class="reference external" href="https://portal.futuregrid.org/node/add/fg-projects">project creation
form</a> for a list
of FutureGrid resources and services.</p>
<p>Here are a view links that may help you:</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/node/add/fg-projects">Create a new project creation
form</a></li>
<li><a class="reference external" href="https://portal.futuregrid.org/projects">See the project&nbsp;list to identify the project you like to
join</a></li>
<li><a class="reference external" href="https://portal.futuregrid.org/faq">Read some FAQs about the account
creation</a></li>
</ul>
</div>
<div class="section" id="step-3-upload-your-ssh-public-key-s">
<h3>Step 3: Upload Your SSH Public Key(s)<a class="headerlink" href="#step-3-upload-your-ssh-public-key-s" title="Permalink to this headline"></a></h3>
<p>In order to be able to log into the started VMs, among other purposes,
you need to provide FG with a secure-shell (ssh) public key.&nbsp;If you are
already a frequent user of ssh, and have a private and public key pair,
it is perfectly reasonable to provide your public key. It&#8217;s&nbsp;<em>public</em>,
after all.</p>
<p>To upload the chosen public key:</p>
<ol class="arabic simple">
<li>Copy your public identity into your system clipboard.</li>
<li>Log into the FG portal&nbsp;<a class="reference external" href="../../">https://portal.futuregrid.org/</a></li>
<li>In the&nbsp;<strong>Accounts</strong>&nbsp;menu, select&nbsp;<strong>My Portal and HPC
Account</strong>&nbsp;page. and activate the panel saying <strong>SSH keys</strong></li>
<li>Click the link that says Add a public key.</li>
</ol>
<p>This step should be fairly instantaneous.</p>
<p>If you are not familiar with ssh key generation, or&nbsp;if you have
difficulty generating a key pair, please inform yourself about ssh keys
or contact us via our <a class="reference external" href="https://portal.futuregrid.org/help">help form</a>
. Detailed instructions on how to generate ssh key pairs will be added
to this document in the near future.</p>
<p>Step 4: Explore the Documentation</p>
<hr class="docutils" />
<p>Once you have access to FutureGrid resources, a good place to start
learning about how to use FutureGrid are the tutorials, specifically the
following:</p>
<ul class="simple">
<li>The&nbsp;<a class="reference external" href="http://portal.futuregrid.org/manual">Manual</a> for detailed
information</li>
</ul>
<p>Our manual will include a variety of topics that are of interest to our
users from many different user communities. It would be a disservice to
you to just list a view of them. Hence we provide you with a convenient
link to our table of contents. Please note that the manual also contains
links to our tutorials and our MOOC.</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/manual/toc">Manual Table of
Contents</a></li>
</ul>
<p>It will be also easy for you to search for some topics in our search box
at the top of the Web page.</p>
</div>
</div>
<div class="section" id="accessing-futuregrid">
<h2>Accessing FutureGrid<a class="headerlink" href="#accessing-futuregrid" title="Permalink to this headline"></a></h2>
<p>To use FutureGrid&nbsp;you must be part of a valid &#8220;project&#8221;. Project
leaders&nbsp;are requested to fill out project applications about the use of
FutureGrid. The
<a class="reference external" href="https://portal.futuregrid.org/node/add/fg-projects">form</a> gathers
some important information about their projects to be conducted. At this
time this information is publicly shared. This information is used to
report and document not only to us but also to our sponsors which
activities are conducted on FutureGrid. The more precise you are in your
descriptions and filling out the forms the better we can highlight your
project. Once a project is formed, project members can join a project.
This must be conducted by the project lead. &nbsp;A user retains an active
account on FutureGrid when they are in at least one active project. A
user that is inactive does not have to apply for a new account, but
instead apply for a new project. Once that project is activated the user
account becomes active.</p>
<div class="section" id="account-management-service">
<h3>Account Management Service<a class="headerlink" href="#account-management-service" title="Permalink to this headline"></a></h3>
<p>Please note the current process of applying for account may change.</p>
<ol class="arabic simple">
<li>Any user can apply easily for a Portal account: Please go to<ul>
<li><a class="reference external" href="http://portal.futuregrid.org/user/register">https://portal.futuregrid.org/user/register</a></li>
<li>it may take a day or two to get a portal account. Portal
accounts&nbsp;will not be created over the weekend.</li>
</ul>
</li>
<li>Once logged in the user has a couple of options<ul>
<li><a class="reference external" href="https://portal.futuregrid.org/manage-my-portal-account">User Profile
Management</a>:
Update information regarding the user profile</li>
<li>Project Management: apply for <a class="reference external" href="https://portal.futuregrid.org/node/add/fg-projects">new
projects</a>,
join existing projects, update information and results of a
project, manage members and roles of users participating in your
projects</li>
<li>Managing Certificates and Keys: Integrate OpenID login for the
portal (with for example your google ID), manage your ssh key for
access to the HPC service, Manage Nimbus and Eucalyptus accounts
and keys</li>
</ul>
</li>
</ol>
<div class="section" id="apply-for-a-project-or-request-an-account">
<h4>Apply for a Project or Request an Account<a class="headerlink" href="#apply-for-a-project-or-request-an-account" title="Permalink to this headline"></a></h4>
<p>To get access to FutureGrid (FG), you need to register your project in
it. Please go to:</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/node/add/fg-projects">https://portal.futuregrid.org/node/add/fg-projects</a></li>
</ul>
<p>to apply for a new project. It may take some time for your project to
get approved. No projects will be approved over the weekend.</p>
<p>In case you like to join an existing project, please find the list of
projects at:</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/projects">https://portal.futuregrid.org/projects</a></li>
</ul>
<p>Clicking on a project title will bring you to the project information
page, where you may see a large gray button on the upper right corner of
the page named &#8216;Join this project&#8217;. Click to send join request to that
project. Please notice the PI of the project has to approve you before
you are part of that project.</p>
<p>Some project may not accept external members so there would be no join
button for them. Please notice this behavior is controlled by the owner
of the project.</p>
<p>You can always communicate with the project PI through external channel
of the FutureGrid portal if you know him/her in person by letting
him/her know your portal username. He/she can then directly add you as a
member to his/her project without going through the join request process
through the portal.</p>
<div class="section" id="implicit-project-responsibilities-for-project-members-and-pi-agreement-for-reporting">
<h5>Implicit Project Responsibilities for Project Members and PI Agreement for Reporting<a class="headerlink" href="#implicit-project-responsibilities-for-project-members-and-pi-agreement-for-reporting" title="Permalink to this headline"></a></h5>
<p>The Project PI has agreed to certain reporting requirements to provide
information to FutureGrid. He will be responsible to make sure that they
are completed and also implemented with the users joining the project.
Thus the user is responsible to comply with the terms of the project in
regards to reporting and acknowledgements in case of publications. Each
project PI has the responsibility to communicate such requirements to
the members and managers. The project agreements overwrite the
individuals agreement.</p>
</div>
</div>
<div class="section" id="cloud-accounts">
<h4>Cloud Accounts<a class="headerlink" href="#cloud-accounts" title="Permalink to this headline"></a></h4>
<p>The cloud accounts are handled currently separately from the HPC account
creation process. In fact the Eucalyptus clouds are disjunctive on each
machine and have their own user management. For Nimbus uploading your
ssh key is sufficient. The turnaround time for you getting access to the
system is typically between 30 minutes and one day.</p>
<p>Please be kind and only apply for these cloud accounts if you really
need them.</p>
</div>
</div>
<div class="section" id="key-reset-or-adding-new-keys">
<h3>Key Reset or Adding new Keys<a class="headerlink" href="#key-reset-or-adding-new-keys" title="Permalink to this headline"></a></h3>
<p>To reset or add a new you ssh key, please update your keys first by
visiting the page</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/user/23/ssh-keys">https://portal.futuregrid.org/user/23/ssh-keys</a></li>
</ul>
<p>and change the keys as needed. Your reset will take 30 minutes to one
day to propagate through the system services.</p>
<p>You are not allowed to use password less keys. &nbsp;Your account may be
deactivated.</p>
</div>
</div>
<div class="section" id="account-creation">
<h2>Account Creation<a class="headerlink" href="#account-creation" title="Permalink to this headline"></a></h2>
<div class="section" id="creating-an-hpc-account">
<h3>Creating an HPC account<a class="headerlink" href="#creating-an-hpc-account" title="Permalink to this headline"></a></h3>
<p>All you need to do to obtain an HPC account is to be in a valid project
and upload your ssh key. Typically you will get the account within one
business day. There will be typically no accounts approved in non
business hours including weekends.</p>
<p>Please add your SSH keys in your profile which you can find at:</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/user/23/ssh-keys">https://portal.futuregrid.org/user/23/ssh-keys</a></li>
</ul>
<p>Once you have done that, please go back to this form and complete it.
After submission, this form will create an e-mail request to FutureGrid.
The FG administrators may contact you to verify this request.</p>
<dl class="docutils">
<dt>Please, make sure you are a member of a valid project.</dt>
<dd>Project IDs can be found at</dd>
</dl>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/projects">https://portal.futuregrid.org/projects</a></li>
</ul>
<p>The project lead is responsible for determining if you can be added to
the project.</p>
<p>Note: Do not send mail to FG staff members about joining a project, as
we are not managing individual project memberships</p>
<p>To view your current memberships and status of your account application,
please visit</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/manage-my-portal-account">https://portal.futuregrid.org/manage-my-portal-account</a></li>
</ul>
</div>
<div class="section" id="resetting-a-ssh-key">
<h3>Resetting a ssh-key<a class="headerlink" href="#resetting-a-ssh-key" title="Permalink to this headline"></a></h3>
<p>Simply visit the ssh-key page and upload a new key. YoU can optionally
delete other keys if you do n longer need them.</p>
</div>
<div class="section" id="nimbus-eucalyptus-openstack">
<h3>Nimbus, Eucalyptus, OpenStack<a class="headerlink" href="#nimbus-eucalyptus-openstack" title="Permalink to this headline"></a></h3>
<p>Once you have uploaded your SSH key, the Nimbus, Eucalyptus, and
OpenStack access will be granted. Please visit the manual pages for more
details on accessing them.</p>
</div>
</div>
<div class="section" id="help-and-support">
<h2>Help and Support<a class="headerlink" href="#help-and-support" title="Permalink to this headline"></a></h2>
<div class="section" id="id4">
<h3>Help and Support<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h3>
<p>FutureGrid provides help and support to its users through various
services. These services include a ticket system, mailinglists&nbsp;&nbsp;(soon to
be replaced by a user forums), manuals, a KnowledgeBase, and an
educational outreach activities developing tutorials and other
educational material.</p>
</div>
<div class="section" id="manual">
<h3>Manual<a class="headerlink" href="#manual" title="Permalink to this headline"></a></h3>
<p>We are creating gradually a manual for FutureGrid. This manual contains
contributions from the FG staff and FG users. The link for the manual is</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/manual">https://portal.futuregrid.org/manual</a></li>
</ul>
<p>In case you have a useful contribution, you can simply comment on each
page, or if you like to create a chapter to be included, please create a
community page.&nbsp;&nbsp;A community page can be created by clicking on the
following link</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/node/add/page-community">Create a
Community&nbsp;Page</a></li>
</ul>
<p>Community pages can also be used to report on elaborate experiment
results. Once you are done with your contribution, please send a ticket
to us so we can make it more prominently visible in the portal:</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/help">https://portal.futuregrid.org/help</a></li>
</ul>
</div>
<div class="section" id="user-forum">
<h3>User Forum<a class="headerlink" href="#user-forum" title="Permalink to this headline"></a></h3>
<p>Based on advice from other projects we have established a number of user
forums that are available while following this link</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/forum">User Forum</a></li>
</ul>
<p>To post to the forum, you must create a portal account.</p>
</div>
<div class="section" id="help-ticketing-system">
<h3>Help Ticketing System<a class="headerlink" href="#help-ticketing-system" title="Permalink to this headline"></a></h3>
<p>To create a ticket for FutureGrid support, please use out ticket form
from the web page at</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/help">https://portal.futuregrid.org/help</a></li>
</ul>
<p>in which you describe problems like feature request, or bug report. A
ticket will be automatically generated and you&#8217;ll receive an auto-reply
mail with the ticket number for further reference. FutureGrid staff will
be looking into the ticket and working on it. You will receive updates
and resolution through email upon the completion of the investigation.
Please be aware that feedback may not be instantaneous. Tickets you have
submitted can be seen at</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/tickets">https://portal.futuregrid.org/tickets</a></li>
</ul>
</div>
<div class="section" id="training-and-education">
<h3>Training and Education<a class="headerlink" href="#training-and-education" title="Permalink to this headline"></a></h3>
<p>FutureGrid provides training and educational materials through manuals
and tutorials.</p>
<p>FutureGrid leverages technologies contributed by its partners and by the
open-source community in the packaging, configuration, and deployment of
virtual clusters - including the plug-and-play, self-configuring Grid
appliance, the Nimbus science cloud middleware, the IPOP/GroupVPN
self-configuring virtual network, and the ViNe virtual network.</p>
<p>For links to the initial appliance-based FutureGrid tutorials, see the
FutureGrid&nbsp;<a class="reference external" href="https://portal.futuregrid.org/outreach">Education and Outreach
page</a>. Additionally, for
video tutorials on the use of the appliances, see the&nbsp;<a class="reference external" href="http://www.youtube.com/acisp2p#p/c/D77781CEF51F72F3">Grid appliance
YouTube
channel</a>.</p>
<p><a class="reference external" href="https://portal.futuregrid.org/gettingstarted"> Quickstart</a></p>
</div>
</div>
<div class="section" id="guide-to-using-the-futuregrid-portal">
<h2>Guide to Using the FutureGrid Portal<a class="headerlink" href="#guide-to-using-the-futuregrid-portal" title="Permalink to this headline"></a></h2>
<div class="section" id="functions-of-the-futuregrid-portal">
<h3>Functions of the FutureGrid Portal<a class="headerlink" href="#functions-of-the-futuregrid-portal" title="Permalink to this headline"></a></h3>
<div class="line-block">
<div class="line"><br /></div>
<div class="line-block">
<div class="line">The FutureGrid portal aims to:</div>
</div>
</div>
<ol class="arabic simple">
<li>Be the definite&nbsp;source for information about FutureGrid (manuals,
papers, forums, FAQ, ...)</li>
<li>Allow management of your FG accounts (portal, services, and
resources)</li>
<li>Allow management of your futuregrid projects</li>
<li>Allow management of FG experiments</li>
<li>Allow the dynamic provisioning via RAIN</li>
</ol>
</div>
<div class="section" id="a-futuregrid-user-dashboard">
<h3>A FutureGrid&nbsp;User Dashboard<a class="headerlink" href="#a-futuregrid-user-dashboard" title="Permalink to this headline"></a></h3>
<p>After login, you will be redirected to a dashboard-like page (go to&nbsp;the
menu Accounts -&gt; My Portal Account), where you will see the following:</p>
<ol class="arabic simple">
<li>A list of useful links, including links to profile, account, SSH key,
and OpenID management information.</li>
<li>A projects summary section that lists the summarized information
about projects that you are the owner of, that you manage, that you
are member of, and that you support as a FutureGrid expert,
respectively. Clicking the project title will bring you to the
project detail info page. For those projects that you own or manage,
an &#8216;edit&#8217; link is also there so you can quickly update the project
information.</li>
<li>A &#8216;My Content&#8217; section where the content that you are responsible for
maintaining (and/or that you have contributed) is listed. This gives
a convenient view so you can easily go back to the content and update
it.</li>
<li>A &#8216;My Publications&#8217; section that lists your publications.</li>
</ol>
</div>
<div class="section" id="update-project-information-and-add-results">
<h3>Update Project Information and Add Results<a class="headerlink" href="#update-project-information-and-add-results" title="Permalink to this headline"></a></h3>
<p>Another frequently used feature is the ability to update your project
information (e.g., add project members) and fill in results in the
&#8216;Project Results&#8217; section. You can do so only when you are either the
owner of the project or the project manager (if the owner has delegated
that to you). By following the links provided in the &#8216;Dashboard&#8217;
section, you can review and edit the project information.</p>
<p>To add a user to your project as a member, the user must have a
FutureGrid&nbsp;portal account first. Then, while editing the project, you
can type a user&#8217;s first name, and the suggestion feature will pop up
with the user&#8217;s username to be added. If you have many members to add,
click the &#8216;Add another item&#8217; in the &#8216;Project Members&#8217; section to add
more. See also <a class="reference external" href="https://portal.futuregrid.org/how-can-i-add-people-project">this
FAQ</a>.</p>
<p>For updating your project results, there is a &#8216;Project Results&#8217; section
with a &#8216;Results&#8217; window that supports WYSIWYG editing. It supports
simple formatted text, embedded images, etc. For text, you can edit
directly in the window, or copy the content you developed in your
favorite editor and paste the content in the edit window. In the case of
images, you&#8217;ll need to upload the image to the server first, and then
insert it to the window, or alternatively refer to an external URL for
an image hosted somewhere else.</p>
<p>For more detailed info on how to include an image, please see <a class="reference external" href="https://portal.futuregrid.org/how-upload-andor-include-image-while-creating-pagenews-etc">this
FAQ</a>.</p>
</div>
<div class="section" id="contribute-to-the-futuregrid-community">
<h3>Contribute to the FutureGrid Community<a class="headerlink" href="#contribute-to-the-futuregrid-community" title="Permalink to this headline"></a></h3>
<p>The FutureGrid portal also provides its users a place where they can
contribute to the community by sharing their ideas, research topics, FG
experience, etc.; in this way people can learn from you, and also you
can learn from others. Emphasizing user participation and
collaboration&nbsp;is one of the main goals shaping the portal to its current
state.</p>
<p>You can contribute by <a class="reference external" href="https://portal.futuregrid.org/node/add/page-community">creating a &#8216;Community
Page&#8217;</a>. You
can find the link in the left side navigation block, under the &#8216;Create
content&#8217; menu. After entering the edit page, you&#8217;ll see a &#8216;Title&#8217; text
box, where you put the content/article title, and a &#8216;Body&#8217; window where
you put the content. Once again, it supports formatted text and embedded
images, etc.</p>
<p>You can cite FutureGrid references also, by enclosing a citekey within
the &#8216;bib&#8217; tag as stated under the editor window (NOTE: Please use &#8216;[]&#8217;
instead of &#8216;&lt;&gt;&#8217;), where CITEKEY could be found in the <a class="reference external" href="https://portal.futuregrid.org/biblio">biblio
page</a>&nbsp;(the content within but
not including the &#8216;[ ]&#8217;).</p>
<p>An example of a user contributed page can be found
<a class="reference external" href="https://portal.futuregrid.org/contrib/testexample-page-user-contributed-page">here</a>.</p>
</div>
<div class="section" id="file-upload-and-attachment-to-a-page">
<h3>File Upload and Attachment to a Page<a class="headerlink" href="#file-upload-and-attachment-to-a-page" title="Permalink to this headline"></a></h3>
<p>Please see <a class="reference external" href="https://portal.futuregrid.org/faq/how-uploadattach-file-page">this
FAQ</a>
for instructions on file upload.</p>
</div>
</div>
<div class="section" id="id5">
<h2>Hardware<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h2>
<p>These are the current resources available in FutureGrid.</p>
<div class="section" id="compute-resources">
<h3>Compute Resources<a class="headerlink" href="#compute-resources" title="Permalink to this headline"></a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="33%" />
<col width="14%" />
<col width="8%" />
<col width="7%" />
<col width="8%" />
<col width="7%" />
<col width="8%" />
<col width="10%" />
<col width="6%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Name</strong></td>
<td><strong>System Type</strong></td>
<td><strong># Nodes</strong></td>
<td><strong># CPUS</strong></td>
<td><strong># Cores</strong></td>
<td><strong>TFLOPS</strong></td>
<td><strong>RAM (GB</strong></td>
<td><strong>Storage (TB)</strong></td>
<td><strong>Site</strong></td>
</tr>
<tr class="row-even"><td><a class="reference external" href="https://portal.futuregrid.org/hardware/india">india</a></td>
<td>IBM iDataplex</td>
<td>128</td>
<td>256</td>
<td>1024</td>
<td>11</td>
<td>3072</td>
<td>335</td>
<td>IU</td>
</tr>
<tr class="row-odd"><td><a class="reference external" href="https://portal.futuregrid.org/hardware/hotel">hotel</a></td>
<td>IBM iDataplex</td>
<td>84</td>
<td>168</td>
<td>672</td>
<td>7</td>
<td>2016</td>
<td>120</td>
<td>UC</td>
</tr>
<tr class="row-even"><td><a class="reference external" href="https://portal.futuregrid.org/hardware/sierra">sierra</a></td>
<td>IBM iDataplex</td>
<td>84</td>
<td>168</td>
<td>672</td>
<td>7</td>
<td>2688</td>
<td>96</td>
<td>SDSC</td>
</tr>
<tr class="row-odd"><td><a class="reference external" href="https://portal.futuregrid.org/hardware/foxtrot">foxtrot</a></td>
<td>IBM iDataplex</td>
<td>32</td>
<td>64</td>
<td>256</td>
<td>3</td>
<td>768</td>
<td>0</td>
<td>UF</td>
</tr>
<tr class="row-even"><td><a class="reference external" href="https://portal.futuregrid.org/hardware/alamo">alamo</a></td>
<td>Dell Poweredge</td>
<td>96</td>
<td>192</td>
<td>768</td>
<td>8</td>
<td>1152</td>
<td>30</td>
<td>TACC</td>
</tr>
<tr class="row-odd"><td><a class="reference external" href="https://portal.futuregrid.org/hardware/xray">xray</a></td>
<td>Cray XT5m</td>
<td>1</td>
<td>166</td>
<td>664</td>
<td>6</td>
<td>1328</td>
<td>5.4</td>
<td>IU</td>
</tr>
<tr class="row-even"><td><a class="reference external" href="https://portal.futuregrid.org/hardware/bravo">bravo</a></td>
<td>HP Proliant</td>
<td>16</td>
<td>32</td>
<td>128</td>
<td>1.7</td>
<td>3072</td>
<td>128</td>
<td>IU</td>
</tr>
<tr class="row-odd"><td><a class="reference external" href="https://portal.futuregrid.org/hardware/delta">delta</a></td>
<td>GPU Cluster(SuperMicro)</td>
<td>16</td>
<td>32</td>
<td>192</td>
<td>&nbsp;</td>
<td>1333</td>
<td>144</td>
<td>IU</td>
</tr>
<tr class="row-even"><td><a class="reference external" href="https://portal.futuregrid.org/hardware/lima">lima</a></td>
<td>Aeon Eclipse64</td>
<td>8</td>
<td>16</td>
<td>128</td>
<td>1.3</td>
<td>512</td>
<td><dl class="first last docutils">
<dt>3.8 (SSD)</dt>
<dd>8.0 (HDD)</dd>
</dl>
</td>
<td>SDSC</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="storage-resources">
<h3>Storage Resources<a class="headerlink" href="#storage-resources" title="Permalink to this headline"></a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="26%" />
<col width="32%" />
<col width="26%" />
<col width="16%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>System Type</strong></td>
<td><strong>Capacity (TB)</strong></td>
<td><strong>File System</strong></td>
<td><strong>Site</strong></td>
</tr>
<tr class="row-even"><td><dl class="first last docutils">
<dt>DDN 9550</dt>
<dd>Data Capacitor</dd>
</dl>
</td>
<td><dl class="first last docutils">
<dt>339 shared with IU +</dt>
<dd>16 TB dedicated</dd>
</dl>
</td>
<td>Luster</td>
<td>IU</td>
</tr>
<tr class="row-odd"><td>DDN 6620</td>
<td>120</td>
<td>GPFS</td>
<td>UC</td>
</tr>
<tr class="row-even"><td>SunFire x4540</td>
<td>96</td>
<td>ZFS</td>
<td>SDSC</td>
</tr>
<tr class="row-odd"><td>Dell MD3000</td>
<td>30</td>
<td>NFS</td>
<td>TACC</td>
</tr>
<tr class="row-even"><td>IBM dx360 M3</td>
<td>24</td>
<td>NFS</td>
<td>UF</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="status">
<h3>Status<a class="headerlink" href="#status" title="Permalink to this headline"></a></h3>
<p>The status of the resources can be monitored through:</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/status">https://portal.futuregrid.org/status</a></li>
</ul>
<p>This page also contains a link to our operational status of FutureGrid
resources and services, see the FutureGrid Inca server at:</p>
<ul class="simple">
<li><a class="reference external" href="http://inca.futuregrid.org/">http://inca.futuregrid.org</a></li>
</ul>
</div>
<div class="section" id="maintenance">
<h3>Maintenance<a class="headerlink" href="#maintenance" title="Permalink to this headline"></a></h3>
<p>Activities during the regular maintenance window for FutureGrid machines
are listed in the FutureGrid wiki at:</p>
<ul class="simple">
<li><a class="reference external" href="https://wiki.futuregrid.org/index.php/Hw:MaintActivities">https://wiki.futuregrid.org/index.php/Hw:MaintActivities</a>.</li>
</ul>
<p>Scheduled outages and maintenance windows on the FutureGrid network are
collected in the Network Operations Calendar at:</p>
<ul class="simple">
<li><a class="reference external" href="https://noc.futuregrid.org/futuregrid/support/operations-calendar3.html">https://noc.futuregrid.org/futuregrid/support/operations-calendar3.html</a></li>
</ul>
</div>
</div>
<div class="section" id="id6">
<h2>Compute Resources<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h2>
<p>GVL: This section belongs to HPC Services</p>
<p>Compute Hardware Resources &amp; HPC Job Queue Information</p>
<table border="1" class="docutils">
<colgroup>
<col width="12%" />
<col width="14%" />
<col width="27%" />
<col width="23%" />
<col width="23%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Resource</th>
<th class="head">Queue name</th>
<th class="head">Default Wallclock Limit</th>
<th class="head">Max Wallclock Limit</th>
<th class="head">NOTES</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>india</td>
<td>batch</td>
<td>4 hours</td>
<td>24 hours</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>&nbsp;</td>
<td>long</td>
<td>8 hours</td>
<td>168 hours</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>&nbsp;</td>
<td>scalemp</td>
<td>8 hours</td>
<td>168 hours</td>
<td>restricted access</td>
</tr>
<tr class="row-odd"><td>&nbsp;</td>
<td>b534</td>
<td>none</td>
<td>none</td>
<td>restricted access</td>
</tr>
<tr class="row-even"><td>&nbsp;</td>
<td>ajyounge</td>
<td>none</td>
<td>none</td>
<td>restricted access</td>
</tr>
<tr class="row-odd"><td>sierra</td>
<td>batch</td>
<td>4 hours</td>
<td>24 hours</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>&nbsp;</td>
<td>long</td>
<td>8 hours</td>
<td>168 hours</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>hotel</td>
<td>extended</td>
<td>none</td>
<td>none</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>alamo</td>
<td>shortq</td>
<td>none</td>
<td>24 hours</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>&nbsp;</td>
<td>longq</td>
<td>none</td>
<td>24 hours</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>foxtrot</td>
<td>batch</td>
<td>1 hour</td>
<td>none</td>
<td>not for general use</td>
</tr>
</tbody>
</table>
<p>For availability, please visit:
<a class="reference external" href="http://portal.futuregrid.org/status">http://portal.futuregrid.org/status</a>
.</p>
<p>GVL: This image belongs to the specific cluster desciption:</p>
<p><img alt="image4" src="https://portal.futuregrid.org/sites/default/files/resize/images/FutureGrid_iDataPlex_Cray_IU-sm-640x425.jpg" /></p>
<p>IU iDataplex &amp; Cray</p>
<p>GVL: This section belongs to the specific cluster description:</p>
<p><img alt="image5" src="https://portal.futuregrid.org/sites/default/files/resize/images/Cray_XT5m_Front_closed-small-427x640.jpg" /></p>
<p>IU Cray</p>
<p>GVL: This belongs to networking:</p>
<div class="section" id="compute-networks">
<h3>Compute Networks<a class="headerlink" href="#compute-networks" title="Permalink to this headline"></a></h3>
<p><strong>Resource Name</strong></p>
<p><strong>Network Devices</strong></p>
<p>IU Cray</p>
<p>Cray 2D Torus SeaStar</p>
<p>IU iDataPlex</p>
<p>DDR IB</p>
<p>QLogic switch with Mellanox ConnectX adapters</p>
<p>Blade Network Technologies &amp; Force10 Ethernet switches</p>
<p>SDSC</p>
<p>DDR IB</p>
<p>Cisco switch with Mellanox ConnectX adapters</p>
<p>Juniper Ethernet switches</p>
<p>TACC</p>
<p>QDR IB</p>
<p>Mellanox switches and adapters</p>
<p>Dell Ethernet switches</p>
<p>UC</p>
<p>DDR IB</p>
<p>QLogic switch with Mellanox ConnectX adapters</p>
<p>Blade Network Technologies &amp; Juniper switches</p>
<p>UF</p>
<p>Ethernet only (Blade Network Technologies &amp; Force10 switches)</p>
<ul class="simple">
<li></li>
</ul>
</div>
</div>
<div class="section" id="alamo">
<h2>Alamo<a class="headerlink" href="#alamo" title="Permalink to this headline"></a></h2>
<p><em>Texas Advanced Computing Center</em></p>
<p>Alamo consists of 96 nodes with dual Intel&nbsp;X5550&nbsp;processors and 1152
GB of memory. Following are detailed specifications for Alamo.</p>
<table border="1" class="docutils">
<colgroup>
<col width="51%" />
<col width="49%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Machine Type</td>
<td>Cluster</td>
</tr>
<tr class="row-even"><td>System Type</td>
<td>Dell PowerEdge&nbsp;M610&nbsp;Blade</td>
</tr>
<tr class="row-odd"><td>CPU type</td>
<td>Intel&nbsp;Xeon&nbsp;X5550</td>
</tr>
<tr class="row-even"><td>Host Name</td>
<td>alamo</td>
</tr>
<tr class="row-odd"><td>CPU Speed</td>
<td>2.66GHz</td>
</tr>
<tr class="row-even"><td>Number of CPUs</td>
<td>192</td>
</tr>
<tr class="row-odd"><td>Number of nodes</td>
<td>96</td>
</tr>
<tr class="row-even"><td>RAM</td>
<td>12 GB&nbsp;DDR3&nbsp;1333Mhz</td>
</tr>
<tr class="row-odd"><td>Total RAM (GB)</td>
<td>1152</td>
</tr>
<tr class="row-even"><td>Number of cores</td>
<td>768</td>
</tr>
<tr class="row-odd"><td>Operating System</td>
<td>Linux</td>
</tr>
<tr class="row-even"><td>Tflops</td>
<td>8</td>
</tr>
<tr class="row-odd"><td>Disk Size (TB)</td>
<td>48</td>
</tr>
<tr class="row-even"><td>Hard Drives</td>
<td>500 GB Internal 7200 RPM&nbsp;SAS Drive</td>
</tr>
<tr class="row-odd"><td>Primary storage, shared by all nodes</td>
<td>NFS</td>
</tr>
<tr class="row-even"><td>Connection configuration</td>
<td>Mellanox&nbsp;4x&nbsp;QDR&nbsp;InfiniBand&nbsp;adapters</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="bravo">
<h2>Bravo<a class="headerlink" href="#bravo" title="Permalink to this headline"></a></h2>
<p><em>Indiana University, Bloomington</em></p>
<p>Bravo consists of 16 nodes with dual Intel Xeon E5620 processors and
192 GB of memory. Following are detailed specifications for Bravo:</p>
<table border="1" class="docutils">
<colgroup>
<col width="51%" />
<col width="49%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Machine Type</td>
<td>Cluster</td>
</tr>
<tr class="row-even"><td>System Type</td>
<td>HP Proliant</td>
</tr>
<tr class="row-odd"><td>CPU type</td>
<td>Intel Xeon E5620</td>
</tr>
<tr class="row-even"><td>Host Name</td>
<td>bravo</td>
</tr>
<tr class="row-odd"><td>CPU Speed</td>
<td>2.40GHz</td>
</tr>
<tr class="row-even"><td>Number of CPUs</td>
<td>32</td>
</tr>
<tr class="row-odd"><td>Number of nodes</td>
<td>16</td>
</tr>
<tr class="row-even"><td>RAM</td>
<td>192 GB DDR3 1333Mhz</td>
</tr>
<tr class="row-odd"><td>Total RAM (GB)</td>
<td>3072</td>
</tr>
<tr class="row-even"><td>Number of cores</td>
<td>128</td>
</tr>
<tr class="row-odd"><td>Operating System</td>
<td>Linux</td>
</tr>
<tr class="row-even"><td>Tflops</td>
<td>1.7</td>
</tr>
<tr class="row-odd"><td>Hard Drives</td>
<td>6x2TB Internal 7200 RPM SATA Drive</td>
</tr>
<tr class="row-even"><td>Primary storage, shared by all nodes</td>
<td>NFS</td>
</tr>
<tr class="row-odd"><td>Connection configuration</td>
<td>Mellanox 4x&nbsp;DDR InfiniBand&nbsp;adapters</td>
</tr>
</tbody>
</table>
<p><a class="reference external" href="https://portal.futuregrid.org/manual/bravo">Instructions for submitting jobs in
bravo.</a></p>
</div>
<div class="section" id="delta">
<h2>Delta<a class="headerlink" href="#delta" title="Permalink to this headline"></a></h2>
<p><em>Indiana University, Bloomington</em></p>
<p>Delta consists of 16 nodes with two core Intel X5560 processors and
192 GB of memory. Each node supports two nVIDIA Tesla C2070 GPUs.</p>
<table border="1" class="docutils">
<colgroup>
<col width="53%" />
<col width="47%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Machine Type</td>
<td>Cluster</td>
</tr>
<tr class="row-even"><td>CPU type</td>
<td>Intel Xeon&nbsp;5660</td>
</tr>
<tr class="row-odd"><td>Host Name</td>
<td>delta</td>
</tr>
<tr class="row-even"><td>CPU Speed</td>
<td>2.80 GHz</td>
</tr>
<tr class="row-odd"><td>CPUs (cores) per node</td>
<td>2 (12)</td>
</tr>
<tr class="row-even"><td>Number of nodes</td>
<td>16</td>
</tr>
<tr class="row-odd"><td>GPU type</td>
<td>nVIDIA Tesla C2070</td>
</tr>
<tr class="row-even"><td>GPUs per node</td>
<td>2</td>
</tr>
<tr class="row-odd"><td>RAM</td>
<td>16 GB DDR3 1333 Mhz</td>
</tr>
<tr class="row-even"><td>Memory per node [GB]</td>
<td>192</td>
</tr>
<tr class="row-odd"><td>Total CPUs (cores)</td>
<td>32 (192)</td>
</tr>
<tr class="row-even"><td>Total Memory [GB]</td>
<td>3072</td>
</tr>
<tr class="row-odd"><td>Total GPUs</td>
<td>32</td>
</tr>
<tr class="row-even"><td>Cores per GPU</td>
<td>448</td>
</tr>
<tr class="row-odd"><td>Operating System</td>
<td>Linux</td>
</tr>
<tr class="row-even"><td>Hard Drives</td>
<td><dl class="first last docutils">
<dt>Seagate Constellation 7.2 K RPM</dt>
<dd>64 MB Cache SATA 92GB</dd>
</dl>
</td>
</tr>
<tr class="row-odd"><td>Storage details</td>
<td><dl class="first last docutils">
<dt>RAID 9260-4i 1pt SAS2</dt>
<dd>512 MB SGL</dd>
</dl>
</td>
</tr>
<tr class="row-even"><td>Batch system</td>
<td>Torque</td>
</tr>
<tr class="row-odd"><td>Disk Size [TB]</td>
<td>15</td>
</tr>
<tr class="row-even"><td>Local storage, per node [GB]</td>
<td>92</td>
</tr>
<tr class="row-odd"><td>Primary storage, shared by all nodes</td>
<td>NFS</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="foxtrot">
<h2>Foxtrot<a class="headerlink" href="#foxtrot" title="Permalink to this headline"></a></h2>
<p><em>University&nbsp;of Florida, Gainesville</em></p>
<p>Foxtrot consists of 32 nodes with dual Intel&nbsp;X5520&nbsp;processors and 768
GB of memory. Following are detailed specifications for Foxtrot.</p>
<table border="1" class="docutils">
<colgroup>
<col width="51%" />
<col width="49%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Machine Type</td>
<td>Cluster</td>
</tr>
<tr class="row-even"><td>System Type</td>
<td>IBM&nbsp;iDataPlex&nbsp;dx&nbsp;360&nbsp;M2</td>
</tr>
<tr class="row-odd"><td>CPU type</td>
<td>Intel&nbsp;Xeon&nbsp;X5520</td>
</tr>
<tr class="row-even"><td>Host Name</td>
<td>foxtrot</td>
</tr>
<tr class="row-odd"><td>CPU Speed</td>
<td>2.26GHz</td>
</tr>
<tr class="row-even"><td>Number of CPUs</td>
<td>64</td>
</tr>
<tr class="row-odd"><td>Number of nodes</td>
<td>32</td>
</tr>
<tr class="row-even"><td>RAM</td>
<td>24 GB&nbsp;DDR3&nbsp;1333Mhz</td>
</tr>
<tr class="row-odd"><td>Total RAM (GB)</td>
<td>768</td>
</tr>
<tr class="row-even"><td>Number of cores</td>
<td>256</td>
</tr>
<tr class="row-odd"><td>Operating System</td>
<td>Linux</td>
</tr>
<tr class="row-even"><td>Tflops</td>
<td>3</td>
</tr>
<tr class="row-odd"><td>Disk Size (TB)</td>
<td>20</td>
</tr>
<tr class="row-even"><td>Hard Drives</td>
<td>500 GB Internal 7200 RPM&nbsp;SATA&nbsp;Drive</td>
</tr>
<tr class="row-odd"><td>Primary storage, shared by all nodes</td>
<td>NFS</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="hotel">
<h2>Hotel<a class="headerlink" href="#hotel" title="Permalink to this headline"></a></h2>
<p><em>University of Illinois, Chicago</em></p>
<p>Hotel consists of 84 nodes with dual Intel&nbsp;X5550&nbsp;processors and 2016
GB of memory. Following are detailed specifications for Hotel.</p>
<table border="1" class="docutils">
<colgroup>
<col width="51%" />
<col width="49%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Machine Type</td>
<td>Cluster</td>
</tr>
<tr class="row-even"><td>System Type</td>
<td>IBM&nbsp;iDataPlex&nbsp;dx&nbsp;360&nbsp;M2</td>
</tr>
<tr class="row-odd"><td>CPU type</td>
<td>Intel&nbsp;Xeon&nbsp;X5550</td>
</tr>
<tr class="row-even"><td>Host Name</td>
<td>hotel</td>
</tr>
<tr class="row-odd"><td>CPU Speed</td>
<td>2.66GHz</td>
</tr>
<tr class="row-even"><td>Number of CPUs</td>
<td>168</td>
</tr>
<tr class="row-odd"><td>Number of nodes</td>
<td>84</td>
</tr>
<tr class="row-even"><td>RAM</td>
<td>24 GB&nbsp;DDR3&nbsp;1333Mhz</td>
</tr>
<tr class="row-odd"><td>Total RAM (GB)</td>
<td>2016</td>
</tr>
<tr class="row-even"><td>Number of cores</td>
<td>672</td>
</tr>
<tr class="row-odd"><td>Operating System</td>
<td>Linux</td>
</tr>
<tr class="row-even"><td>Tflops</td>
<td>7</td>
</tr>
<tr class="row-odd"><td>Disk Size (TB)</td>
<td>120</td>
</tr>
<tr class="row-even"><td>Hard Drives</td>
<td>1 TB Internal 7200 RPM&nbsp;SATA&nbsp;Drive</td>
</tr>
<tr class="row-odd"><td>Primary storage, shared by all nodes</td>
<td>GPFS</td>
</tr>
<tr class="row-even"><td>Connection configuration</td>
<td>Mellanox&nbsp;4x&nbsp;DDR&nbsp;InfiniBand&nbsp;adapters</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="india">
<h2>India<a class="headerlink" href="#india" title="Permalink to this headline"></a></h2>
<p><em>Indiana University, Bloomington</em></p>
<p>India consists of 128 nodes with dual Intel X5550 processors and 24
GB of memory. Following are detailed specifications for India.</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Machine Type</td>
<td>Cluster</td>
</tr>
<tr class="row-even"><td>System Type</td>
<td>IBM iDataPlex dx 360 M2</td>
</tr>
<tr class="row-odd"><td>CPU type</td>
<td>Intel Xeon X5550</td>
</tr>
<tr class="row-even"><td>Host Name</td>
<td>india</td>
</tr>
<tr class="row-odd"><td>CPU Speed</td>
<td>2.66GHz</td>
</tr>
<tr class="row-even"><td>Number of CPUs</td>
<td>256</td>
</tr>
<tr class="row-odd"><td>Number of nodes</td>
<td>128</td>
</tr>
<tr class="row-even"><td>RAM</td>
<td>24 GB DDR3 1333Mhz</td>
</tr>
<tr class="row-odd"><td>Total RAM (GB)</td>
<td>3072</td>
</tr>
<tr class="row-even"><td>Number of cores</td>
<td>1024</td>
</tr>
<tr class="row-odd"><td>Operating System</td>
<td>Linux</td>
</tr>
<tr class="row-even"><td>Tflops</td>
<td>11</td>
</tr>
<tr class="row-odd"><td>Disk Size (TB)</td>
<td>335</td>
</tr>
<tr class="row-even"><td>Hard Drives</td>
<td>3000 GB Internal 7200 RPM SATA Drive</td>
</tr>
<tr class="row-odd"><td>Primary storage, shared by all nodes</td>
<td>NFS</td>
</tr>
<tr class="row-even"><td>Connection configuration</td>
<td>Mellanox 4x&nbsp;DDR InfiniBand&nbsp;adapters</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="sierra">
<h2>Sierra<a class="headerlink" href="#sierra" title="Permalink to this headline"></a></h2>
<p><em>San Diego Supercomputer Center</em></p>
<p>Sierra consists of 84 nodes with dual Intel L5420 processors and 2688
GB of memory. Following are detailed specifications for Sierra.</p>
<table border="1" class="docutils">
<colgroup>
<col width="34%" />
<col width="66%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Machine Type</td>
<td>Cluster</td>
</tr>
<tr class="row-even"><td>System Type</td>
<td>IBM&nbsp;iDataPlex&nbsp;dx&nbsp;340</td>
</tr>
<tr class="row-odd"><td>CPU type</td>
<td>Intel&nbsp;Xeon&nbsp;L5420</td>
</tr>
<tr class="row-even"><td>Host Name</td>
<td>sierra</td>
</tr>
<tr class="row-odd"><td>CPU Speed</td>
<td>2.5GHz</td>
</tr>
<tr class="row-even"><td>Number of CPUs</td>
<td>168</td>
</tr>
<tr class="row-odd"><td>Number of nodes</td>
<td>84</td>
</tr>
<tr class="row-even"><td>RAM</td>
<td>32 GB&nbsp;DDR2-667</td>
</tr>
<tr class="row-odd"><td>Total RAM (GB)</td>
<td>2688</td>
</tr>
<tr class="row-even"><td>Number of cores</td>
<td>672</td>
</tr>
<tr class="row-odd"><td>Operating System</td>
<td>Linux</td>
</tr>
<tr class="row-even"><td>Tflops</td>
<td>7</td>
</tr>
<tr class="row-odd"><td>Disk Size (TB)</td>
<td>72</td>
</tr>
<tr class="row-even"><td>Hard Drives</td>
<td>160 GB Internal 7200 RPM&nbsp;SATA&nbsp;Drive</td>
</tr>
<tr class="row-odd"><td>Primary storage, shared by all nodes</td>
<td>ZFS filesystem with 76.8 TB raid2 storage and 5.4 TB of raid0 (for scratch)</td>
</tr>
<tr class="row-even"><td>Connection configuration</td>
<td>Mellanox&nbsp;4x&nbsp;DDR&nbsp;InfiniBand&nbsp;adapters</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="xray">
<h2>Xray<a class="headerlink" href="#xray" title="Permalink to this headline"></a></h2>
<p><em>Indiana University, Bloomington</em></p>
<p>Xray consists of a single node with AMD OPteron 2378 processor and
1344 GB of memory. Following are detailed specifications for Xray.</p>
<table border="1" class="docutils">
<colgroup>
<col width="55%" />
<col width="45%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Machine Type</td>
<td>Cluster</td>
</tr>
<tr class="row-even"><td>System Type</td>
<td>Cray XT5m</td>
</tr>
<tr class="row-odd"><td>CPU type</td>
<td>AMD Opteron&nbsp;2378</td>
</tr>
<tr class="row-even"><td>Host Name</td>
<td>xray</td>
</tr>
<tr class="row-odd"><td>CPU Speed</td>
<td>2.4GHz</td>
</tr>
<tr class="row-even"><td>Number of CPUs</td>
<td>168</td>
</tr>
<tr class="row-odd"><td>Number of nodes</td>
<td>1</td>
</tr>
<tr class="row-even"><td>RAM</td>
<td>8 GB DDR2-800</td>
</tr>
<tr class="row-odd"><td>Total RAM (GB)</td>
<td>1344</td>
</tr>
<tr class="row-even"><td>Number of cores</td>
<td>672</td>
</tr>
<tr class="row-odd"><td>Operating System</td>
<td>Linux</td>
</tr>
<tr class="row-even"><td>Tflops</td>
<td>6</td>
</tr>
<tr class="row-odd"><td>Disk Size (TB)</td>
<td>335</td>
</tr>
<tr class="row-even"><td>Hard Drives</td>
<td>6 TB Internal Lustre Storage</td>
</tr>
<tr class="row-odd"><td>Primary storage, shared by all nodes</td>
<td>NFS</td>
</tr>
<tr class="row-even"><td>Connection configuration</td>
<td>Cray SeaStar Interconnect</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="network">
<h2>Network<a class="headerlink" href="#network" title="Permalink to this headline"></a></h2>
<p>The FutureGrid network provides for connectivity among FutureGrid
participants, and network access to the Network Impairments Device
(NID).&nbsp; Five participants connect through a variety of network providers
(see Figure 1).</p>
<p><img alt="image6" src="https://portal.futuregrid.org/sites/default/files/images/FutureGrid%20Logocal%20v3.png" />
<em>Figure 1:&nbsp; Diagram of FutureGrid Network</em></p>
<p>FutureGrid partners with XSEDE to allow participants of each to
utilize the resources of both networks. Through interconnects at the
Indiana Gigapop, Internet2 and NLR extend access to FutureGrid resources
to researchers nationally.</p>
<p>FutureGrid deployed a Juniper EX8208 at the Core in Chicagos
StarLight facility.</p>
<p><img alt="image7" src="https://portal.futuregrid.org/sites/default/files/resize/images/Juniper%20EX8208-140x184.png" /></p>
<p><em>Figure 2: Juniper EX8208</em></p>
<p>The Juniper EX series provides for Layer2 and Layer3 connectivity.
FutureGrid uses the EX to fulfill the network services (i.e.,
interconnectivity and external connectivity). The EX provides a special
feature labeled firewall based forwarding, allowing seamless
integration with network impairments.</p>
<p>GlobalNOC at Indiana University provides network engineering
services, and the Operations Center provides email and phone support
24x7. GlobalNOC resources for public wan information can be found at
<a class="reference external" href="http://noc.futuregrid.org/">http://noc.futuregrid.org/</a>. Available
tools include:</p>
<ul class="simple">
<li><strong>`FutureGrid
Atlas &lt;http://noc.futuregrid.org/futuregrid/live-network-status/maps&#8211;graphs/futuregrid-atlas.html&gt;`__:</strong>&nbsp;View
the current level of FutureGrid Network traffic as displayed on a
geographical map.</li>
<li><strong>`FutureGrid SNAPP Traffic
Graphs &lt;http://noc.futuregrid.org/futuregrid/live-network-status/traffic-statistics/futuregrid-snapp-trafic-graphs2.html&gt;`__:</strong>&nbsp;View
high-speed traffic graphs collected at one-minute samples using the
Indiana University-developed SNAPP tool. Create custom views of the
FutureGrid network and view historic utilization with greater data
resolution.</li>
<li><strong>`FutureGrid NOC Router
Proxy &lt;http://noc.futuregrid.org/futuregrid/live-network-status/traffic-statistics/router-proxy3.html&gt;`__:</strong>
Submit show commands to the FutureGrid router.</li>
<li><strong>`FutureGrid Physical
Map &lt;http://noc.futuregrid.org/futuregrid/maps&#8211;documentation/maps.html#FutureGrid%20Physical%20Map&gt;`__</strong></li>
<li><strong>`FutureGrid Topology
Map &lt;http://noc.futuregrid.org/futuregrid/maps&#8211;documentation/maps.html#FutureGrid%20Topology%20Map&gt;`__</strong></li>
</ul>
<p>As IPv6 test beds are not available at all FutureGrid partner sites,
early implementation of iPv6 would slow important software development
work. As a result, current plans call for FutureGrid to continue using
IPv4. Any change in plans will be noted here.</p>
<p>Below is further information about networking:</p>
<p>FutureGrid Core: Juniper EX8200
India; Force10, C-150
Bravo, Delta, Echo; Force10, S60
Sierra: Juniper EX4200
Hotel: EX4200
Dlamo: Dell PowerConnect 6000 Series
Xray: Force10, C-150
foxtrot:
internal network switch: IBM/BLADE Rack Switch G8000
public network switch: Force10 S50
Node NICs: built-in (IBM iDataPlex DX360 M2) dual Intel 82575EB
Gigabit Network Connection
10Gbps: Myricom Myri-10G Dual-Protocol NIC (available on login node)
Alamo: Dell PowerConnett 6224</p>
<p>Juniper, EX series
-&nbsp;<a class="reference external" href="https://www.juniper.net/us/en/products-services/switching/ex-series/Force10">https://www.juniper.net/us/en/products-services/switching/ex-series/Force10</a>&nbsp;(now
Dell),</p>
<p>C Series and S Series:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.dell.com/us/enterprise/p/force10-c150/pd">http://www.dell.com/us/enterprise/p/force10-c150/pd</a></li>
<li><a class="reference external" href="http://www.dell.com/us/enterprise/p/force10-s60/pd">http://www.dell.com/us/enterprise/p/force10-s60/pd</a></li>
</ul>
<p>Dell, PowerConnect
-&nbsp;<a class="reference external" href="http://www.dell.com/us/enterprise/p/switch-powerconnect">http://www.dell.com/us/enterprise/p/switch-powerconnect</a>
IBM (formerly BNT)
-&nbsp;<a class="reference external" href="http://www-03.ibm.com/systems/networking/switches/rack.html">http://www-03.ibm.com/systems/networking/switches/rack.html</a></p>
<p><img alt="image8" src="https://portal.futuregrid.org/sites/default/files/u23/futuregrid-physical.png" />
<img alt="image9" src="https://portal.futuregrid.org/sites/default/files/u23/futuregrid-topology.png" /></p>
</div>
<div class="section" id="futuregrid-network-impairments-device-nid">
<h2>FutureGrid Network Impairments Device (NID)<a class="headerlink" href="#futuregrid-network-impairments-device-nid" title="Permalink to this headline"></a></h2>
<p><strong>FutureGrid Network Impairments Device (NID)</strong></p>
<p>Researchers on FutureGrid may perodically employ the use of a Spirent
XGEM, a Network Impairments Emulator.&nbsp; The XGEM allows users to
accurately create the delays and impairments that occur over live
production networks for validating and evaluating new products and
technologies.&nbsp; The XGEM supports a variety of impairments with the most
common being delay, packet loss, jitter and re-ordering.</p>
<p><img alt="image10" src="https://portal.futuregrid.org/sites/default/files/images/Spirent%20XGEM.png" />
<em>Figure 1: Spirent XGEM</em></p>
<p>The XGEM contains two blades, each with a 10GE interface.&nbsp; Traffic
received on one blade is automatically transmitted on the other blade,
and vice-versa.&nbsp; This allows the XGEM to be deployed as a pass-through
device.&nbsp; Impairments are applied unidirectionally.&nbsp; An identical or
different impairment can be applied in either direction, or
simultaneously in both directions.</p>
<p>FutureGrid also utilizes a Juniper EX8208 as the primary network
element between all of the FutureGrid participants.&nbsp; Junipers EX
platform contains firewall-based forwarding, which allows us to insert
the XGEM between FutureGrid participants with only software changes (see
Figure 2).</p>
<p><img alt="image11" src="https://portal.futuregrid.org/sites/default/files/images/FutureGrid%20Logocal%20v3.png" />
<em>Figure 2.&nbsp; Diagram of FutureGrid Network.</em></p>
<p>The firewall-based forwarding feature can be configured to forward
traffic unidirectionally or bidirectionally through the XGEM using a
single IP address or any size subnet.&nbsp; Only traffic defined by the
Juniper EX8208 will be impaired, leaving all other traffic between
FutureGrid participants unaffected.</p>
<p>This implementation requires traffic to traverse the FutureGrid Core
in order to be impaired.&nbsp; Additional configuration might be required at
the participants&#8217; individual sites for impairments to occur locally.</p>
</div>
<div class="section" id="id7">
<h2>Storage<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h2>
<p>Clustername(site)</p>
<p>Mountpoint<a href="#id8"><span class="problematic" id="id9">``</span></a><a href="#id10"><span class="problematic" id="id11">``</span></a></p>
<p>Size</p>
<p>Type</p>
<p>Backups</p>
<p>Use</p>
<p>Notes</p>
<p><a class="reference external" href="https://portal.futuregrid.org/kb/document/bbns">Sierra</a>(UCSD/SDSC)</p>
<p>/N/u/username</p>
<p>40.6TB</p>
<p>ZFS (RAID2)</p>
<p>Yes (nightly incremental)</p>
<p>Home dir</p>
<p>By default, quotas on home directories are 50 GB and quotas on scratch
directories are 100 GB.</p>
<p>/N/scratch/username</p>
<p>5.44TB</p>
<p>ZFS (RAID0)</p>
<p>No</p>
<p>Scratch</p>
<p>/N/soft</p>
<p>50GB</p>
<p>ZFS (RAID2)</p>
<p>Yes (nightly incremental)</p>
<p>Software installs</p>
<p>/N/images</p>
<p>6TB</p>
<p>ZFS (RAID2)</p>
<p>Yes (nightly incremental)</p>
<p>VM images</p>
<p><a class="reference external" href="https://portal.futuregrid.org/kb/document/bbms">India (IU)</a></p>
<p>/N/u/username</p>
<p>15TB</p>
<p>NFS (RAID5)</p>
<p>Yes (nightly incremental)</p>
<p>Home dir</p>
<p>At the moment, we do not have any quota implemented on India and we use
the local/tmp (77 GB) as scratch space.</p>
<p>/share/project</p>
<p>14TB</p>
<p>NFS (RAID5)</p>
<p>Yes (nightly incremental)</p>
<p>Shared/group folders</p>
<p>/tmp</p>
<p>77GB</p>
<p>local disk</p>
<p>No</p>
<p>Scratch</p>
<p><a class="reference external" href="https://portal.futuregrid.org/kb/document/bcao">Bravo</a> (IU)</p>
<p>/N/u/username</p>
<p>15TB</p>
<p>NFS (RAID5)</p>
<p>Yes (nightly incremental)</p>
<p>Home dir</p>
<p>The same NFS shares in India are mounted in Bravo (users do not log in
here; jobs are submitted through India). There are two local partitions,
which are used for HDFS and swift tests.</p>
<p>/share/project</p>
<p>14TB</p>
<p>NFS (RAID5)</p>
<p>Yes (nightly incremental)</p>
<p>Shared/group folders</p>
<p><a class="reference external" href="https://portal.futuregrid.org/kb/document/bcaj">Delta</a> (IU)</p>
<p>/N/u/username</p>
<p>15TB</p>
<p>NFS (RAID5)</p>
<p>Yes (nightly incremental)</p>
<p>Home dir</p>
<p>Same as Bravo. The NFS shares are mounted for user and group share
(users do not log in directly here; jobs are submitted through India).</p>
<p>/share/project</p>
<p>14TB</p>
<p>NFS (RAID5)</p>
<p>Yes (nightly incremental)</p>
<p>Shared/group folders</p>
<p>Hotel (UC)</p>
<p>/gpfs/home</p>
<p>15TB</p>
<p>GPFS (RAID6)</p>
<p>No</p>
<p>Home dir</p>
<p>By default, quotas on home directories are 10 GB.</p>
<p>/gpfs/scratch</p>
<p>57TB</p>
<p>GPFS (RAID6)</p>
<p>No</p>
<p>Scratch</p>
<p>/gpfs/software</p>
<p>7.1GB</p>
<p>GPFS (RAID6)</p>
<p>No</p>
<p>Software installs</p>
<p>/gpfs/images</p>
<p>7.1TB</p>
<p>GPFS (RAID6)</p>
<p>No</p>
<p>VM images</p>
<p>/scratch/local</p>
<p>862GB</p>
<p>ext3 (local disk)</p>
<p>No</p>
<p>Local scratch</p>
<p>Foxtrot (UFL)</p>
<p>/N/u/username</p>
<p>16TiB</p>
<p>NFS (RAID5)</p>
<p>No</p>
<p>Home dir</p>
<p>At the moment, we do not have any quota implemented on Foxtrot.</p>
</div>
<div class="section" id="using-hpss-from-futuregrid">
<h2>Using HPSS from FutureGrid<a class="headerlink" href="#using-hpss-from-futuregrid" title="Permalink to this headline"></a></h2>
<p><strong>Note</strong>: FutureGrid does not provide an HPSS server. The HSI is used to
access IU&#8217;s HPSS service from INDIA. This is available only for IU
faculty, staff, and students.</p>
<p>Through the <a class="reference external" href="http://rc.uits.iu.edu/storage/sda">SDA</a> (formerly known
as MDSS) service, IU provides distributed storage service to faculty,
staff, and graduate students. The
<a class="reference external" href="http://rc.uits.iu.edu/storage/hsi">HSI</a> (Hierarchical Storage
Interface) client is available in INDIA. To use the HSI client:</p>
<ul class="simple">
<li>First, activate your SDA account. Detailed instructions are available
at IU&#8217;s <a class="reference external" href="http://rc.uits.iu.edu/storage/mdss-starter-kit">MDSS Service Starter
Kit</a> page.</li>
<li>Then, from INDIA, load the HSI module as follows:</li>
</ul>
<div class="highlight-python"><pre>$ module load hsi
hsi version 3.5.3 loaded</pre>
</div>
<ul class="simple">
<li>Connect to the SDA:</li>
</ul>
<div class="highlight-python"><pre>$ hsi -A combo
Principal: your_iu_userid
[youriuid]Password:
Username: your_iu_userid UID: 1122636 Acct: 1122636(1122636) Copies: 1 Firewall: off [hsi.3.5.3 Fri Nov 20 10:01:25 EST 2009]
?</pre>
</div>
<p><strong>Note:</strong>Your Principal is your IU Network ID, and your password is
the IU passphrase.</p>
<ul>
<li><p class="first">Enable firewall mode; otherwise, you will receive this error:</p>
<div class="highlight-python"><pre>put: Error -5 on transfer</pre>
</div>
</li>
</ul>
<div class="highlight-python"><pre>? firewall -on
A: firewall mode set ON, I/O mode set to extended (parallel=off), autoscheduling currently set to OFF</pre>
</div>
<ul class="simple">
<li>List local folder:</li>
</ul>
<div class="highlight-python"><pre> ? lls
testfile.txt</pre>
</div>
<ul class="simple">
<li>List the current directory in HPSS:</li>
</ul>
<div class="highlight-python"><pre>? pwd
pwd0: /hpss/pathtoyouriuusername</pre>
</div>
<ul class="simple">
<li>For transferring files (<em>put</em> and <em>get</em>), search the <a class="reference external" href="http://kb.iu.edu/?search=hsi">IU Knowledge
Base</a>.</li>
</ul>
</div>
<div class="section" id="id12">
<h2>Status<a class="headerlink" href="#id12" title="Permalink to this headline"></a></h2>
<blockquote>
<div>Please note that the information on this page is not 100% accurate
as it is maintained by hand. However, dynamic monitoring tools are
available and can be consulted for more details.</div></blockquote>
<blockquote>
<div><p>The <strong>first Tuesday of each month</strong> is the standard maintenance
window.</p>
<ul class="simple">
<li><a class="reference external" href="/admin/build/views/edit/active_outage_list?destination=book%2Fexport%2Fhtml%2F104#views-tab-default">Edit</a></li>
<li><a class="reference external" href="/admin/build/views/export/active_outage_list">Export</a></li>
<li><a class="reference external" href="/admin/build/views/clone/active_outage_list">Clone</a></li>
</ul>
<table border="1" class="docutils">
<colgroup>
<col width="4%" />
<col width="7%" />
<col width="30%" />
<col width="7%" />
<col width="11%" />
<col width="11%" />
<col width="5%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Status</th>
<th class="head">Type</th>
<th class="head">Title</th>
<th class="head">Impacted Systems</th>
<th class="head">Start of Outage</th>
<th class="head">Anticipated End of Outage</th>
<th class="head">Resolution</th>
<th class="head">Edit link</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Ongoing</td>
<td>Software System</td>
<td><a class="reference external" href="/outages/130514/bravo-reserved-6-weeks">Bravo is reserved in 6 weeks</a></td>
<td>other</td>
<td>Tue, 14 May 2013, 17:00 EDT</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><a class="reference external" href="/node/2513/edit?destination=book%2Fexport%2Fhtml%2F104">edit</a></td>
</tr>
<tr class="row-odd"><td>Planned</td>
<td>Software System</td>
<td><a class="reference external" href="/outages/120901/eucalyptus-gui-not-available">Eucalyptus GUI not available</a></td>
<td>india</td>
<td>Sat, 01 Sep 2012 (All day)</td>
<td>Fri, 05 Oct 2012 (All day)</td>
<td>&nbsp;</td>
<td><a class="reference external" href="/node/2226/edit?destination=book%2Fexport%2Fhtml%2F104">edit</a></td>
</tr>
</tbody>
</table>
</div></blockquote>
<p>You can find a list of previous outages <a class="reference external" href="/outages_all">here</a>
including an <a class="reference external" href="/feeds/fg_outages_all_rss.xml">RSS</a> feed.</p>
<p>~</p>
<blockquote>
<div><p>The system is configured at this time using either&nbsp;HPC, Nimbus, or
Eucalyptus services. The distribution used at this time can be
found&nbsp;<a class="reference external" href="http://inca.futuregrid.org:8080/inca/jsp/partitionTable.jsp">here</a>&nbsp;and&nbsp;<a class="reference external" href="http://inca.futuregrid.org:8080/inca/jsp/status.jsp?suiteNames=Info&amp;xsl=info.xsl">here</a>.</p>
<p><a class="reference external" href="http://inca.futuregrid.org:8080/inca/jsp/partitionTable.jsp"><img alt="image12" src="https://portal.futuregrid.org/sites/default/files/images/status_incapart.PNG" /></a></p>
</div></blockquote>
<blockquote>
<div><p>Lists the status of keyFutureGrid&nbsp;services (e.g.,&nbsp;SSH, Nimbus,
Eucalyptus) in a sortable table.</p>
<p><a class="reference external" href="http://inca.futuregrid.org:8080/inca/jsp/status.jsp?queryNames=Health&amp;xsl=table.xsl&amp;resourceIds=FutureGrid"><img alt="image13" src="https://portal.futuregrid.org/sites/default/files/ScreenSnapz.jpg" /></a></p>
</div></blockquote>
<p>Ganglia:</p>
<blockquote>
<div><a class="reference external" href="http://ganglia.futuregrid.org"><img alt="image14" src="https://portal.futuregrid.org/sites/default/files/ganglia.png" /></a></div></blockquote>
<blockquote>
<div><p>The network status can be viewed via the&nbsp;<a class="reference external" href="http://noc.futuregrid.org">FutureGrid&nbsp;Network
Operations Center page</a>. &nbsp;Upcoming
network events are maintained on the&nbsp;<a class="reference external" href="http://noc.futuregrid.org/futuregrid/support/operations-calendar3.html">Network Operations
Calendar</a>.</p>
<p><a class="reference external" href="http://noc.futuregrid.org"><img alt="image15" src="https://portal.futuregrid.org/sites/default/files/images/large_status_nocmap.PNG" /></a></p>
</div></blockquote>
<blockquote>
<div><p>The operational status of&nbsp;FutureGrid&nbsp;machines are continuously
monitored via a number of tools. To view some of the contents,
please
visit&nbsp;<a class="reference external" href="http://inca.futuregrid.org">http://inca.futuregrid.org</a></p>
<p><a class="reference external" href="http://inca.futuregrid.org"><img alt="image16" src="https://portal.futuregrid.org/sites/default/files/u23/Screen%20shot%202011-01-14%20at%207.48.06%20PM.png" /></a></p>
</div></blockquote>
<p><a class="reference external" href="https://portal.futuregrid.org/monitoring/cloud">Here</a>&nbsp;you will find
information about how&nbsp;FG&nbsp;cloud services are currently used.</p>
<blockquote>
<div><a class="reference external" href="https://portal.futuregrid.org/monitoring/cloud"><img alt="image17" src="https://portal.futuregrid.org/sites/default/files/u23/Screen%20shot%202011-04-07%20at%203.23.05%20PM.png" /></a></div></blockquote>
<blockquote>
<div><div class="line-block">
<div class="line">Metric system provides an integrated accounting service to view</div>
</div>
<p>cloud usage statistics and graphs regarding the utilization of
virtual machine (VM) instances.
<a class="reference external" href="https://portal.futuregrid.org/metrics"><img alt="image18" src="https://portal.futuregrid.org/sites/default/files/screenshot-for-status-small.png" /></a></p>
</div></blockquote>
<blockquote>
<div><div class="line-block">
<div class="line">The version info of the installed system software across</div>
</div>
<p>all&nbsp;FG&nbsp;sites:
<a class="reference external" href="http://inca.futuregrid.org:8080/inca/HTML/rest/HPC/FutureGrid"><img alt="image19" src="https://portal.futuregrid.org/sites/default/files/fg-sys-sw-ver.PNG" /></a></p>
</div></blockquote>
<p><a class="reference external" href="http://inca.futuregrid.org/nimbus-stats">Displays</a>&nbsp;weekly usage
graphs of each of the&nbsp;FutureGrid&nbsp;Nimbus deployments.</p>
<blockquote>
<div><a class="reference external" href="http://inca.futuregrid.org/nimbus-stats"><img alt="image20" src="https://portal.futuregrid.org/sites/default/files/u15/nimbus-usage.png" /></a></div></blockquote>
</div>
<div class="section" id="accessing-futuregrid-resources-via-ssh">
<h2>Accessing FutureGrid resources via SSH<a class="headerlink" href="#accessing-futuregrid-resources-via-ssh" title="Permalink to this headline"></a></h2>
<p>To properly view this manual page, please log into the FutureGrid portal
with your FutureGrid name.</p>
<p>To access the various FutureGrid resources, you need to provide a public
ssh key to FutureGrid. In this manual, we explain how to generate a ssh
key, upload it to the FutureGrid portal and log onto the resources. This
manual covers both UNIX and Windows Users.</p>
</div>
<div class="section" id="requirement-for-windows-users">
<h2>Requirement for Windows Users<a class="headerlink" href="#requirement-for-windows-users" title="Permalink to this headline"></a></h2>
<p>Windows users need to have some special software to be able to use the
SSH commands. We recommend you use Cygwin (Linux-like environment for
Windows) because it will ease your experience with FutureGrid. We have
prepared a Cygwin version that is ready to use (If for some reason you
decide to download and install Cygwin from the official site, remember
that you need to install the ssh packages).</p>
<ol class="arabic simple">
<li>Download Cygwin from our
Portal&nbsp;<a class="reference external" href="https://portal.futuregrid.org/sites/default/files/cygwin.zip">https://portal.futuregrid.org/sites/default/files/cygwin.zip</a>.</li>
<li>Uncompress the file.</li>
<li>Execute the file the &#8216;Windows Batch File&#8217; called Cygwin.bat
<img alt="image21" src="https://portal.futuregrid.org/sites/default/files/u30/cygwim1.png" /></li>
<li>You may get a warning. Click in the Run button
<img alt="image22" src="https://portal.futuregrid.org/sites/default/files/u30/cygwin2.png" /></li>
<li>You get a Linux-like terminal that will allow you to continue with
this manual.
<img alt="image23" src="https://portal.futuregrid.org/sites/default/files/u30/cygwinfirst.png" /></li>
</ol>
<p><strong>NOTE</strong>: When showing examples of commands, the $ symbol precedes the
actual command. So, the other lines are the output obtained after
executing the command.</p>
</div>
<div class="section" id="instructions-for-both-windows-and-unix-users">
<h2>Instructions for both Windows and Unix users<a class="headerlink" href="#instructions-for-both-windows-and-unix-users" title="Permalink to this headline"></a></h2>
<div class="section" id="generate-ssh-key">
<h3>Generate SSH key<a class="headerlink" href="#generate-ssh-key" title="Permalink to this headline"></a></h3>
<p>Use the tool ssh-keygen. This program is commonly available on most UNIX
systems (this includes Cygwin). It will ask you for the location and
name of the new key. It will also ask you for a passphrase, we
<strong>STRONGLY RECOMMEND</strong> that you use a passphrase. We have seem advise by
teachers and teachin assistants to not use passphrases: this is
<strong>WRONG</strong>. If you are not using a pasphrase and someone were to steal
your private key they have easily access to your account. We recommend
using the default location ~/.ssh/ and the default name id_rsa. A
sample session:</p>
<div class="highlight-python"><pre>$ ssh-keygen

Generating public/private rsa key pair.
Enter file in which to save the key (/home/Javi/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/Javi/.ssh/id_rsa
Your public key has been saved in /home/Javi/.ssh/id_rsa.pub.
The key fingerprint is:
90:46:9b:cf:09:16:94:17:df:43:f4:99:97:0d:42:4a Javi@Javi-PC
The key's randomart image is:</pre>
</div>
<div class="highlight-python"><pre>+--[ RSA 2048]----+
|     .+...Eo= .  |
|     ..=.o + o +o |
|      O.  o o +.o |
|     o = .   . . |
|        S             |
|                 |
|                 |
| |
|                 |
+-----------------+</pre>
</div>
<p>This command requires the interaction of the user.</p>
<p>&nbsp; &nbsp; 1. The first question is:</p>
<div class="highlight-python"><pre>Enter file in which to save the key (/home/Javi/.ssh/id_rsa):</pre>
</div>
<p>We recommend you use the default. To do so, just press the enter key. In
case you already have a ssh key in your machine, you can skip this whole
section or use a different file name.</p>
<p>&nbsp; &nbsp; 2. The second and third question is to protect your ssh key with a
passphrase. This password will protect your key because you need to type
it when you want to use it. Thus, you can either type a passphrase or
press enter to leave it without passphrase. To avoid security problems,
we DO recommend that chose a passphrase as discussed previously. Make
sure to not just type return for an empty passphrase.</p>
<div class="highlight-python"><pre>Enter passphrase (empty for no passphrase):</pre>
</div>
<p>and</p>
<div class="highlight-python"><pre>Enter same passphrase again:</pre>
</div>
</div>
<div class="section" id="check-your-ssh-key">
<h3>Check your ssh key<a class="headerlink" href="#check-your-ssh-key" title="Permalink to this headline"></a></h3>
<p>Once, you have generated your key, you should have them in the .ssh
directory</p>
<div class="highlight-python"><pre>$ ls -l ~/.ssh</pre>
</div>
</div>
<div class="section" id="copy-the-content-of-your-public-key">
<h3>Copy the content of your public key<a class="headerlink" href="#copy-the-content-of-your-public-key" title="Permalink to this headline"></a></h3>
<p>You need to copy the content of your public key to upload it to the
portal. A sample asumming that you used the default options during the
key generation:</p>
<div class="highlight-python"><pre>$ cat ~/.ssh/id_rsa.pub

ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCXJH2iG2FMHqC6T/U7uB8kt6KlRh4kUOjgw9sc4Uu+Uwe/EwD0wk6CBQMB+HKb9upvCRW/851UyRUagtlQexCRM2rMCi0VvhTVZhj61pTdhyl1t8hlkoL19JVnVBPP5kIN3wVyNAJjYBrAUNW4dXKXtmfkXp98T3OW4mxAtTH434MaT+QcPTcxims/hwsUeDAVKZY7UgZhEbiExxkejtnRBHTipi0W03W05TOUGRW7EuKf/4ftNVPilCO4DpfY44NFG1xPwHeimUk+t9h48pBQj16FrUCp0rS02Pj+4/9dNeS1kmNJu5ZYS8HVRhvuoTXuAY/UVcynEPUegkp+qYnR Javi@Javi-PC</pre>
</div>
<p>Go ahead and select the ouptut, right click, and copy</p>
<div class="highlight-python"><pre>$ cat ~/.ssh/id_rsa.pub

ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCXJH2iG2FMHqC6T/U7uB8kt6KlRh4kUOjgw9sc4Uu+Uwe/EwD0wk6CBQMB+HKb9upvCRW/851UyRUagtlQexCRM2rMCi0VvhTVZhj61pTdhyl1t8hlkoL19JVnVBPP5kIN3wVyNAJjYBrAUNW4dXKXtmfkXp98T3OW4mxAtTH434MaT+QcPTcxims/hwsUeDAVKZY7UgZhEbiExxkejtnRBHTipi0W03W05TOUGRW7EuKf/4ftNVPilCO4DpfY44NFG1xPwHeimUk+t9h48pBQj16FrUCp0rS02Pj+4/9dNeS1kmNJu5ZYS8HVRhvuoTXuAY/UVcynEPUegkp+qYnR Javi@Javi-PC</pre>
</div>
</div>
<div class="section" id="upload-the-key-to-the-futuregrid-portal">
<h3>Upload the key to the FutureGrid Portal<a class="headerlink" href="#upload-the-key-to-the-futuregrid-portal" title="Permalink to this headline"></a></h3>
<p>Click on the button bellow to add your SSH key (Note: The quick link
button below only works if you are logged in before visiting this page.
Otherwise please make sure you login first and REFRESH this page.)</p>
<blockquote>
<div><a class="reference external" href="https://portal.futuregrid.org/user/23/ssh-keys"><img alt="image24" src="https://portal.futuregrid.org/sites/default/files/u23/register-sshkey.png" /></a></div></blockquote>
<ol class="arabic simple">
<li>If you were logged into the Portal, this button redirects you to a
page that includes a link Add a public key.</li>
<li>Otherwise, this button redirects you to the login page.<ol class="arabic">
<li>Log into the portal
<img alt="image25" src="https://portal.futuregrid.org/sites/default/files/u30/portalLogin_0.png" /></li>
<li>Click in the &#8220;ssh key&#8221; button
<img alt="image26" src="https://portal.futuregrid.org/sites/default/files/u30/portalsshkey.png" /></li>
</ol>
</li>
<li>Click in the &#8220;add a public key&#8221; link.
<img alt="image27" src="https://portal.futuregrid.org/sites/default/files/u30/portalclikaddkey_0.png" /></li>
<li>Paste your ssh key into the box marked Key.
<img alt="image28" src="https://portal.futuregrid.org/sites/default/files/u30/portalkeypaste_0.png" /></li>
<li>Click the submit button.</li>
</ol>
<ul class="simple">
<li><strong>IMPORTANT</strong>:<ul>
<li>Leave the Title field blank.</li>
<li>Make sure that when you paste your key, it does not contain
newlines or carriage returns that may have been introduced by
incorrect pasting and copying. If so, please remove them.</li>
</ul>
</li>
</ul>
<p>At this point you are all set. However you will still need to wait till
all accounts have been set up to use the resources. Please, check your
email for further updates. You can also refresh this page and see if the
boxes in your account status information are all green. Than you can
continue.</p>
</div>
<div class="section" id="testing-your-ssh-key">
<h3>Testing your ssh key<a class="headerlink" href="#testing-your-ssh-key" title="Permalink to this headline"></a></h3>
<p>Test you key by logging onto India. India cluster gets the new ssh key
updated almost immediately. For other clusters like Hotel, it can take
around 10 minutes to update the ssh keys. If you are viewing this page
anonymously, please replace &lt;USER&gt; with your FutureGrid user name (the
one used to log into the Portal).</p>
<p>If you placed the ssh key in the default location:</p>
<div class="highlight-python"><pre>$ ssh -A gvonlasz@india.futuregrid.org</pre>
</div>
<p>If you used a different path or name for your key:</p>
<div class="highlight-python"><pre>$ ssh -A -i &lt;path to private key&gt; gvonlasz@india.futuregrid.org</pre>
</div>
<p>The first time you ssh into a machine you will see a message like the
one shown in the picture. You have to type yes and press enter:</p>
<div class="highlight-python"><pre>**Note**: the presence of the -A argument above is required for Nimbus</pre>
</div>
<p>tutorials.</p>
<p><strong>Note 1</strong>: If you are asked for a password when trying to ssh onto
Hotel, do <strong>NOT</strong> type any password. This means that your ssh key is not
updated yet. You need to wait a bit more.</p>
</div>
<div class="section" id="testing-your-ssh-key-on-hotel">
<h3>Testing your ssh key on Hotel<a class="headerlink" href="#testing-your-ssh-key-on-hotel" title="Permalink to this headline"></a></h3>
<p>After uploading your ssh key, it can take around 10 minutes to update
the ssh keys of Hotel. So, if you were able to log onto India, you have
set up properly your ssh key. So, after a while you will be able to log
onto Hotel.&nbsp; If you are viewing this page anonymously, Please replace
&lt;USER&gt; with your FutureGrid user name (the one used to log into the
Portal).</p>
<p>If you placed the ssh key in the default location:</p>
<div class="highlight-python"><pre>$ ssh -A gvonlasz@hotel.futuregrid.org</pre>
</div>
<p>If you used a different path or name for your key:</p>
<div class="highlight-python"><pre>$ ssh -A -i &lt;path to private key&gt; gvonlasz@hotel.futuregrid.org</pre>
</div>
<p>The first time you ssh into a machine you will see a message like this:</p>
<div class="highlight-python"><pre>The authenticity of host 'hotel.futuregrid.org (149.165.148.5)' can't be established.
RSA key fingerprint is f8:96:15:b7:21:eb:64:92:6c:de:e0:79:f3:fb:86:dd.
Are you sure you want to continue connecting (yes/no)? yes</pre>
</div>
<p><strong>Note</strong>: the presence of the -A argument above is required for Nimbus
tutorials.</p>
<p><strong>Note 1</strong>: If you are asked for a password when trying to ssh onto
Hotel, do&nbsp;<strong>NOT</strong>&nbsp;type any password. This means that your ssh key is
not updated yet. You need to wait a bit more.</p>
</div>
</div>
<div class="section" id="hpc-services">
<h2>HPC Services<a class="headerlink" href="#hpc-services" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="using-hpc-services-on-futuregrid">
<h2>Using HPC Services on FutureGrid<a class="headerlink" href="#using-hpc-services-on-futuregrid" title="Permalink to this headline"></a></h2>
<div class="section" id="accessing-systems">
<h3>Accessing Systems<a class="headerlink" href="#accessing-systems" title="Permalink to this headline"></a></h3>
<p>Several of the clusters that are part of FutureGrid have partitions
that operate as High Performance Computing (HPC) systems. These
partitions are batch scheduled, are not virtualized, have computer nodes
with fixed operating systems, and are suitable for running parallel
applications. FutureGrid provides a <a class="reference external" href="http://inca.futuregrid.org:8080/inca/jsp/partitionTable.jsp">list of HPC
partitions</a>
that currently consists of nodes on Alamo, Hotel, India, Sierra, and
Xray.</p>
<p>To access the FutureGrid HPC partitions, you need a FutureGrid
account and an SSH public key you have uploaded to FutureGrid (this
process is described on our <a class="reference external" href="https://portal.futuregrid.org/gettingstarted">Getting Started
page</a>). You can then
simply ssh to the login node of the FutureGrid system you would like to
use. These login nodes are named <em>&lt;system&gt;.futuregrid.org</em>,
specifically:</p>
<ul class="simple">
<li>alamo.futuregrid.org</li>
<li>bravo.futuregrid.org</li>
<li>hotel.futuregrid.org</li>
<li>india.futuregrid.org</li>
<li>sierra.futuregrid.org</li>
<li>xray.futuregrid.org</li>
</ul>
<p>If your FutureGrid username is different from your username on your
system, you will need to include it in your ssh command: <em>ssh
&lt;futuregrid user name&gt;&#64;&lt;system&gt;.futuregrid.org</em>. You can find out your
HPC account name by visiting your Portal account page.</p>
</div>
<div class="section" id="filesystem-layout">
<h3>Filesystem Layout<a class="headerlink" href="#filesystem-layout" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><em>Home</em> ($HOME) directories are located at <em>/N/u/&lt;username&gt;</em>, with
automated nightly backups. This is where users are encouraged to keep
source files, configuration files and executables. Users should not
run code from their $HOME directories. Please note that this is an
NFS file system, and may result in slower access for some
applications.</li>
<li><em>Scratch</em> directories are located at different locations on the
systems. To find out more about the file layout, please see <a class="reference external" href="http://portal.futuregrid.org/kb/document/bcgv">Storage
information for FutureGrid
hardware.</a></li>
<li><em>System software</em> directories are located at<em>/N/soft,</em> with
automated nightly backups. System and community software are
typically installed here.</li>
</ul>
</div>
<div class="section" id="modules">
<h3>Modules<a class="headerlink" href="#modules" title="Permalink to this headline"></a></h3>
<p>Resources in the FutureGrid HPC partitions have the Modules utility to
let you dynamically control your environment. Modules allows you to load
and unload packages and ensure a coherent working environment. The most
basic Modules commands let you add and remove packages from your
environment:</p>
<div class="highlight-python"><pre>module load &lt;package name&gt;/&lt;optional package version&gt;
module unload &lt;package name&gt;/&lt;optional package version&gt;</pre>
</div>
<p>To display the list of available modules:</p>
<div class="highlight-python"><pre>module avail</pre>
</div>
<p>To display the list of currently loaded modules:</p>
<div class="highlight-python"><pre>module list</pre>
</div>
<p>It is very important to make sure the proper modules are loaded in the
environment before you try to use FutureGrid HPC partitions. This
ensures that your $PATH, $LD_LIBRARY_PATH, $LD_PRELOAD and other
environment variables are properly set and that you can access the
programs and libraries you need. Additional information about the
Modules utility is available via &#8216;man module&#8217; on any FutureGrid login
node.</p>
</div>
<div class="section" id="managing-applications-with-torque">
<h3>Managing Applications with Torque<a class="headerlink" href="#managing-applications-with-torque" title="Permalink to this headline"></a></h3>
<p>To run any jobs on resources within FutureGrid&nbsp;HPC&nbsp;partitions (single
core, OpenMP or MPI&nbsp;jobs), users must use the job scheduler and a job
submission script. Users should NOT run jobs on the login or headnodes.
On FutureGrid machines, the job scheduler is the Torque (a variant of
PBS). To load torque into your environment, execute:</p>
<div class="highlight-python"><pre>-bash-3.2$ module load torque</pre>
</div>
<p>To run a serial job, you start by creating a job submission script
that both describes your job and will be executed on the compute nodes
by Torque. An example of a job script for India, Sierra, and Alamo that
runs <em>/bin/hostname</em> is:</p>
<div class="highlight-python"><pre>#!/bin/bash

#PBS -N hostname_test
#PBS -o hostname.out
#PBS -e hostname.err
#PBS -q short
#PBS -l nodes=1
#PBS -l walltime=00:20:00

/bin/hostname</pre>
</div>
<p>Options are passed to Torque on lines that begin with #PBS. The options
above are:</p>
<ul class="simple">
<li>-N: An optional job name</li>
<li>-o: The name of the file to write stdout to</li>
<li>-e: The name of the file to write stderr to</li>
<li>-q: The queue to submit the job to</li>
<li>-l: The resources needed by the job (in the case above, 1 node for 20
minutes)</li>
</ul>
<p>Additional information about the options that can be specified in a
submit script is available in the qsub manual page via &#8216;man qsub&#8217;. Note
that there are multiple queues available on each FutureGrid system:</p>
<ul class="simple">
<li>Alamo: short, long, default</li>
<li>Hotel: extended, batch, long and route</li>
<li>India: scalemp, batch, long and b534</li>
<li>Sierra: batch and long</li>
<li>Xray: batch</li>
</ul>
<p>You can find information (such as limits) that will help you select
which queue to use by running qstat -q on the login node for the system
you are interested in.</p>
<p>Once you have created a submission script, you can then use the Torque
qsub command to submit this job to be executed on the compute nodes:</p>
<div class="highlight-python"><pre>-bash-3.2$ qsub ring.sh
19095.master1.cm.cluster</pre>
</div>
<p>The qsub command outputs either a job identifier or an error message
describing why Torque would not accept your job. If your job is
submitted successfully, you can track its execution using the qstat
command:</p>
<div class="highlight-python"><pre>-bash-3.2$ qstat
Job id                    Name             User            Time Use S Queue
------------------------- ---------------- --------------- -------- - -----
...
19095.master1             hostname_test    user            00:00:00 R short
...</pre>
</div>
<p>If the system is busy, your job will initially be queued (Q) waiting for
resources to become available. It will then be in the running state (R),
and finally it will complete and not be visible in the qstat output. The
full set of Torque job states is provided in the qstat manual page via
man qstat on a FutureGrid login node. The stdout and stderr from your
job will be placed in the files you specified in your submission script.</p>
<p>A final Torque command you will use occasionally is the qdel command
that asks Torque to delete a job. If the job hasn&#8217;t begun running, it is
simply deleted from the queue. If the job has begun, it is killed on the
nodes it&#8217;s running on, and deleted from the queue.</p>
<p>A list of all available Torque commands is available from the <a class="reference external" href="http://www.clusterresources.com/torquedocs21/">Torque
manual page</a>.</p>
</div>
<div class="section" id="message-passing-interface-mpi">
<h3>Message Passing Interface (MPI)<a class="headerlink" href="#message-passing-interface-mpi" title="Permalink to this headline"></a></h3>
<p>The Message Passing Interface Standard (MPI) is a message passing
library standard based on the consensus of the MPI Forum, which has
dozens of participating organizations, including vendors, researchers,
software library developers, and users. The goal of the Message Passing
Interface is to establish a portable, efficient, and flexible standard
for message passing that will be widely used for writing message passing
programs. MPI is the <em>de facto</em> standard communication library for
almost all HPC systems, and is available in a variety of
implementations.</p>
<p>For more information, please visit:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.mpi-forum.org/">http://www.mpi-forum.org/</a></li>
<li><a class="reference external" href="http://www.mcs.anl.gov/research/projects/mpi/tutorial/">http://www.mcs.anl.gov/research/projects/mpi/tutorial/</a></li>
</ul>
<p>For more information on OpenMPI, the default MPI distribution on
FutureGrid, please visit:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.open-mpi.org/">http://www.open-mpi.org/</a></li>
</ul>
</div>
<div class="section" id="mpi-libraries">
<h3>MPI Libraries<a class="headerlink" href="#mpi-libraries" title="Permalink to this headline"></a></h3>
<p>The FutureGrid systems that support HPC-style usage have an MPI
implementation. In most cases, it is OpenMPI-1.4.x compiled with Intel
11.1 compilers.</p>
<table border="1" class="docutils">
<colgroup>
<col width="13%" />
<col width="18%" />
<col width="15%" />
<col width="25%" />
<col width="28%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>System</strong></td>
<td><strong>MPI version</strong></td>
<td><strong>Compiler</strong></td>
<td><strong>Infiniband Support</strong></td>
<td><strong>Module</strong></td>
</tr>
<tr class="row-even"><td>Alamo</td>
<td>OpenMPI 1.4.3</td>
<td>Intel 11.1</td>
<td>yes</td>
<td>openmpi</td>
</tr>
<tr class="row-odd"><td>Bravo</td>
<td>OpenMPI 1.4.2</td>
<td>Intel 11.1</td>
<td>no</td>
<td>openmpi</td>
</tr>
<tr class="row-even"><td>&nbsp;</td>
<td>OpenMPI 1.4.3</td>
<td>gcc 4.4.6</td>
<td>no</td>
<td>openmpi/1.4.3-gnu</td>
</tr>
<tr class="row-odd"><td>&nbsp;</td>
<td>OpenMPI 1.4.3</td>
<td>Intel 11.1</td>
<td>no</td>
<td>openmpi/1.4.3-intel</td>
</tr>
<tr class="row-even"><td>&nbsp;</td>
<td>OpenMPI 1.5.4</td>
<td>gcc 4.4.6</td>
<td>no</td>
<td>openmpi/1.5.4-[gnu,intel]</td>
</tr>
<tr class="row-odd"><td>Hotel</td>
<td>OpenMPI 1.4.3</td>
<td>gcc 4.1.2</td>
<td>yes</td>
<td>openmpi</td>
</tr>
<tr class="row-even"><td>India</td>
<td>OpenMPI 1.4.2</td>
<td>Intel 11.1</td>
<td>yes</td>
<td>openmpi</td>
</tr>
<tr class="row-odd"><td>Sierra</td>
<td>OpenMPI 1.4.2</td>
<td>Intel 11.1</td>
<td>no</td>
<td>openmpi</td>
</tr>
<tr class="row-even"><td>Xray</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>N/A</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>In cases where the OpenMPI is compiled with the Intel compilers,
loading the OpenMPI module will automatically load the Intel compilers
as a dependency:</p>
<div class="highlight-python"><pre>-bash-3.2$ module load openmpi
Intel compiler suite version 11.1/072 loaded
OpenMPI version 1.4.3 loaded</pre>
</div>
<div class="line-block">
<div class="line">Loading the OpenMPI module adds the MPI compilers to your $PATH</div>
</div>
<p>environment variable and the OpenMPI shared library directory to your
$LD_LIBRARY_PATH. This is an important step to ensure MPI applications
will compile and run successfully. Loading the torque module allows you
to submit jobs to the scheduler.</p>
</div>
<div class="section" id="compiling-mpi-applications">
<h3>Compiling MPI&nbsp;Applications<a class="headerlink" href="#compiling-mpi-applications" title="Permalink to this headline"></a></h3>
<p>To compile MPI applications, users have two options:</p>
<ol class="arabic simple">
<li>Use the MPI&nbsp;compilers instead of regular Intel/GNU compilers</li>
<li>Use the regular compilers (Intel/GNU) with MPI compilation flags</li>
</ol>
<p>We recommend using the MPI compilers to avoid compilation issues. This
is accomplished by making the following replacements:</p>
<ul class="simple">
<li>CC/icc/gcc with mpicc</li>
<li>CXX/icpc/g++ with mpicxx</li>
<li>F90/F77/FC/ifort/gfortran with mpif90</li>
</ul>
<p>Alternatively, for some codes that require intricate compilation flags
and complicated make systems, and where changing compilers is not an
option, you can edit the compilation/linking options for your codes.
These options are machine, compiler, and language dependent. To view the
options required for C, C++ and Fortran on any machine, you can issue
the commands mpicc-show, mpicxx-show, and mpif90-show. Extra care must
be taken when using these flags, as dependencies govern the order in
which they appear in the link line. Should you run into compilation
errors or problems, please submit a consulting ticket.</p>
<p>Assuming you have loaded the openmpi module into your environment,
you can compile a <a class="reference external" href="/tutorials/hpc/ring">simple MPI application</a> as
easily as executing:</p>
<div class="highlight-python"><pre>-bash-3.2$ mpicc -o ring ring.c</pre>
</div>
</div>
<div class="section" id="running-mpi-applications">
<h3>Running MPI&nbsp;Applications<a class="headerlink" href="#running-mpi-applications" title="Permalink to this headline"></a></h3>
<p>Once your MPI application is compiled, you run it on the compute nodes
of a cluster via Torque. An example of an MPI parallel job script for
India, Sierra, and Alamo that runs the ring application is:</p>
<div class="highlight-python"><pre>#!/bin/bash

#PBS -N ring_test
#PBS -o ring_$PBS_JOBID.out
#PBS -e ring_$PBS_JOBID.err
#PBS -q short
#PBS -l nodes=4:ppn=8
#PBS -l walltime=00:20:00

# make sure MPI is in the environment
module load openmpi

# launch the parallel application with the correct number of processs
# Typical usage: mpirun -np &lt;number of processes&gt; &lt;executable&gt; &lt;arguments&gt;
mpirun -np 32 ring -t 1000</pre>
</div>
<p>There are two important differences between this script and the submit
script shown previously. The first is that :ppn=8 is added to the
request for four nodes. What this does is indicate that your application
wants to allocate eight virtual processors per node. A virtual processor
corresponds to a processing core. Alamo, Hotel, India, and Sierra all
have eight cores per node, so the script above asks for exclusive access
to four nodes with a total of 32 cores. The second importand difference
from the previous submit script is that it executes mpirun with
arguments that describe your MPI application. Note that the number of
processes specified to mpirun is 32matching the 32 cores allocated by
Torque.</p>
<p>A minor difference between this script and the previous one is that
the environment variable $PBS_JOBID is used when creating the stdin and
stdout files. Torque sets a number of environment variables that you can
use in your submit script, starting with PBS_ .</p>
</div>
</div>
<div class="section" id="log-in-to-hpc-services">
<h2>Log in to HPC services<a class="headerlink" href="#log-in-to-hpc-services" title="Permalink to this headline"></a></h2>
<p>To access a FutureGrid system via Torque/Moab, you should ssh to the
login node for the system. The login node is one of the following:</p>
<ul class="simple">
<li>india.futuregrid.org</li>
<li>bravo.futuregrid.org</li>
<li>sierra.futuregrid.org</li>
<li>foxtrot.futuregrid.org</li>
<li>hotel.futuregrid.org</li>
<li>alamo.futuregrid.org</li>
<li>xray.futuregrid.org</li>
</ul>
<p>An example session follows:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ ssh sierra.futuregrid.org
Last login: Thu Aug 12 19:19:22 2010 from ....
Welcome to Sierra.FutureGrid.Org
$</pre>
</div>
</div></blockquote>
<p>Once you ssh into these nodes, you&#8217;ll have access to the HPC queuing
services for the machine you have logged into. You will enter into a
Unix/Linux shell in which you can enter the typical Unix commands.
However, access to the clusters is provided through&nbsp;Torque/Moab commands
from the command line.</p>
</div>
<div class="section" id="generating-ssh-keys-for-futuregrid-access">
<h2>Generating SSH Keys for FutureGrid Access<a class="headerlink" href="#generating-ssh-keys-for-futuregrid-access" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="key-generation">
<h2>Key Generation<a class="headerlink" href="#key-generation" title="Permalink to this headline"></a></h2>
<p>To gain access to FutureGrid&nbsp;Resources including HPC and Nimbus
services, you need to provide a public key to FutureGrid. We recommend
that you are familiar with public keys and have the understanding that
we do REQUIRE passphrase protected keys. To generate and send such a
key, please follow the following steps. To find more out about open
ssh&nbsp;you can also go to</p>
<ul class="simple">
<li><a class="reference external" href="http://openssh.com/manual.html">http://openssh.com/manual.html</a></li>
</ul>
<p>Other good resources include</p>
<ul class="simple">
<li><a class="reference external" href="http://help.github.com/key-setup-redirect">http://help.github.com/key-setup-redirect</a></li>
<li><a class="reference external" href="http://help.github.com/working-with-key-passphrases/">http://help.github.com/working-with-key-passphrases/</a></li>
<li><a class="reference external" href="http://www.dribin.org/dave/blog/archives/2007/11/28/ssh_agent_leopard/">http://www.dribin.org/dave/blog/archives/2007/11/28/ssh_agent_leopard/</a></li>
</ul>
<div class="section" id="generate-public-private-key-pair">
<h3><strong>1.&nbsp;</strong><strong>Generate Public/Private Key Pair</strong><a class="headerlink" href="#generate-public-private-key-pair" title="Permalink to this headline"></a></h3>
<p>First, you have to generate a key. You do this as follows:</p>
<ul class="simple">
<li>Step 1: use the command ssh-keygen -t rsa -C &lt;your-e-mail&gt; to
generate the key</li>
<li>Step 2:&nbsp;specifiy the KeyPair location and name. We recommend that you
use the default location if you do not yet have another key there.
&nbsp;e.g. /home/username/.ssh/id_rsa</li>
<li>Step 3: type user defined passphrase when asking passphrase for the
key</li>
</ul>
<p>Example:</p>
<div class="highlight-python"><pre>ssh-keygen -t rsa -C johndoe@indiana.edu

Generating public/private rsa key pair.
Enter file in which to save the key (/home/johndoe/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/johndoe/.ssh/id_rsa.
Your public key has been saved in /home/johndoe/.ssh/id_rsa.pub.
The key fingerprint is:
34:87:67:ea:c2:49:ee:c2:81:d2:10:84:b1:3e:05:59 johndoe@indiana.edu</pre>
</div>
</div>
<div class="section" id="list-the-result">
<h3><strong>2.&nbsp;</strong><strong>List the result</strong><a class="headerlink" href="#list-the-result" title="Permalink to this headline"></a></h3>
<p>You can find your key under the key location. As we user the .ssh
directory it will be located there.</p>
<div class="highlight-python"><pre>$ls -lisa ~/.ssh
-rw-------  1 johndoe johndoe        1743 2011-02-10 09:44 id_rsa
-rw-r--r--  1 johndoe johndoe         399 2011-02-10 09:44 id_rsa.pub</pre>
</div>
</div>
<div class="section" id="add-or-replace-passphrase-for-an-already-generated-key">
<h3><strong>3.&nbsp;</strong><strong>Add or Replace Passphrase for an Already Generated Key</strong><a class="headerlink" href="#add-or-replace-passphrase-for-an-already-generated-key" title="Permalink to this headline"></a></h3>
<p>In case you need to change your change passphrase, you can simply run
ssh-keygen -p command. Then specify the location of your current key,
and input (old and) new passphrases. There is no need to re-generate
keys.</p>
<div class="highlight-python"><pre>ssh-keygen -p

Enter file in which the key is (/home/johndoe/.ssh/id_rsa):
Enter old passphrase:
Key has comment '/home/johndoe/.ssh/id_rsa'
Enter new passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved with the new passphrase.</pre>
</div>
</div>
<div class="section" id="capture-the-public-key-for-futuregrid">
<h3><strong>4.</strong>&nbsp;&nbsp;<strong>Capture the Public Key for FutureGrid</strong><a class="headerlink" href="#capture-the-public-key-for-futuregrid" title="Permalink to this headline"></a></h3>
<p>Use a text editor to open the id_rsa.pub.&nbsp;Copy the <strong>entire</strong>
contents of this file into the ssh key field as part of your profile
information. You can now add this key to your keys at the following
page:</p>
</div>
<div class="section" id="key-management">
<h3>5. Key Management<a class="headerlink" href="#key-management" title="Permalink to this headline"></a></h3>
<p>This is a future section that will include material about how to use
ssh-add and keychain. You can find instructions on them via the
github&nbsp;link that we provided above. However, we are not github ;-)</p>
</div>
<div class="section" id="resetting-the-ssh-key">
<h3>6. Resetting the SSH key<a class="headerlink" href="#resetting-the-ssh-key" title="Permalink to this headline"></a></h3>
<p>Please follow the instructions given at</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/how-do-i-reset-my-ssh-key">https://portal.futuregrid.org/how-do-i-reset-my-ssh-key</a></li>
</ul>
</div>
<div class="section" id="i-still-can-not-access-fg-resources">
<h3>7. I still can not access FG resources<a class="headerlink" href="#i-still-can-not-access-fg-resources" title="Permalink to this headline"></a></h3>
<p>In order for you to access FG resources you must be in an active
project. Please make sure you join a project or create your own while
applying for one.</p>
<p>=</p>
</div>
</div>
<div class="section" id="working-with-hpc-job-services">
<h2>Working with HPC Job Services<a class="headerlink" href="#working-with-hpc-job-services" title="Permalink to this headline"></a></h2>
<div class="section" id="running-queued-jobs-as-part-of-the-hpc-services">
<h3>Running Queued Jobs as Part of the HPC Services<a class="headerlink" href="#running-queued-jobs-as-part-of-the-hpc-services" title="Permalink to this headline"></a></h3>
<p>To run a job in the HPC service, you need to create a job script that
tells the job manager how to run the job and how to handle things like
output and notifications. You can then submit your job to the scheduler,
monitor its progress in the job queue, and examine the output when it
finishes.</p>
<div class="section" id="a-simple-job-script">
<h4>&nbsp;A Simple Job Script<a class="headerlink" href="#a-simple-job-script" title="Permalink to this headline"></a></h4>
<p>An example job script looks like this:</p>
<blockquote>
<div><div class="highlight-python"><pre>#!/bin/bash
#PBS -N testjob
#PBS -l nodes=1:ppn=1
#PBS -q batch
#PBS -j oe
##PBS -M username@example.com
##PBS -m ae ##PBS -o testjob.out
##
## Everything following is run by the scheduler
##
sleep 10
echo -n "Host operating system version: "
uname -a
echo "Nodes allocated to this job: "
cat $PBS_NODEFILE
echo
sleep 10
##
## End of job script
##</pre>
</div>
</div></blockquote>
<p>In the job script, lines that begin with&nbsp;<strong>#PBS</strong>&nbsp;are directives to
the job scheduler. You can disable any of these lines by adding an
extra&nbsp;<strong>#</strong>character at the beginning of the line, for example:</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span class="c">##PBS -M username@example.com</span>
</pre></div>
</div>
</div></blockquote>
<p>This job script shows some common examples of directives that you might
want to use in your job scripts. The directives in this job script are
described below:</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span class="c">#!/bin/bash</span>
</pre></div>
</div>
</div></blockquote>
<p>This line isn&#8217;t strictly required, but it is added as a fail-safe in
case something unexpected happens. Normally, the job manager reads your
script and processes the directives, and then runs your script as a
normal shell script. This simply ensures that the system uses the
standard bash shell to run your script.</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span class="c">#PBS -N testjob</span>
</pre></div>
</div>
</div></blockquote>
<p>This line gives your job a name of&nbsp;<strong>testjob</strong>. This name will be used
by the job manager when it shows a job listing, and will be used for
your output file(s) unless you explicitly specify an output file.</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span class="c">#PBS -l nodes=1:ppn=1</span>
</pre></div>
</div>
</div></blockquote>
<p>This line tells the job manager what your job requires for resources. In
this case, your job is asking for one node (<strong>nodes=1</strong>) and at least
one processor per node (<strong>ppn=1</strong>). See the
[[Sw:Manual/PBSDirectives|PBSDirectives] page for other options you can
specify here.</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span class="c">#PBS -q batch</span>
</pre></div>
</div>
</div></blockquote>
<p>This line tells the job manager which job queue your job should be sent
to. Each job queue has different characteristics, such as the maximum
time a job is allowed to run, or the maximum number of nodes a job can
use.</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span class="c">#PBS -j oe</span>
</pre></div>
</div>
</div></blockquote>
<p>This line tells the job manager to join the job standard output and
standard error into a single file. For jobs with a small amount of
output, this is usually helpful. If your job produces a lot of standard
output, it may be helpful to keep the files separate so you can easily
locate error messages in the single error file.</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span class="c">##PBS -M username@example.com</span>
</pre></div>
</div>
</div></blockquote>
<p>Note that this line is a comment since it starts with&nbsp;<strong>##</strong>&nbsp;instead
of&nbsp;<strong>#PBS</strong>. If you remove the first&nbsp;<strong>#</strong>, this line will set the
email address that will get notified about events related to this job.
The events that get reported are set by the next line.</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span class="c">##PBS -m ae</span>
</pre></div>
</div>
</div></blockquote>
<p>Again, note that this line is commented out. If you remove the
first&nbsp;<strong>#</strong>, this line will send email whenever the job fails
(or&nbsp;<strong>a</strong>borts) (<strong>a</strong>&nbsp;option), and when the job ends (<strong>e</strong>&nbsp;option).
This is particularly helpful if your job has to wait a long time in the
queue before it runs.</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span class="c">##PBS -o testjob.out</span>
</pre></div>
</div>
</div></blockquote>
<p>Again, note that this line is commented out. If you remove the
first&nbsp;<strong>#</strong>, this line will specify the file name to be used for job
output.</p>
</div>
<div class="section" id="submitting-your-job">
<h4>&nbsp; Submitting Your Job<a class="headerlink" href="#submitting-your-job" title="Permalink to this headline"></a></h4>
<p>You can submit your job with the&nbsp;<strong>qsub</strong>&nbsp;or&nbsp;<strong>msub</strong>&nbsp;commands.
The&nbsp;<strong>msub</strong>&nbsp;and&nbsp;<strong>qsub</strong>&nbsp;are almost identical, and can mostly be used
interchangeably. See the respective man pages for specific differences.
Neither submission command provides much output.&nbsp;Examples of a job
submission using both commands follows:</p>
<p>Using&nbsp;<strong>msub</strong>:</p>
<blockquote>
<div><div class="highlight-python"><pre>[62]s1::gpike&gt; msub testjob.pbs
292250
[63]s1::gpike&gt;</pre>
</div>
</div></blockquote>
<p>Using&nbsp;<strong>qsub</strong>:</p>
<blockquote>
<div><div class="highlight-python"><pre>[63]s1::gpike&gt; qsub testjob.pbs
292251.s82
[64]s1::gpike&gt;</pre>
</div>
</div></blockquote>
<p>In both cases, the number that gets returned is the job number that the
scheduler assigned to your job. In the case of&nbsp;<strong>qsub</strong>, the job
number is followed by the host name where you submitted the job.</p>
</div>
</div>
<div class="section" id="monitoring-your-job">
<h3>Monitoring Your Job<a class="headerlink" href="#monitoring-your-job" title="Permalink to this headline"></a></h3>
<p>To monitor your job after it has been submitted, you can use
the&nbsp;<strong>qstat</strong>&nbsp;or&nbsp;<strong>showq</strong>&nbsp;commands. Both commands will show you the
state of the job manager, but the information is displayed in different
formats. In general, the&nbsp;<strong>showq</strong>&nbsp;command gives more complete
information, and in a form that is a bit easier to read.
The&nbsp;<strong>qstat</strong>&nbsp;command gives a very concise listing of the job queue,
and in some instances this may give you a better quick overview of the
resource.</p>
<p>Using the test job script as an example, here is the output from
the&nbsp;<strong>showq</strong>&nbsp;command:</p>
<blockquote>
<div><div class="highlight-python"><pre>[66]s1::gpike&gt; showq
active jobs
------------------------
JOBID    USERNAME       STATE PROCS    REMAINING            STARTTIME
292252   greg       Running     16        3:59:59 Tue Aug 17 09:02:40
1 active job 16 of 264 processors in use by local jobs (6.06%)
                  2 of 33 nodes active (6.06%) eligible jobs
----------------------
JOBID    USERNAME       STATE PROCS    REMAINING            STARTTIME
0 eligible jobs blocked jobs
-----------------------
JOBID    USERNAME       STATE PROCS    REMAINING            STARTTIME
0 blocked jobs
Total job: 1</pre>
</div>
</div></blockquote>
<p>You can see the output is divided into three sections:&nbsp;<strong>active
jobs</strong>,&nbsp;<strong>eligible jobs</strong>, and&nbsp;<strong>blocked jobs</strong>.</p>
<p><strong>1. Active jobs</strong>&nbsp;are jobs that are currently running on the resource.</p>
<p><strong>2.</strong><strong>Eligible jobs</strong>&nbsp;are jobs that are waiting for nodes to become
available before they can run. As a general rule, jobs are listed in the
order that they will be scheduled, but scheduling algorithms may change
the order over time.</p>
<p><strong>3.</strong><strong>Blocked jobs</strong>&nbsp;are jobs that the scheduler cannot run for some
reason. Usually a job becomes blocked because it is requesting something
that is impossible, such as more nodes than currently exist, or more
processors per node than are installed.</p>
<p>Using the test job as an example again, here is the output from
the&nbsp;<strong>qstat</strong>&nbsp;command:</p>
<blockquote>
<div><div class="highlight-python"><pre>[109]i136::gpike&gt; qstat
Job id                             Name               User          Time Use S Queue
------------------------- --------------------- ------------------- -------- - -----
1981.i136                       sub19327.sub      inca               00:00:00 C batch
1982.i136                       testjob           greg                      0 R batch</pre>
</div>
</div></blockquote>
<p>The&nbsp;<strong>qstat</strong>&nbsp;command provides output in six columns:</p>
<ol class="arabic simple">
<li>Job id is the identifier assigned to your job.</li>
<li>Name is the name that you assigned to your job.</li>
<li>User is the username of the person who submitted the job.</li>
<li>Time Use is the amount of time the job has been running.</li>
<li>S shows the job state. Common job states are R for a running job, Q
for a job that is queued and waiting to run, C for a job that has
completed, and H for a job that is being held.</li>
<li>Queue is the name of the job queue where your job will run.</li>
</ol>
</div>
<div class="section" id="examining-your-job-output">
<h3>Examining Your Job Output<a class="headerlink" href="#examining-your-job-output" title="Permalink to this headline"></a></h3>
<p>If you gave your job a name with the&nbsp;<strong>#PBS -N &lt;jobname&gt;</strong>&nbsp;directive
in your job script or by specifying the job name on the command line,
your job output will be available in a file named&nbsp;<strong>jobname.o######</strong>,
where the&nbsp;<strong>######</strong>&nbsp;is the job number assigned by the job manager.
You can type&nbsp;<strong>ls jobname.o*</strong>&nbsp;to see all output files from the same
job name.</p>
<p>If you explicitly name an output file with the&nbsp;<strong>#PBS -o
&lt;outfile&gt;</strong>&nbsp;directive in your job script or by specifying the output
file on the command line, your output will be in the file you specified.
If you run the job again, the output file will be overwritten.</p>
<p>If you don&#8217;t specify any output file, your job output will have the same
name as your job script, and will be numbered in the same manner as if
you had specified a job name (<strong>jobname,o######</strong>).</p>
</div>
</div>
<div class="section" id="analyzing-code-performance">
<h2>Analyzing Code Performance<a class="headerlink" href="#analyzing-code-performance" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="papi">
<h2>PAPI<a class="headerlink" href="#papi" title="Permalink to this headline"></a></h2>
<p><img alt="image29" src="https://portal.futuregrid.org/sites/default/files/u30/icl_footer.gif" /></p>
<p><strong>Summary</strong></p>
<p><a class="reference external" href="http://icl.cs.utk.edu/papi/overview/index.html">PAPI</a>&nbsp;is an acronym
for&nbsp;<strong>P</strong>erformance&nbsp;<strong>A</strong>pplication&nbsp;<strong>P</strong>rogramming&nbsp;<strong>I</strong>nterface.
The PAPI Project is being developed at the Computer Science Department
of the&nbsp;<a class="reference external" href="http://icl.cs.utk.edu/">University of Tennessees Innovative Computing
Laboratory</a>. This project was created to
design, standardize, and implement a portable and efficient API
(Application Programming Interface) to access the hardware performance
counters found on most modern microprocessors.&nbsp;&nbsp;For more information,
please see the user guides listed below. &nbsp;<a class="reference external" href="http://icl.cs.utk.edu/projects/papi/wiki/User_Guide">Read more
...</a></p>
<p><strong>Availability</strong></p>
<p>PAPI 4.2.0 is available on the Bravo machine and PAPI version 3.6.2.2 is
available on the Xray machine. To load PAPI on Bravo, &#8216;module add papi&#8217;
as below:</p>
<blockquote>
<div><tt class="docutils literal"><span class="pre">%</span> <span class="pre">module</span> <span class="pre">add</span> <span class="pre">papi</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">papi</span> <span class="pre">version</span> <span class="pre">4.2.0</span> <span class="pre">loaded</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">%</span> <span class="pre">env</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">papi</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">MANPATH=/opt/papi-4.2.0/man:/usr/share/man</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">LD_LIBRARY_PATH=/opt/papi-4.2.0/lib:/N/u/inca/openssl/lib:</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">PATH=/opt/papi-4.2.0/bin:...</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">PAPI_ROOT=/opt/papi-4.2.0</span></tt></div></blockquote>
<p>To load PAPI on Xray, type &#8216;module add xt-papi&#8217; as below:</p>
<blockquote>
<div><tt class="docutils literal"><span class="pre">%</span> <span class="pre">module</span> <span class="pre">add</span> <span class="pre">xt-papi</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">%</span> <span class="pre">env</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">PAPI</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">PAPI_POST_LINK_OPTS=</span> <span class="pre">-L/opt/xt-tools/papi/3.6.2.2/lib</span> <span class="pre">-lpapi</span> <span class="pre">-lpfm</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">PAPI_VERSION=3.6.2.2</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">PAPI_INCLUDE_OPTS=</span> <span class="pre">-I/opt/xt-tools/papi/3.6.2.2/include</span></tt></div></blockquote>
<p>Our plans are to make PAPI 4.2.0 available on Alamo, India, Sierra, and
Hotel when the machines are upgraded to Redhat 6.</p>
<p><strong>PAPI User Guide</strong></p>
<ul class="simple">
<li><a class="reference external" href="http://icl.cs.utk.edu/projects/papi/wiki/User_Guide">User Guide</a></li>
</ul>
</div>
<div class="section" id="vampir">
<h2>Vampir<a class="headerlink" href="#vampir" title="Permalink to this headline"></a></h2>
<p><strong>Introduction</strong></p>
<p>Performance optimization is a key issue for the development of efficient
parallel software applications. Vampir provides a manageable framework
for analysis, which enables developers to quickly display program
behavior at any level of detail. Detailed performance data obtained from
a parallel program execution can be analyzed with a collection of
different performance views. Intuitive navigation and zooming are the
key features of the tool, which help to quickly identify inefficient or
faulty parts of a program code. Vampir implements optimized event
analysis algorithms and customizable displays which enable a fast and
interactive rendering of very complex performance monitoring data.
Ultra-large data volumes can be analyzed with a parallel version of
Vampir, which is available on request. Vampir has a product history of
more than 15 years and is well established on Unix-based HPC systems.
This tool experience is now available for HPC systems that are based on
Microsoft Windows HPC Server 2008. This new Windows edition of Vampir
combines modern scalable event processing techniques with a fully
redesigned graphical user interface.</p>
<div class="section" id="vampir-on-futuregrid">
<h3><strong>Vampir on FutureGrid</strong><a class="headerlink" href="#vampir-on-futuregrid" title="Permalink to this headline"></a></h3>
<p>VampirServer is currently available on India
at&nbsp;/N/soft/x86_64/el5/india/vampirserver. &nbsp;The VampirTrace modules are
installed on Alamo, Hotel, India, and Sierra. &nbsp;To load, type &#8216;module
load vampirtrace&#8217;.</p>
</div>
<div class="section" id="event-based-performance-tracing-and-profiling">
<h3><strong>Event-based Performance Tracing and Profiling</strong><a class="headerlink" href="#event-based-performance-tracing-and-profiling" title="Permalink to this headline"></a></h3>
<p>In software analysis, the term profiling refers to the creation of
tables which summarize the runtime behavior of programs by means of
accumulated performance measurements. Its simplest variant lists all
program functions in combination with the number of invocations and the
time that was consumed. This type of profiling is also called inclusive
profiling, as the time spent in subroutines is included in the
statistics computation. A commonly applied method for analyzing details
of parallel program runs is to record so-called trace log files during
runtime. The data collection process itself is also referred to as
tracing a program. Unlike profiling, the tracing approach records timed
application events like function calls and message communication as a
combination of timestamp, event type, and event specific data. This
creates a stream of events, which allows very detailed observations of
parallel programs. With this technology, synchronization and
communication patterns of parallel program runs can be traced and
analyzed in terms of performance and correctness. The analysis is
usually carried out in a postmortem step, i. e., after completion of the
program. Needless to say, program traces can also be used to calculate
the profiles mentioned above. Computing profiles from trace data allows
arbitrary time intervals and process groups to be specified. This is in
contrast to <em>fixed profiles accumulated during runtime.</em></p>
</div>
<div class="section" id="the-open-trace-format-otf">
<h3><strong>The Open Trace Format (OTF)</strong><a class="headerlink" href="#the-open-trace-format-otf" title="Permalink to this headline"></a></h3>
<p>The Open Trace Format (OTF) was designed as a well-defined trace format
with open, public domain libraries for writing and reading. This open
specification of the trace information provides analysis and
visualization tools like Vampir to operate efficiently at large scale.
The format addresses large applications written in an arbitrary
combination of Fortran77, Fortran (90/95/etc.), C, and C++.</p>
<p><img alt="image30" src="https://portal.futuregrid.org/sites/default/files/images/otf_0.png" /></p>
<p><strong>Representation of Streams by Multiple Files</strong></p>
<p>OTF uses a special ASCII data representation to encode its data items
with numbers and tokens in hexadecimal code without special prefixes.
That enables a very powerful format with respect to storage size, human
readability, and search capabilities on timed event records. In order to
support fast and selective access to large amounts of performance trace
data, OTF is based on a stream-model, i.e., single separate units
representing segments of the overall data. OTF streams may contain
multiple independent processes, whereas a process belongs to a single
stream exclusively. As shown in the figure, each stream is represented
by multiple files, which store definition records, performance events,
status information, and event summaries separately. A single global
master file holds the necessary information for the process to stream
mappings. Each file name starts with an arbitrary common prefix defined
by the user. The master file is always named {name}.otf. The global
definition file is named {name}.0.def. Events and local definitions are
placed in files {name}.x.events and {name}.x.defs, where the latter
files are optional. Snapshots and statistics are placed in files named
{name}.x.snaps and {name}.x.stats, which are also optional.</p>
<p><strong>Note</strong>: Open the master file (*.otf) to load a trace. When
copying, moving, or deleting traces, it is important to take all
according files into account; otherwise, Vampir will render the whole
trace invalid! Good practice is to hold all files belonging to one trace
in a dedicated directory. Detailed information about the Open Trace
Format can be found in the <a class="reference external" href="http://www.tu-dresden.de/zih/otf%20">*Open Trace Format
(OTF)*&nbsp;documentation</a>.</p>
</div>
</div>
<div class="section" id="id13">
<h2><strong>Getting Started</strong><a class="headerlink" href="#id13" title="Permalink to this headline"></a></h2>
<div class="section" id="generation-of-trace-data">
<h3><strong>Generation of Trace Data</strong><a class="headerlink" href="#generation-of-trace-data" title="Permalink to this headline"></a></h3>
<p>The generation of trace files for the (Vampir) performance visualization
tool requires a working monitoring system to be attached to your
parallel program. Contrary to Windows HPC Server 2008  whereby the
performance monitor is integrated into the operating system  recording
performance under Linux is done by a separate performance monitor. We
recommend our VampirTrace monitoring facility, which is available as
open source software. During a program run of an application,
VampirTrace generates an OTF trace file, which can be analyzed and
visualized by Vampir. The VampirTrace library allows MPI communication
events of a parallel program to be recorded in a trace file.
Additionally, certain program-specific events can also be included. To
record MPI communication events, simply relink the program with the
VampirTrace library. A new compilation of the program source code is
only necessary if program-specific events should be added. Detailed
information on the installation and usage of VampirTrace can be found at
<a class="reference external" href="https://portal.futuregrid.org/manual/vampir/trace">VampirTrace</a>.</p>
</div>
<div class="section" id="enabling-performance-tracing">
<h3><strong>Enabling Performance Tracing</strong><a class="headerlink" href="#enabling-performance-tracing" title="Permalink to this headline"></a></h3>
<p>To perform measurements with VampirTrace, the application program needs
to be instrumented. VampirTrace handles this automatically by default,
while manual instrumentation is also possible. All the necessary
instrumentation of user functions, MPI, and OpenMP events is handled by
the compiler wrappers of VampirTrace (vtcc, vtcxx, vtf77, vtf90). All
compile and link commands in the used makefile should be replaced by the
VampirTrace compiler wrapper, which performs the necessary
instrumentation of the program and links the suitable VampirTrace
library. Automatic instrumentation is the most convenient method to
instrument your program. Therefore, simply use the compiler wrappers
without any parameters, e.g.:</p>
<blockquote>
<div><div class="highlight-python"><pre>vtf90 hello.f90 -o hello</pre>
</div>
</div></blockquote>
<p>For manual instrumentation with the VampirTrace API, simply include:</p>
<blockquote>
<div><div class="highlight-python"><pre>vt_user.inc (Fortran)

vt_user.h (C, C++)</pre>
</div>
</div></blockquote>
<p>and label any user defined sequence of statements for instrumentation as
follows:</p>
<blockquote>
<div><div class="highlight-python"><pre>VT_USER_START(name) ... VT_USER_END(name)</pre>
</div>
</div></blockquote>
<p>in Fortran and C, respectively, and in C++ as follows:</p>
<blockquote>
<div><div class="highlight-python"><pre>VT_TRACER(``name);</pre>
</div>
</div></blockquote>
<p>Afterwards, use</p>
<blockquote>
<div><div class="highlight-python"><pre>vtcc -DVTRACE hello.c -o hello</pre>
</div>
</div></blockquote>
<p>to combine the manual instrumentation with automatic compiler
instrumentation or</p>
<blockquote>
<div><div class="highlight-python"><pre>vtcc -vt:inst manual -DVTRACE hello.c -o hello</pre>
</div>
</div></blockquote>
<p>to prevent an additional compiler instrumentation.</p>
</div>
<div class="section" id="tracing-an-application">
<h3><strong>Tracing an Application</strong><a class="headerlink" href="#tracing-an-application" title="Permalink to this headline"></a></h3>
<p>Running a VampirTrace instrumented application should normally result in
an OTF trace file in the current working directory where the application
was executed. On Linux, Mac OS, and Sun Solaris, the default name of the
trace file will be equal to the application name. For other systems, the
default name is&nbsp;<em>a.otf</em>&nbsp;but can be defined manually by setting the
environment variable VT_FILE_PREFIX to the desired name. After a run
of an instrumented application, the traces of the single processes need
to be unified in terms of timestamps and event IDs. In most cases, this
happens automatically. If it is necessary to perform unification of
local traces manually, use the following command:</p>
<blockquote>
<div><div class="highlight-python"><pre>vtunify &lt;nproc&gt;</pre>
</div>
</div></blockquote>
<p>If VampirTrace was built with support for OpenMP and/or MPI, it is
possible to speed up the unification of local traces significantly. To
distribute the unification on multiple processes, the MPI parallel
version vtunify-mpi can be used as follows:</p>
<blockquote>
<div><div class="highlight-python"><pre>mpirun -np &lt;nranks&gt; vtunify-mpi &lt;nproc&gt;</pre>
</div>
</div></blockquote>
</div>
<div class="section" id="starting-vampir-and-loading-a-trace-file">
<h3><strong>Starting Vampir and Loading a Trace File</strong><a class="headerlink" href="#starting-vampir-and-loading-a-trace-file" title="Permalink to this headline"></a></h3>
<p>To open a trace file, from the &#8220;File&#8221; menu, select &#8220;Open...&#8221;. This will
provide the file-open dialog depicted below. It is possible to filter
the files in the list. The file type input selector determines the
visible files. The default &#8220;OTF Trace Files (*.otf )&#8221; shows only files
that can be processed by the tool. All file types can be displayed by
using &#8220;All Files (*)&#8221;. Alternatively, on Windows, a command-line
invocation is possible:</p>
<blockquote>
<div><div class="highlight-python"><pre>C:\Program Files\Vampir\Vampir.exe [trace file]</pre>
</div>
</div></blockquote>
<p>To open multiple trace files at once, you can take them one after
another as command-line arguments:</p>
<blockquote>
<div><div class="highlight-python"><pre>C:\Program Files\Vampir\Vampir.exe [file 1]...[file n]</pre>
</div>
</div></blockquote>
<p>It is also possible to start the application by double-clicking on an
*.otf file (if Vampir was associated with *.otf files during the
installation process). The trace files to be loaded have to be compliant
with the Open Trace Format (OTF) standard. Microsoft HPC Server 2008 is
shipped with the translator program etl2otf.exe, which produces
appropriate input files.</p>
<p><img alt="image31" src="https://portal.futuregrid.org/sites/default/files/images/open_file.png" /></p>
<p><strong>Loading a Trace Log File in Vampir</strong></p>
<p>While Vampir is loading the trace file, an empty &#8220;Trace View&#8221; window
with a progress bar at the bottom opens. After Vampir loaded the trace
data completely, a default set of charts will appear. The illustrated
loading process can be interrupted at any point of time by clicking on
the cancel button in the lower right corner. Because events in the trace
file are traversed one after another, the GUI will also open, but will
show only the earliest information from the tracefile. For huge
tracefiles with performance problems assumed to be at the beginning,
this proceeding is a suitable strategy to save time.</p>
<p><img alt="image32" src="https://portal.futuregrid.org/sites/default/files/images/cancel_loading_resize.png" /></p>
<p><strong>Progress Bar and Cancel Loading Button</strong></p>
<p>Basic functionality and navigation elements are described
in&nbsp;<a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#Basics">Basics</a>.
The available charts and the information provided by them are explained
in&nbsp;<a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#Performance_Data_Visualization">Performance_Data_Visualization</a>.</p>
</div>
</div>
<div class="section" id="basics">
<h2><strong>Basics</strong><a class="headerlink" href="#basics" title="Permalink to this headline"></a></h2>
<p>After loading has been completed, the&nbsp;<em>Trace View</em>&nbsp;window title
displays the trace file&#8217;s name. By default, the&nbsp;<em>Charts</em>&nbsp;toolbar and
the&nbsp;<em>Zoom Toolbar</em>&nbsp;are available.</p>
<p><img alt="image33" src="https://portal.futuregrid.org/sites/default/files/images/Startup.png" /></p>
<p><strong>Trace View Window with Charts Toolbar (A) and Zoom Toolbar (B)</strong></p>
<p>Furthermore, the default set of charts is opened automatically after
loading has been finished. The charts can be divided into three groups:
timeline, statistical, and informational charts. Timeline charts show
detailed event-based information for arbitrary time intervals, while
statistical charts reveal accumulated measures computed from the
corresponding event data. Informational charts provide additional or
explanatory information regarding timeline and statistical charts. All
available charts can be opened with the&nbsp;<em>Charts</em>toolbar (explained
in <a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#The_Charts_Toolbar">The Charts
Toolbar</a>).
In the following section, we will explain the basic functions of the
Vampir GUI which are generic to all charts.</p>
<div class="section" id="chart-arrangement">
<h3><strong>Chart Arrangement</strong><a class="headerlink" href="#chart-arrangement" title="Permalink to this headline"></a></h3>
<p>The utility of charts can be increased by correlating them and their
provided information. Vampir supports this mode of operation by allowing
you to display multiple charts at the same time. Charts that display a
sequence of events such as the&nbsp;<em>Master Timeline</em>&nbsp;and the&nbsp;<em>Process
Timeline</em>chart are aligned vertically. This alignment ensures that the
temporal relationship of events is preserved across chart boundaries.
The user can arrange the placement of the charts according to his
preferences by dragging them into the desired position. When the left
mouse button is pressed while the mouse pointer is located above a
placement decoration, the layout engine will give visual clues as to
where the chart may be moved. As soon as the user releases the left
mouse button, the chart arrangement will be changed according to his
intentions. The entire procedure is depicted in figures below. The
flexible display architecture also allows increasing or decreasing the
screen space that is used by a chart. Charts of particular interest may
get more space in order to render information in more detail.</p>
<p><img alt="image34" src="https://portal.futuregrid.org/sites/default/files/images/Display_arranging_a.png" /></p>
<p><strong>Moving and Arranging Charts in the Trace View Window</strong></p>
<p><img alt="image35" src="https://portal.futuregrid.org/sites/default/files/images/Display_arranging_b.png" /></p>
<p><strong>Moving and Arranging Charts in the Trace View Window</strong></p>
<p><img alt="image36" src="https://portal.futuregrid.org/sites/default/files/images/Custom_arrangement.png" /></p>
<p><strong>A Custom Chart Arrangement in the Trace View Window</strong></p>
<p><img alt="image37" src="https://portal.futuregrid.org/sites/default/files/images/close_display.png" /></p>
<p><strong>Closing (right) and Undocking (left) a Chart</strong></p>
<p>The&nbsp;<em>Trace View</em>&nbsp;window can host an arbitrary number of charts. Charts
can be added by clicking on the respective&nbsp;<em>Charts</em>&nbsp;toolbar icon or
the corresponding&nbsp;<em>Chart</em>&nbsp;menu entry. With a few more clicks, charts
can be combined to a custom chart arrangement. Customized layouts can be
saved as described in&nbsp;<a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#Saving_Policy">Saving
Policy</a>.
Every chart can be undocked or closed by clicking the dedicated icon in
its upper right corner. Undocking a chart means to free the chart from
the current arrangement and present it in its own window.</p>
<p><img alt="image38" src="https://portal.futuregrid.org/sites/default/files/images/Undocking_1.png" /></p>
<p><strong>Undocking of a Chart</strong></p>
<p><img alt="image39" src="https://portal.futuregrid.org/sites/default/files/images/Undocking_2.png" /></p>
<p><strong>Docking a Chart</strong></p>
<p>Considering that labels (e.g., those showing names or values of
functions) often need more space to show their whole text, there is a
further form of resizing/arranging. In order to read labels completely,
it is possible to resize the distribution of space owned by the labels
and the graphical representation in a chart. When one hovers over the
blank space between labels and graphical representation, a moveable
separator appears. After clicking a separator decoration, moving the
mouse while holding the left mouse button causes resizing.</p>
<p><img alt="image40" src="https://portal.futuregrid.org/sites/default/files/images/Resize_labels.png" /></p>
<p><strong>Resizing Labels: (A) Hover over a Separator Decoration; (B) Drag and
Drop the Separator</strong></p>
</div>
<div class="section" id="context-menus">
<h3><strong>Context Menus</strong><a class="headerlink" href="#context-menus" title="Permalink to this headline"></a></h3>
<p>All of the chart displays have their own context menus with common
entries as well as display-specific ones. In the following section, only
the most common entries will be discussed. A context menu can be
accessed by right clicking in the display window. Common entries are:</p>
<ul class="simple">
<li><strong>Reset Zoom:</strong> Go back to the initial state in horizontal zooming.</li>
<li><strong>Reset Vertical Zoom:</strong> Go back to the initial state in vertical
zooming.</li>
<li><strong>Set Metric:</strong> Change values which should be represented in the
chart, e.g.&nbsp;<em>Exclusive Time</em>&nbsp;to&nbsp;<em>Inclusive Time</em>.</li>
<li><strong>Sort By:</strong>Rearrange values or bars by a certain characteristic.</li>
</ul>
</div>
<div class="section" id="zooming">
<h3><strong>Zooming</strong><a class="headerlink" href="#zooming" title="Permalink to this headline"></a></h3>
<p>Zooming is a key feature of Vampir. In most charts it is possible to
zoom in and out to get abstract and detailed views of the visualized
data. In the timeline charts, zooming produces a more detailed view of a
special time interval and therefore reveals new information that could
not be seen in the larger section. Short function calls in the&nbsp;<em>Master
Timeline</em>&nbsp;may not be visible unless an appropriate zooming level has
been reached. If the execution time of these short functions is too
short regarding the pixel resolution of your computer display, the
selection of a shorter time interval is required. Note: Other charts can
be affected when zooming in timeline displays: The interval chosen in a
timeline chart such as&nbsp;<em>Master Timeline</em>&nbsp;or&nbsp;<em>Process Timeline</em>&nbsp;also
defines the time interval for the calculation of accumulated
measurements in the statistical charts. Statistical charts like
the&nbsp;<em>Function Summary</em>&nbsp;provide zooming of statistic values. In these
cases zooming does not affect any other chart. Zooming is disabled in
the&nbsp;<em>Pie Chart</em>&nbsp;mode of the&nbsp;<em>Function Summary</em>&nbsp;reachable via context
menu under&nbsp;<em>Set Chart Mode-&gt;Pie Chart</em>.</p>
<p><img alt="image41" src="https://portal.futuregrid.org/sites/default/files/images/Zooming.png" /></p>
<p><strong>Zooming within a Chart</strong></p>
<p>To zoom into an area, click and hold the left mouse button and select
the area. It is possible to zoom horizontally and in some charts also
vertically. Horizontal zooming in the&nbsp;<em>Master Timeline</em>&nbsp;defines the
time interval to be visualized whereas vertical zooming selects a group
of processes to be displayed. To scroll horizontally move the slider at
the bottom or use the mouse wheel. Additionally, the zoom can be
accessed with help of the&nbsp;<em>Zoom Toolbar</em>&nbsp;by dragging the borders of
the selection rectangle or scrolling down the mouse wheel. To return to
the previous zooming state, the global &#8220;Undo&#8221; is provided that in the
&#8220;Edit&#8221; menu; alternatively, press &#8220;Ctrl+Z&#8221; to revert to the last zoom.
Accordingly, a zooming action can be repeated by selecting &#8220;Redo&#8221; in the
&#8220;Edit&#8221; menu or pressing &#8220;Ctrl+Shift+Z&#8221;. Both functions work
independently of the current mouse position. Next to &#8220;Undo&#8221; and &#8220;Redo&#8221;
it is shown which kind of action in which display could be undone and
redone, respectively. To get back to the initial state of zooming in a
fast way select&nbsp;<em>Reset Horizontal Zoom</em>&nbsp;or&nbsp;<em>Reset Vertical Zoom</em>&nbsp;in
the context menu of the desired timeline display. To reset zoom is also
an action that can be reverted by &#8220;Undo&#8221;.</p>
</div>
<div class="section" id="the-zoom-toolbar">
<h3><strong>The Zoom Toolbar</strong><a class="headerlink" href="#the-zoom-toolbar" title="Permalink to this headline"></a></h3>
<p>Vampir provides a&nbsp;<em>Zoom Toolbar</em> that can be used for zooming and
navigation in the trace data. It is situated in the upper right corner
of the <em>Trace View</em>&nbsp;window. Of course it is possible to drag and drop it
as desired. The&nbsp;<em>Zoom Toolbar</em>&nbsp;offers an overview of the data
displayed in the corresponding charts. The current zoomed area can be
seen highlighted as a rectangle within the&nbsp;<em>Zoom Toolbar</em>. Clicking on
one of the two boundaries and moving it (with left mouse button held) to
the intended position executes horizontal zooming in all charts.
<strong>Note</strong>: Instead of dragging boundaries, it is also possible to use
the mouse wheel for zooming. Hover over the&nbsp;<em>Zoom Toolbar</em>&nbsp;and scroll
up to zoom in and scroll down to zoom out. Dragging the zoom area
changes the section that is displayed without changing the zoom factor.
For dragging, click in the highlighted zoom area and drag and drop it to
the desired region. If the user double clicks in the&nbsp;<em>Zoom Toolbar</em>,
the initial zooming state is reverted to.</p>
<p><img alt="image42" src="https://portal.futuregrid.org/sites/default/files/images/Zoom_toolbar.png" /></p>
<p><strong>Zooming and Navigation within the Zoom Toolbar: (A+B) Zooming in/out
with Mouse Wheel; (C) Scrolling by Moving the Highlighted Zoom Area; (D)
Zooming by Selecting and Moving a Boundary of the Highlighted Zoom
Area</strong></p>
<p>The colors represent user-defined groups of functions or activities.
Please note that all charts added to the&nbsp;<em>Trace View</em>&nbsp;window will
adapt their statistics information according to this time interval
selection. The&nbsp;<em>Zoom Toolbar</em>&nbsp;can be disabled and enabled with the
toolbar&#8217;s context menu entry&nbsp;<em>Zoom Toolbar</em>.</p>
</div>
<div class="section" id="the-charts-toolbar">
<h3><strong>The Charts Toolbar</strong><a class="headerlink" href="#the-charts-toolbar" title="Permalink to this headline"></a></h3>
<p>Use the&nbsp;<em>Charts</em>&nbsp;toolbar to open instances of the different charts. It
is situated in the upper left corner of the main window by default. Of
course, it is also possible to drag and drop it as desired.
The&nbsp;<em>Charts</em>&nbsp;toolbar can be disabled with the toolbar&#8217;s context menu
entry&nbsp;<em>Charts</em>. The table below shows the different icons representing
the charts in&nbsp;<em>Charts</em>&nbsp;toolbar. The icons are arranged in three
groups, divided by a small separator. The first group represents
timeline charts, whose zooming states affect all other charts. The
second group consists of statistical charts, providing special
information and statistics for a chosen interval. Vampir allows multiple
instances for charts of these categories. The last group comprises
informational charts, providing specific textual information or legends.
Only one instance of an informational chart can be opened at a time.</p>
<p><strong>Icons of the Toolbar</strong></p>
<p>Icon</p>
<p>Name</p>
<p>Description</p>
<p><img alt="image43" src="https://portal.futuregrid.org/sites/default/files/images/icon_master_tl.png" /></p>
<p>Master Timeline</p>
<p><a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#Master_Timeline_and_Process_Timeline">Master
Timeline</a></p>
<p><img alt="image44" src="https://portal.futuregrid.org/sites/default/files/images/icon_process_tl.png" /></p>
<p>Process Timeline</p>
<p><a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#Master_Timeline_and_Process_Timeline">Process
Timeline</a></p>
<p><img alt="image45" src="https://portal.futuregrid.org/sites/default/files/images/icon_counter_tl.png" /></p>
<p>Counter Data Timeline</p>
<p><a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#Counter_Data_Timeline">Counter
Data</a></p>
<p><img alt="image46" src="https://portal.futuregrid.org/sites/default/files/images/icon_radar.png" /></p>
<p>Performance Radar</p>
<p><a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#Performance_Radar">Performance
Radar</a></p>
<p><img alt="image47" src="https://portal.futuregrid.org/sites/default/files/images/icon_function_summ.png" /></p>
<p>Function Summary</p>
<p><a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#Function_Summary">Function
Summary</a></p>
<p><img alt="image48" src="https://portal.futuregrid.org/sites/default/files/images/icon_message_summ.png" /></p>
<p>Message Summary</p>
<p><a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#Message_Summary">Message
Summary</a></p>
<p><img alt="image49" src="https://portal.futuregrid.org/sites/default/files/images/icon_process_summ.png" /></p>
<p>Process Summary</p>
<p><a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#Process_Summary">Process
Summary</a></p>
<p><img alt="image50" src="https://portal.futuregrid.org/sites/default/files/images/icon_matrix.png" /></p>
<p>Communication Matrix View</p>
<p><a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#Communication_Matrix_View">Communication Matrix
View</a></p>
<p><img alt="image51" src="https://portal.futuregrid.org/sites/default/files/images/icon_calltree.png" /></p>
<p>Call Tree</p>
<p><a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#Call_Tree">Call
Tree</a></p>
<p><img alt="image52" src="https://portal.futuregrid.org/sites/default/files/images/icon_legend.png" /></p>
<p>Function Legend</p>
<p><a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#Function_Legend">Function
Legend</a></p>
<p><img alt="image53" src="https://portal.futuregrid.org/sites/default/files/images/icon_context.png" /></p>
<p>Context View</p>
<p><a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#Context_View">Context
View</a></p>
<p><img alt="image54" src="https://portal.futuregrid.org/sites/default/files/images/icon_marker.png" /></p>
<p>Marker View</p>
<p><a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir#Information_Filtering_and_Reduction">Marker
View</a></p>
</div>
<div class="section" id="properties-of-the-tracefile">
<h3><strong>Properties of the Tracefile</strong><a class="headerlink" href="#properties-of-the-tracefile" title="Permalink to this headline"></a></h3>
<p>Vampir provides a display containing the most important
characterizations of the used tracefile. This tabular is called&nbsp;<em>Trace
Properties</em>&nbsp;and can be accessed by&nbsp;<em>File-&gt;Trace Properties</em>. The
information, such as the filename, the creator and its version,
originates from the tracefile and is not changed by Vampir.</p>
</div>
</div>
<div class="section" id="performance-data-visualization">
<h2><strong>Performance Data Visualization</strong><a class="headerlink" href="#performance-data-visualization" title="Permalink to this headline"></a></h2>
<p>This chapter deals with the different charts that can be used to analyze
the behavior of a program and the comparison between different function
groups, e.g., MPI and Calculation. In addition, the chapter addresses
communication performance issues. Various charts address the
visualization of data transfers between processes. The following
sections describe them in detail.</p>
<div class="section" id="timeline-charts">
<h3><strong>Timeline Charts</strong><a class="headerlink" href="#timeline-charts" title="Permalink to this headline"></a></h3>
<p>A very common chart type used in event-based performance analysis is the
so-called timeline chart. This chart type graphically presents the chain
of events of monitored processes or counters on a horizontal time axis.
Multiple timeline chart instances can be added to the&nbsp;<em>Trace
View</em>&nbsp;window via the&nbsp;<em>Chart</em>&nbsp;menu or the&nbsp;<em>Charts</em>&nbsp;toolbar.</p>
<p><strong>Note</strong>: To measure the duration between two events in a timeline
chart, Vampir provides a tool called ruler. Click on the first event in
a timeline display and move the mouse while keeping the left mouse key
and&nbsp;<em>Shift</em>&nbsp;pressed. A ruler-like pattern appears in the current
timeline chart, which provides rough measurement directly. The exact
time of the start event and the mouse position and the interval in
between is given at the very bottom. If the&nbsp;<em>Shift</em>&nbsp;key is released
before the left mouse key, Vampir will proceed with zooming.</p>
<div class="section" id="master-timeline-and-process-timeline">
<h4><strong>Master Timeline and Process Timeline</strong><a class="headerlink" href="#master-timeline-and-process-timeline" title="Permalink to this headline"></a></h4>
<p>In the Master and Process Timelines, detailed information about
functions, communication, and synchronization events is shown. Timeline
charts are available for individual processes (<em>Process Timeline</em>) as
well as for a collection of processes (<em>Master Timeline</em>). The&nbsp;<em>Master
Timeline</em>&nbsp;consists of a collection of rows. Each row represents a single
process, as shown in the figure below. A&nbsp;<em>Process Timeline</em>&nbsp;shows the
different levels of function calls in a stacked bar chart for a single
process, as depicted in the second figure.</p>
<p><img alt="image55" src="https://portal.futuregrid.org/sites/default/files/images/Master_timeline.png" /></p>
<p><strong>Master Timeline</strong></p>
<p><img alt="image56" src="https://portal.futuregrid.org/sites/default/files/images/Process_timeline.png" /></p>
<p><strong>Process Timeline</strong></p>
<p>Every timeline row consists of a process name on the left and a colored
sequence of function calls or program phases on the right. The color of
a function is defined by its group membership; e.g., MPI_Send()
belonging to the function group MPI has the same color, presumably red,
as MPI_Recv(), which also belongs to the function group MPI. Clicking
on a function highlights it and causes the&nbsp;<em>Context View</em>&nbsp;display to
show detailed information about that particular function, e.g., its
corresponding function group name, time interval, and the complete name.
The&nbsp;<em>Context View</em>&nbsp;display is explained in its own section below. Some
function invocations are very short, and will not show up in the overall
view because of a lack of display pixels. A zooming mechanism is
provided to inspect a specific time interval in more detail. If zooming
is performed, panning in a horizontal direction is possible with the
scroll bar at the bottom. The&nbsp;<em>Process Timeline</em>&nbsp;resembles
the&nbsp;<em>Master Timeline</em>with slight differences. The chart&#8217;s timeline
is divided into levels, which represent the different call stack levels
of function calls. The initial function begins at the first level, a
sub-function called by that function is located a level beneath, and so
forth. If a sub-function returns to its caller, the graphical
representation also returns to the level above. In addition to the
display of categorized function invocations, Vampir&#8217;s
<em>Master</em>&nbsp;and&nbsp;<em>Process Timeline</em>&nbsp;also provide information about
communication events. Messages exchanged between two different processes
are depicted as black lines. In timeline charts, the progress in time is
reproduced from left to right. The leftmost starting point of a message
line and its underlying process bar therefore identify the sender of the
message, whereas the rightmost position of the same line represents the
receiver of the message. The corresponding function calls normally
reflect a pair of MPI communication directives like MPI_Send() and
MPI_Recv(). It is also possible to show a collective communication like
MPI_Allreduce() by selecting one corresponding message as shown in the
figure.</p>
<p><img alt="image57" src="https://portal.futuregrid.org/sites/default/files/images/collectives.png" /></p>
<p><strong>Selected MPI Collective in Master Timeline</strong></p>
<p>Additional information like message bursts, markers, and I/O events is
also available. The table shows the symbols and descriptions of these
objects.</p>
<p><strong>Additional Information in Master and Process Timeline</strong></p>
<p><strong>Symbol</strong></p>
<p><strong>Description</strong></p>
<p>Message Burst</p>
<p><img alt="image58" src="https://portal.futuregrid.org/sites/default/files/images/burst.png" /></p>
<p>Because of a lack of pixels it is not possible to display a large number
of messages in a very short interval. Therefore, these messages are
summarized as so-called message bursts. Zooming into this interval
reveals the corresponding single messages.</p>
<p>Markers</p>
<p><img alt="image59" src="https://portal.futuregrid.org/sites/default/files/images/marker-multiple.png" />multiple</p>
<p><img alt="image60" src="https://portal.futuregrid.org/sites/default/files/images/marker-template.png" />single</p>
<p>To indicate particular points (like errors or warnings) during the
runtime of an application, markers can be used in a tracefile. They are
drawn as triangles, which are colored according to their types. To
illustrate that two or more markers are placed at the same pixel, a
multiple marker is drawn.</p>
<p>I/O Events</p>
<p><img alt="image61" src="https://portal.futuregrid.org/sites/default/files/images/io-multiple.png" />multiple</p>
<p><img alt="image62" src="https://portal.futuregrid.org/sites/default/files/images/io-single.png" />single</p>
<p><img alt="image63" src="https://portal.futuregrid.org/sites/default/files/images/io-single-selected.png" />single, selected</p>
<p>Vampir shows detailed information about I/O operations, if they are
included in the tracefile. I/O events are depicted as triangles at the
beginning of an I/O interval. Multiple I/O events are tricolored and
occupy a line to the end of the interval. To see the whole interval of a
single I/O event, the triangle has to be selected. In that case, a
second triangle at the end of the interval appears.</p>
<p>Since the&nbsp;<em>Process Timeline</em>&nbsp;reveals information of one process only,
short black arrows are used to indicate outgoing communication. Clicking
on message lines or arrows shows message details like sender process,
receiver process, message length, message duration, and message tag in
the&nbsp;<em>Context View</em>&nbsp;display.</p>
</div>
<div class="section" id="counter-data-timeline">
<h4><strong>Counter Data Timeline</strong><a class="headerlink" href="#counter-data-timeline" title="Permalink to this headline"></a></h4>
<p>Counters are values collected over time to count certain events like
floating point operations or cache misses. Counter values can be used to
store not just hardware performance counters but arbitrary sample
values. There can be counters for different statistical information as
well, for instance, counting the number of function calls or a value in
an iterative approximation of the final result. Counters are defined
during the instrumentation of the application and can be individually
assigned to processes.</p>
<p><img alt="image64" src="https://portal.futuregrid.org/sites/default/files/images/Counter_data_timeline.png" /></p>
<p><strong>Counter Data Timeline</strong></p>
<p>The chart is restricted to one counter at a time. It shows the selected
counter for one process. Using multiple instances of the&nbsp;<em>Counter Data
Timeline,</em> counters or processes can be compared easily. The context
menu entry&nbsp;<em>Set Counter</em>&nbsp;allows you to choose the displayed counter
directly from a drop-down list. The entry&nbsp;<em>Set Process</em>&nbsp;selects the
particular process for which the counter is shown.</p>
</div>
<div class="section" id="performance-radar">
<h4><strong>Performance Radar</strong><a class="headerlink" href="#performance-radar" title="Permalink to this headline"></a></h4>
<p>The Performance Radar chart provides the search of function occurrences
in the trace file and the extended visualization of counters. It can
happen that a function is not shown in&nbsp;<em>Master</em>&nbsp;and&nbsp;<em>Process
Timeline</em>&nbsp;due to a short runtime. An alternative to zooming is the
option&nbsp;<em>Find Function...</em>. A color-coded timeline indicates the
intervals in which the function is executed.</p>
<p><img alt="image65" src="https://portal.futuregrid.org/sites/default/files/images/performance_radar_find_function.png" /></p>
<p><strong>Performance Radar Timeline - Search of Functions</strong></p>
<p>By default, the Performance Radar shows the values of one counter for
each process (thread). In this mode the user can choose between <em>Line
Plot</em>&nbsp;and&nbsp;<em>Color Coded</em>&nbsp;drawing. In the latter case, a color scale on
the bottom provides information about the range of values. Clicking
on&nbsp;<em>Set Counter...</em>&nbsp;leads to a dialog that offers the option of
choosing another counter and calculating the sum or average values.
Summarizing means that the values of the selected counter of all
processes are summed up. The average is this sum divided by the number
of processes. Both options provide a single graph.</p>
<p><img alt="image66" src="https://portal.futuregrid.org/sites/default/files/images/performance_radar_set_counter.png" /></p>
<p><strong>Performance Radar Timeline - Visualization of Counters</strong></p>
</div>
</div>
<div class="section" id="statistical-charts">
<h3><strong>Statistical Charts</strong><a class="headerlink" href="#statistical-charts" title="Permalink to this headline"></a></h3>
<div class="section" id="call-tree">
<h4><strong>Call Tree</strong><a class="headerlink" href="#call-tree" title="Permalink to this headline"></a></h4>
<p>The&nbsp;<em>Call Tree</em> illustrates the invocation hierarchy of all monitored
functions in a tree representation. The display reveals information
about the number of invocations of a given function, the time spent in
the different calls, and the caller-callee relationship.</p>
<p><img alt="image67" src="https://portal.futuregrid.org/sites/default/files/images/Call_tree.png" /></p>
<p><strong>Call Tree</strong></p>
<p>The entries of the&nbsp;<em>Call Tree</em>&nbsp;can be sorted in various ways. Simply
click on one header of the tree representation to use its characteristic
to resort the&nbsp;<em>Call Tree</em>. Please note that not all available
characteristics are enabled by default. To add or remove
characteristics, a context menu is accessible by right-clicking on any
of the tree headers. To leaf through the different function calls, it is
possible to fold and unfold the levels of the tree. This can be achieved
by double-clicking a level, or by using the fold level buttons next to
the function name. Functions can be called by many different caller
functions, which is hardly obvious in the tree representation.
Therefore, a relation view shows all callers and callees of the
currently selected function in two separated lists, as shown in the
lower area. To find a certain function by its name, Vampir provides a
search option accessible with the context menu entry&nbsp;<em>Show Find View</em>.
The entered keyword has to be confirmed by pressing the Return key.
The&nbsp;<em>Previous</em>&nbsp;and&nbsp;<em>Next</em>&nbsp;buttons can be used to flip through the
results afterwards.</p>
</div>
<div class="section" id="function-summary">
<h4><strong>Function Summary</strong><a class="headerlink" href="#function-summary" title="Permalink to this headline"></a></h4>
<p>The&nbsp;<em>Function Summary</em>&nbsp;chart gives an overview of the accumulated time
consumption across all function groups and functions. For example every
time a process calls the MPI_Send() function, the elapsed time of that
function is added to the MPI function group time. The chart gives a
condensed view on the execution of the application and a comparison
between the different function groups can be made so that dominant
function groups can be distinguished easily.</p>
<p><img alt="image68" src="https://portal.futuregrid.org/sites/default/files/images/Function_summary.png" /></p>
<p><strong>Function Summary</strong></p>
<p>It is possible to change the information displayed via the context menu
entry&nbsp;<em>Set Metric</em>, which offers values like&nbsp;<em>Average Exclusive
Time</em>, <em>Number of Invocations</em>,&nbsp;<em>Accumulated Inclusive Time</em>&nbsp;and
others. Note:&nbsp;<em>Inclusive</em>&nbsp;means the amount of time spent in a function
and all of its subroutines.&nbsp;<em>Exclusive</em>&nbsp;means the amount of time just
spent in this function. The context menu entry&nbsp;<em>Set Event
Category</em>&nbsp;specifies whether either function groups or functions should
be displayed in the chart. The functions own the color of their function
group. It is possible to hide functions and function groups from the
displayed information with the context menu entry&nbsp;<em>Filter</em>. To mark
the function or function group to be filtered, click the associated
label or color representation in the chart. Using the&nbsp;<em>Process
Filter</em>&nbsp;allows you to restrict this view to a set of processes. As a
result, only the consumed time of these processes is displayed for each
function group or function. Instead of using the filter (which affects
all other displays by hiding processes), it is possible to select a
single process via&nbsp;<em>Set Process</em>&nbsp;in the context menu of the <em>Function
Summary</em>. This does not have any effect on other timeline displays.
The&nbsp;<em>Function Summary</em>&nbsp;can be shown as a&nbsp;<em>Histogram</em>&nbsp;(a bar chart,
as in timeline charts) or as a&nbsp;<em>Pie Chart</em>. To switch between these
representations, use the&nbsp;<em>Set Chart Mode</em>&nbsp;entry of the context menu.
The shown functions or function groups can be sorted by name or value
via the context menu option&nbsp;<em>Sort By</em>.</p>
</div>
<div class="section" id="process-summary">
<h4><strong>Process Summary</strong><a class="headerlink" href="#process-summary" title="Permalink to this headline"></a></h4>
<p>The&nbsp;<em>Process Summary</em>&nbsp;is similar to the&nbsp;<em>Function Summary</em>&nbsp;but shows
the information for every process independently.</p>
<p><img alt="image69" src="https://portal.futuregrid.org/sites/default/files/images/Process_summary.png" /></p>
<p><strong>Process Summary</strong></p>
<p>This is useful for analyzing the balance between processes to reveal
bottlenecks. For instance, finding that one process spends a
significantly high time performing the calculations could indicate an
unbalanced distribution of work that can slow down the entire
application. The context menu entry&nbsp;<em>Set Event Category</em>&nbsp;specifies
whether either function groups or functions should be displayed in the
chart. The functions own the color of their function group. The chart
can calculate the analysis based on&nbsp;<em>Exclusive Time</em>&nbsp;or&nbsp;<em>Inclusive
Time</em>. To change between these two modes, use the context menu
entry&nbsp;<em>Set Metric</em>. It is possible to hide functions and function
groups from the displayed information with the context menu
entry&nbsp;<em>Filter</em>. To mark the function or function group to be filtered,
click on the associated color representation in the chart. Using
the&nbsp;<em>Process Filter</em>&nbsp;allows you to restrict this view to a set of
processes.</p>
</div>
<div class="section" id="message-summary">
<h4><strong>Message Summary</strong><a class="headerlink" href="#message-summary" title="Permalink to this headline"></a></h4>
<p>The&nbsp;<em>Message Summary</em>&nbsp;is a statistical chart showing an overview of
the different messages grouped by certain characteristics.</p>
<p><img alt="image70" src="https://portal.futuregrid.org/sites/default/files/images/Messagesummary.png" /></p>
<p><strong>Message Summary Chart with metric set to&nbsp;*Message Transfer
Rate*&nbsp;showing the average transfer rate&nbsp;(A), and the minimal/maximal
transfer rate&nbsp;(B)</strong></p>
<p>All values are represented in a bar chart fashion. The number next to
each bar is the group base, while the number inside a bar depicts the
different values depending on the chosen metric. Therefore, the&nbsp;<em>Set
Metric</em>&nbsp;sub-menu of the context menu can be used to switch between
<em>Aggregated Message Volume</em>,&nbsp;<em>Message Size</em>,&nbsp;<em>Number of Messages</em>,
and&nbsp;<em>Message Transfer Rate</em>. The group base can be changed via the
context menu entry&nbsp;<em>Group By</em>. It is possible to choose
between&nbsp;<em>Message Size</em>,&nbsp;<em>Message Tag</em>, and&nbsp;<em>Communicator (MPI)</em>.</p>
<p><strong>Note</strong>: There will be one bar for every occurring group. However,
if metric is set to&nbsp;<em>Message Transfer Rate</em>, the minimal and the
maximal transfer rate is given in an additional bar beneath the one
showing the average transfer rate. The additional bar starts at the
minimal rate and ends at the maximal one. To filter out messages, click
on the associated label or color representation in the chart and
choose&nbsp;<em>Filter</em>&nbsp;from the context menu afterwards.</p>
</div>
<div class="section" id="communication-matrix-view">
<h4><strong>Communication Matrix View</strong><a class="headerlink" href="#communication-matrix-view" title="Permalink to this headline"></a></h4>
<p>The&nbsp;<em>Communication Matrix View</em>&nbsp;is another way of analyzing
communication imbalances. It shows information about messages sent
between processes.</p>
<p><img alt="image71" src="https://portal.futuregrid.org/sites/default/files/images/Communication_matrix_view.png" /></p>
<p><strong>Communication Matrix View</strong></p>
<p>The chart is realized as a table. Its rows represent the sending
processes while its columns represent the receivers. The color legend on
the right indicates the displayed values. Depending on the displayed
information, the color legend changes. It is possible to change the type
of displayed values. Different metrics like the average duration of
messages passed from sender to recipient or minimum and maximum
bandwidth are offered. To change the type of value that is displayed,
use the context menu option&nbsp;<em>Set Metric</em>. Use the&nbsp;<em>Process
Filter</em>&nbsp;to define which processes/groups should be displayed.</p>
<p><strong>Note</strong>: A high duration is not automatically caused by a slow
communication path between two processes, but can also be due to the
fact that the time between starting transmission and successful
reception of the message can be increased by a recipient that delays
reception for some reason. This will cause the duration to increase (by
this delay) and the message rate, which is the size of the message
divided by the duration, to decrease accordingly.</p>
</div>
</div>
<div class="section" id="informational-charts">
<h3><strong>Informational Charts</strong><a class="headerlink" href="#informational-charts" title="Permalink to this headline"></a></h3>
</div>
<div class="section" id="function-legend">
<h3><strong>Function Legend</strong><a class="headerlink" href="#function-legend" title="Permalink to this headline"></a></h3>
<p>The&nbsp;<em>Function Legend</em>&nbsp;lists all visible function groups of the loaded
trace file along with its corresponding color.</p>
<p><img alt="image72" src="https://portal.futuregrid.org/sites/default/files/images/Function_legend.png" /></p>
<p><strong>Function Legend</strong></p>
<p>If colors of functions are changed, they appear in a tree-like fashion
under their respective function group as well.</p>
<p><img alt="image73" src="https://portal.futuregrid.org/sites/default/files/images/Marker_view.png" /></p>
<p><strong>A chosen marker (A) and its representation in the Marker View (B)</strong></p>
<p>The display is given in a tree-like fashion and organizes the marker
events in their respective groups and types. Additional information,
like the time of occurrence in the trace file and its description, is
provided for each marker. By clicking on a marker event in the&nbsp;<em>Marker
View</em>, this event gets selected in the timeline displays that are
currently open, and vice-versa. If this marker event is not visible, the
zooming area jumps to this event automatically. It is possible to select
markers and types. Then all events belonging to that marker or type get
selected in the&nbsp;<em>Master Timeline</em>&nbsp;and the&nbsp;<em>Process Timeline</em>.
If&nbsp;<em>Ctrl</em>&nbsp;or&nbsp;<em>Shift</em>&nbsp;is pressed, the user can highlight several
events. In this case, the user can fit the borders of the zooming area
in the timeline charts to the timestamps of the two marker events that
were chosen at last.</p>
<div class="section" id="context-view">
<h4><strong>Context View</strong><a class="headerlink" href="#context-view" title="Permalink to this headline"></a></h4>
<p><img alt="image74" src="https://portal.futuregrid.org/sites/default/files/images/Context_view.png" /></p>
<p><strong>Context View, showing context information (B) of a selected function
(A)</strong></p>
<p>As implied by its name, the&nbsp;<em>Context View</em>&nbsp;provides more detailed
information of a selected object compared to its graphical
representation. An object, e.g., a function, function group, message, or
message burst, can be selected directly in a chart by clicking its
graphical representation. For different types of objects, different
context information is provided by the&nbsp;<em>Context View</em>. For example,
the object-specific information for functions holds properties
like&nbsp;<em>Interval Begin</em>,&nbsp;<em>Interval End</em>, and&nbsp;<em>Duration</em>.
The&nbsp;<em>Context View</em>&nbsp;may contain several tabs, and a new empty one can
be added by clicking on the&nbsp;<em>add</em>-symbol on the right hand side. If an
object in another chart is selected, its information is displayed in the
current tab. If the&nbsp;<em>Context View</em>&nbsp;is closed, it opens automatically
in that moment. The&nbsp;<em>Context View</em>&nbsp;offers a comparison between the
information that is displayed in different tabs. Just use the&nbsp;<em>=</em>&nbsp;on
the left hand side and choose two objects in the emerged dialog. It is
possible to compare different elements from different charts, which can
be useful in some cases. The comparison shows a list of common
properties. The corresponding values are displayed, along with their
difference if the values are numbers. The first line always shows the
names of the displays.</p>
<p><img alt="image75" src="https://portal.futuregrid.org/sites/default/files/images/context_compare.png" /></p>
<p><strong>Comparison between Context Information</strong></p>
</div>
</div>
<div class="section" id="information-filtering-and-reduction">
<h3><strong>Information Filtering and Reduction</strong><a class="headerlink" href="#information-filtering-and-reduction" title="Permalink to this headline"></a></h3>
<p>Due to the large amount of information that can be stored in trace
files, it is usually necessary to reduce the displayed information
according to some filter criteria. In Vampir, there are different ways
of filtering. It is possible to limit the displayed information to a
certain choice of processes or to specific types of communication
events, e.g., to certain types of messages or collective operations.
Deselecting an item in a filter means that this item is fully masked. In
Vampir, filters are global. Therefore, masked items will no longer show
up in any chart. Filtering not only affects the different charts, but
also the <em>Zoom Toolbar. The different filters can be reached via
the&nbsp;</em>Filter <em>entry in the main menu.</em></p>
<p>The example below shows a typical process representation in
the&nbsp;<em>Process Filter</em>&nbsp;window. This kind of representation is equal to
all other filters. Processes can be filtered by their&nbsp;<em>Process
Group</em>,&nbsp;<em>Communicators</em>&nbsp;and&nbsp;<em>Process Hierarchy</em>. Items to be filtered
are arranged in a spreadsheet representation. In addition to selecting
or deselecting an entire group of processes, it is certainly possible to
filter single processes.</p>
<p><img alt="image76" src="https://portal.futuregrid.org/sites/default/files/images/process_filter.png" /></p>
<p><strong>Process Filter</strong></p>
<p>Different selection methods can be used in a filter. The check
box&nbsp;<em>Include/Exclude All</em>&nbsp;either selects or deselects every item.
Specific items can be selected/deselected by clicking the check box next
to it. Furthermore, it is possible to select/deselect multiple items at
once; mark the desired entries by clicking their names while holding
either the&nbsp;<em>Shift</em>&nbsp;or the&nbsp;<em>Ctrl</em>&nbsp;key. By holding the&nbsp;<em>Shift</em>&nbsp;key
every item in between the two clicked items will be marked. Holding
the&nbsp;<em>Ctrl</em>&nbsp;key, on the other hand, enables you to add or remove
specific items from/to the marked ones. Clicking the check box of one of
the marked entries will cause selection/deselection for all of them.</p>
<p><strong>Options of Filtering</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Filter Object</strong></td>
<td><strong>Filter Criteria</strong></td>
</tr>
<tr class="row-even"><td>Processes</td>
<td>Process Groups</td>
</tr>
<tr class="row-odd"><td>&nbsp;</td>
<td>Communicators</td>
</tr>
<tr class="row-even"><td>&nbsp;</td>
<td>Process Hierarchy</td>
</tr>
<tr class="row-odd"><td>&nbsp;</td>
<td>Single Processes</td>
</tr>
<tr class="row-even"><td>Collective Operations</td>
<td>Communicators</td>
</tr>
<tr class="row-odd"><td>&nbsp;</td>
<td>Collective Operations</td>
</tr>
<tr class="row-even"><td>Messages</td>
<td>Message Communicators</td>
</tr>
<tr class="row-odd"><td>&nbsp;</td>
<td>Message Tags</td>
</tr>
<tr class="row-even"><td>I/O Events</td>
<td>I/O Groups</td>
</tr>
<tr class="row-odd"><td>&nbsp;</td>
<td>Files</td>
</tr>
<tr class="row-even"><td>&nbsp;</td>
<td>Types</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="customization">
<h2><strong>Customization</strong><a class="headerlink" href="#customization" title="Permalink to this headline"></a></h2>
<p>The appearance of the trace file and various other application settings
can be altered in the preferences accessible via the main menu entry
<em>File-&gt;Preferences</em>. Settings concerning the trace file itself, e.g.,
layout or function group colors, are saved individually next to the
tracefile in a file, whose end is&nbsp;<em>.vsettings</em>. In this way, it is
possible to adjust the colors for one trace file without interfering
with other trace files. The options <em>Import Preferences</em>&nbsp;and&nbsp;<em>Export
Preferences</em>&nbsp;provide the loading and saving of preferences of arbitrary
tracefiles.</p>
<div class="section" id="general-preferences">
<h3><strong>General Preferences</strong><a class="headerlink" href="#general-preferences" title="Permalink to this headline"></a></h3>
<p>The&nbsp;<em>General</em>&nbsp;settings allow you to change application and trace
specific values.</p>
<p><img alt="image77" src="https://portal.futuregrid.org/sites/default/files/images/pref_general.png" /></p>
<p><strong>General Settings</strong></p>
<p><em>Show time as</em>decides whether the time format for the trace analysis
is based on seconds or ticks. The next point&nbsp;<em>Use color gradient in
charts</em>allows you to switch off the color gradient used in the
performance charts. The next option is to change the style and size of
the font.&nbsp;<em>Show source code</em> allows you to open an editor showing the
respective source file. In order to open a source file, first click on
the intended function in the&nbsp;<em>Master Timeline</em>&nbsp;and then on the source
code path in the&nbsp;<em>Context View</em>. For the source code location to work
properly, you need a trace file with source code location support. The
path of the source file can be adjusted in&nbsp;<em>Preferences</em>. A limit for
the size of the file can be set, too. Finally, the user can decide if he
wants Vampir to automatically check for new versions.</p>
</div>
<div class="section" id="appearance">
<h3><strong>Appearance</strong><a class="headerlink" href="#appearance" title="Permalink to this headline"></a></h3>
<p>In the&nbsp;<em>Appearance</em>&nbsp;settings of the&nbsp;<em>Preferences</em>&nbsp;dialog, there are
six different objects for which the color options can be changed: the
functions/function groups, markers, counters, collectives, messages and
I/O events. Choose an entry and click on its color to make a
modification. A color picker dialog opens where it is possible to adjust
the color. For messages and collectives, a change of the line width is
also available.</p>
<p><img alt="image78" src="https://portal.futuregrid.org/sites/default/files/images/pref_appearance.png" /></p>
<p><strong>Appearance Settings</strong></p>
<p>In order to quickly find the desired item a search box is provided at
the bottom of the dialog.</p>
</div>
<div class="section" id="saving-policy">
<h3><strong>Saving Policy</strong><a class="headerlink" href="#saving-policy" title="Permalink to this headline"></a></h3>
<p>Vampir detects whenever changes to the various settings are made. In
the&nbsp;<em>Saving Policy</em>&nbsp;dialog it is possible to adjust the saving
behavior of the different components to your own needs.</p>
<p><strong>Saving Policy Settings</strong></p>
<div class="line-block">
<div class="line">In the dialog&nbsp;<em>Saving Behavior</em>&nbsp;you tell Vampir what to do in the</div>
</div>
<p>case of changed preferences. The user can choose the categories of
settings (e.g., layout) that should be treated. Possible options are
that the application automatically&nbsp;<em>Always</em>&nbsp;or&nbsp;<em>Never</em>&nbsp;saves changes.
The default option is to have Vampir asking you whether to save or
discard changes. Usually the settings are stored in the folder of the
tracefile. If the user has no access to it, it is possible to place them
in the&nbsp;<em>Application Data Folder</em>. They are listed in the
tab&nbsp;<em>Locally Stored Preferences</em>&nbsp;with creation and modification date.
|
<strong>Note</strong>: On loading, Vampir favors settings in the&nbsp;<em>Application
Data Folder</em>.&nbsp;<em>Default Preferences</em>&nbsp;offers to save preferences of the
current trace file as default settings, where they are then used for
tracefiles without settings. Another option is to restore the default
settings; in this case, the current preferences of the tracefile are
reverted.</p>
</div>
</div>
<div class="section" id="footnotes">
<h2><strong>Footnotes</strong><a class="headerlink" href="#footnotes" title="Permalink to this headline"></a></h2>
<p>Additional links that might be of interest to the reader:</p>
<p>...
(OTF)&nbsp;<a class="reference external" href="http://www.tu-dresden.de/zih/otf">http://www.tu-dresden.de/zih/otf</a></p>
<p>...
WindowsHPC&nbsp;<a class="reference external" href="http://resourcekit.windowshpc.net/MORE_INFO/TracingMPIApplications.html">http://resourcekit.windowshpc.net/MORE_INFO/TracingMPIApplications.html</a></p>
<p>...
Manual&nbsp;<a class="reference external" href="http://www.tu-dresden.de/zih/vampirtrace">http://www.tu-dresden.de/zih/vampirtrace</a></p>
<p>Retrieved from
&#8220;<a class="reference external" href="https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir">https://wiki.futuregrid.org/index.php/Docs/Performance/Vampir</a>&#8220;</p>
</div>
<div class="section" id="vampirtrace">
<h2>VampirTrace<a class="headerlink" href="#vampirtrace" title="Permalink to this headline"></a></h2>
<p>VampirTrace consists of a tool set and a runtime library for
instrumentation and tracing of software applications. It is particularly
tailored to parallel and distributed High Performance Computing (HPC)
applications.</p>
<p><strong>Availability</strong><a href="#id14"><span class="problematic" id="id15">**</span></a><a href="#id16"><span class="problematic" id="id17">**</span></a></p>
<p>VampirTrace is currently available on FutureGrid machines under module
&#8216;vampirtrace&#8217;. VampirTrace is also available in OpenMPI versions 1.5.x
or higher. For example on Bravo, it is available as <em>openmpi/1.5.4-gnu</em>
or <em>openmpi/1.5.4-intel</em>.</p>
<p><strong>Overview</strong><a href="#id18"><span class="problematic" id="id19">**</span></a><a href="#id20"><span class="problematic" id="id21">**</span></a></p>
<p>The instrumentation part of VampirTrace modifies a given application in
order to inject additional measurement calls during runtime. The tracing
part provides the actual measurement functionality used by the
instrumentation calls. By this means, a variety of detailed performance
properties can be collected and recorded during runtime. This includes
function enter and leave events, MPI communication, OpenMP events, and
performance counters.</p>
<p>After a successful tracing run, VampirTrace writes all collected data to
a trace file in the Open Trace Format (OTF). As a result, the
information is available for post-mortem analysis and visualization by
various tools. Most notably, VampirTrace provides the input data for the
Vampir analysis and visualization tool.</p>
<p>Trace files can quickly become very large, especially with automatic
instrumentation. Tracing applications for only a few seconds can result
in trace files of several hundred megabytes. To protect users from
creating trace files of several gigabytes, the default behavior of
VampirTrace limits the internal buffer to 32MB per process (2GB on
FutureGrid systems). Thus, even for larger scale runs the total trace
file size will be moderate.</p>
<p>The following list shows a summary of all instrumentation and tracing
features that VampirTrace offers. Note that not all features are
supported on all platforms.</p>
<p><strong>Tracing of User Functions</strong></p>
<ul class="simple">
<li>Record function enter and leave events</li>
<li>Record name and source code location (file name, line)</li>
<li>Manual instrumentation using VampirTrace API</li>
</ul>
<p><strong>MPI Tracing</strong></p>
<ul class="simple">
<li>Record MPI functions</li>
<li>Record MPI communication: participating processes, transferred bytes,
tag, communicator</li>
</ul>
<p><strong>OpenMP Tracing</strong></p>
<ul class="simple">
<li>OpenMP directives, synchronization, thread idle time</li>
<li>Also hybrid (MPI and OpenMP) applications are supported</li>
</ul>
<p><strong>Pthread Tracing</strong></p>
<ul class="simple">
<li>Trace POSIX thread API calls</li>
<li>Also hybrid (MPI and POSIX threads) applications are supported</li>
</ul>
<p><strong>Java Tracing</strong></p>
<ul class="simple">
<li>Record method calls</li>
<li>Using JVMTI as interface between VampirTrace and Java Applications</li>
</ul>
<p><strong>3rd-Party Library tracing</strong></p>
<ul class="simple">
<li>Trace calls to arbitrary third party libraries</li>
<li>Generate wrapper for library functions based on librarys header
file(s)</li>
<li>No recompilation of application or library is required</li>
</ul>
<p><strong>MPI Correctness Checking</strong></p>
<ul class="simple">
<li>Record MPI usage errors</li>
<li>Using UniMCI as interface between VampirTrace and a MPI correctness
checking tool (e.g., Marmot)</li>
</ul>
<p><strong>User API</strong></p>
<ul class="simple">
<li>Manual instrumentation of source code regions</li>
<li>Measurement controls</li>
<li>User-defined counters</li>
<li>User-defined marker</li>
</ul>
<p><strong>Performance Counters</strong></p>
<ul class="simple">
<li>Hardware performance counters using PAPI, CPC, or NEC SX performance
counter</li>
<li>Resource usage counters using getrusage</li>
</ul>
<p><strong>Memory Tracing</strong></p>
<ul class="simple">
<li>Trace GLIBC memory allocation and free functions</li>
<li>Record size of currently allocated memory as counter</li>
</ul>
<p><strong>I/O Tracing</strong></p>
<ul class="simple">
<li>Trace LIBC I/O calls</li>
<li>Record I/O events: file name, transferred bytes</li>
</ul>
<p><strong>CPU ID Tracing</strong></p>
<ul class="simple">
<li>Trace core ID of a CPU on which the calling thread is running</li>
<li>Record core ID as counter</li>
</ul>
<p><strong>Fork/System/Exec Tracing</strong></p>
<ul class="simple">
<li>Trace applications calling LIBCs fork, system, or one of the exec
functions</li>
<li>Add forked processes to the trace</li>
</ul>
<p><strong>Filtering &amp; Grouping</strong></p>
<ul class="simple">
<li>Runtime and post-mortem filter (i.e., exclude functions from being
recorded in the trace)</li>
<li>Runtime grouping (i.e., assign functions to groups for improved
analysis)</li>
</ul>
<p><strong>OTF Output</strong></p>
<ul class="simple">
<li>Writes compressed OTF files</li>
<li>Output as trace file, statistical summary (profile), or both</li>
</ul>
<p><strong>Instrumentation</strong></p>
<p>To perform measurements with VampirTrace, the user&#8217;s application program
needs to be instrumented; that is, at specific points of interest
(called &#8220;events&#8221;), VampirTrace measurement calls have to be activated.
Common events are, among others, entering and leaving of functions as
well as sending and receiving of MPI messages. VampirTrace handles this
automatically by default. In order to enable the instrumentation of
function calls, the user needs only to replace the compiler and linker
commands with VampirTraces wrappers (see below). VampirTrace supports
different ways of instrumentation as described in the sections below.</p>
<p><strong>Compiler Wrappers</strong></p>
<p>All the necessary instrumentation of user functions, MPI, and OpenMP
events is handled by VampirTraces compiler wrappers (vtcc, vtcxx,
vtf77, and vtf90). In the script used to build the application (e.g., a
makefile), all compile and link commands should be replaced by the
VampirTrace compiler wrapper. The wrappers perform the necessary
instrumentation of the program and link the suitable VampirTrace
library. The following list shows some examples specific to the
parallelization type of the program:</p>
<ul class="simple">
<li>Serial programs</li>
</ul>
<p>Compiling serial codes is the default behavior of the wrappers. Simply
replace the compiler by VampirTraces wrapper:</p>
<p>original:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gfortran hello.f90 -o hello</p>
<p>with instrumentation: <strong>vtf90</strong> hello.f90 -o hello</p>
<p>This will instrument user functions (if supported by the compiler) and
link the VampirTrace library.</p>
<ul class="simple">
<li>MPI parallel programs</li>
</ul>
<p>MPI instrumentation is always handled by means of the PMPI interface,
which is part of the MPI standard. This requires the compiler wrapper to
link with an MPI-aware version of the VampirTrace library. If your MPI
implementation uses special MPI compilers (e.g. mpicc, mpxlf90), you
will need to tell VampirTraces wrapper to use this compiler instead of
the serial one:</p>
<p>original:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mpicc hello.c -o hello</p>
<p>with instrumentation: <strong>vtcc -vt:cc mpicc</strong> hello.c -o hello</p>
<p>MPI implementations without their own compilers require the user to link
the MPI library manually. In this case, simply replace the compiler by
VampirTraces compiler wrapper:</p>
<p>original:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; icc hello.c -o hello lmpi</p>
<p>with instrumentation: <strong>vtcc</strong> hello.c -o hello -lmpi</p>
<p>If you want to instrument MPI events only (this creates smaller trace
files and less overhead), use the option <em>-vt:inst manual</em> to disable
automatic instrumentation of user functions.</p>
<ul class="simple">
<li>Threaded parallel programs</li>
</ul>
<p>When VampirTrace detects OpenMP or Pthread flags on the command line,
special instrumentation calls are invoked. For OpenMP events, OPARI is
invoked for automatic source code instrumentation.</p>
<p>original:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ifort &lt;-openmp|-pthread&gt; hello.f90 -o hello</p>
<p>with instrumentation: <strong>vtf90</strong> &lt;-openmp|-pthread&gt; hello.f90 -o hello</p>
<p>For more information about OPARI, read the documentation available in
VampirTraces installation directory at:
<a class="reference external" href="http://share/vampirtrace/doc/opari/Readme.html%20">share/vampirtrace/doc/opari/Readme.html</a></p>
<ul class="simple">
<li>Hybrid MPI/Threaded parallel programs</li>
</ul>
<p>With a combination of the above mentioned approaches, hybrid
applications can be instrumented:</p>
<p>original:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mpif90 &lt;-openmp|-pthread&gt; hello.F90 -o
hello</p>
<p>with instrumentation: <strong>vtf90 -vt:f90 mpif90</strong> &lt;-openmp|-pthread&gt;
hello.F90 -o hello</p>
<p>The VampirTrace compiler wrappers automatically try to detect which
parallelization method is used by means of the compiler flags (e.g.,
-lmpi, -openmp or -pthread) and the compiler command (e.g. mpif90). If
the compiler wrapper failed to detect this correctly, the
instrumentation could be incomplete and an unsuitable VampirTrace
library would be linked to the binary. In this case, you should tell the
compiler wrapper which parallelization method your program uses by using
the switches -vt:mpi, -vt:mt, and -vt:hyb for MPI, multithreaded, and
hybrid programs, respectively. Note that these switches do not change
the underlying compiler or compiler flags. Use the option -vt:verbose to
see the command line that the compiler wrapper executes.</p>
<p>The default settings of the compiler wrappers can be modified in the
files <em>share/vampirtrace/vtcc-wrapper-data.txt</em> (and similar for the
other languages) in the installation directory of VampirTrace. The
settings include compilers, compiler flags, libraries, and
instrumentation types. You could, for instance, modify the default C
compiler from gcc to mpicc by changing the line <em>compiler=gcc</em> to
<em>compiler=mpicc</em>. This may be convenient if you instrument MPI parallel
programs only.</p>
<p><strong>Instrumentation Types</strong></p>
<p>The wrapper option <em>-vt:inst &lt;insttype&gt;</em> specifies the instrumentation
type to be used. The following values for <em>&lt;insttype&gt;</em> are possible:</p>
<ul class="simple">
<li>compinst</li>
</ul>
<p>Fully-automatic instrumentation by the compiler</p>
<ul class="simple">
<li>manual</li>
</ul>
<p>Manual instrumentation by using VampirTraces API (needs source-code
modifications)</p>
<p><strong>Automatic Instrumentation</strong></p>
<p>Automatic instrumentation is the most convenient method to instrument
your program. If available, simply use the compiler wrappers without any
parameters, e.g.:</p>
<p>vtf90 hello.f90 -o hello</p>
<p><strong>Notes for Using the GNU or Intel Compiler</strong></p>
<p>For these compilers, the command nm is required to get symbol
information of the running application executable. To get the
application executable for nm during runtime, VampirTrace uses the /proc
file system. As /proc is not present on all operating systems, automatic
symbol information might not be available. In this case, it is necessary
to set the environment variable VT APPPATH to the pathname of the
application executable to get symbols resolved via nm.</p>
<p>Should any problems emerge to get symbol information automatically, then
the environment variable VT GNU NMFILE can be set to a symbol list file,
which is created with the command nm, like:</p>
<p>nm hello &gt; hello.nm</p>
<p>To get the source code line for the application functions use nm -l (on
Linux systems). VampirTrace will include this information in the trace.
Note that the output format of nm must be written in BSD-style. See the
manual page of nm for help in dealing with the output format setting.</p>
<p><strong>Notes on Instrumentation of Inline Functions</strong></p>
<p>Compilers behave differently when they automatically instrument inlined
functions. The GNU and Intel (10.0++) compilers instrument all functions
by default when they are used with VampirTrace. They therefore switch
off inlining completely, disregarding the optimization level chosen. One
can prevent these particular functions from being instrumented by
appending the following attribute to function declarations, hence making
them able to be inlined (this works only for C/C++):</p>
<p>__attribute__ ((__no_instrument_function__))</p>
<p>The PGI and IBM compilers prefer inlining over instrumentation when
compiling with enabled inlining. Thus, one needs to disable inlining to
enable the instrumentation of inline functions and vice versa.</p>
<p>The bottom line is that a function cannot be inlined and instrumented at
the same time. Note that you can also use the option <em>-vt:inst manual</em>
with non-instrumented sources. Binaries created in this manner only
contain MPI and OpenMP instrumentation, which might be desirable in some
cases. For more on how to inline functions, read your compilers manual.</p>
<p><strong>Manual Instrumentation</strong></p>
<p><strong>Using the VampirTrace API</strong></p>
<p>The <em>VT USER START, VT USER END</em> calls can be used to instrument any
user-defined sequence of statements.</p>
<p>Fortran</p>
<p>#include &#8220;vt_user.inc&#8221;</p>
<p>VT_USER_START(name)</p>
<p>...</p>
<p>VT_USER_END(name)</p>
<p>C</p>
<p>#include &#8220;vt_user.h&#8221;</p>
<p>VT_USER_START(&#8220;name&#8221;);</p>
<p>...</p>
<p>VT_USER_END(&#8220;name&#8221;);</p>
<p>If a block has several exit points (as is often the case for functions),
all exit points have to be instrumented with VT USER END, too.</p>
<p>For C++ it is simpler, as is demonstrated in the following example. Only
entry points into a scope need to be marked. The exit points are
detected automatically when C++ deletes scope-local variables.</p>
<p>C++</p>
<p>#include &#8220;vt_user.h&#8221;</p>
<p>{</p>
<p>&nbsp;&nbsp;&nbsp; VT_TRACER(&#8220;name&#8221;);</p>
<p>&nbsp;&nbsp;&nbsp; ...</p>
<p>}</p>
<p>The instrumented sources have to be compiled with -DVTRACE for all three
languages; otherwise the VT * calls are ignored. Note that Fortran
source files instrumented this way have to be preprocessed, too.</p>
<p>In addition, you can combine this particular instrumentation type with
all other types. In such a way, all user functions can be instrumented
by a compiler while special source code regions (e.g., loops) can be
instrumented by VTs API.</p>
<p>Use VTs compiler wrapper (described above) for compiling and linking
the instrumented source code, such as:</p>
<ul class="simple">
<li>combined with automatic compiler instrumentation:</li>
</ul>
<p>vtcc <strong>-DVTRACE</strong> hello.c -o hello</p>
<ul class="simple">
<li>without compiler instrumentation:</li>
</ul>
<p>vtcc -vt:inst manual <strong>-DVTRACE</strong> hello.c -o hello</p>
<p>Note that you can also use the option -vt:inst manual with
non-instrumented sources. Binaries created in this manner only contain
MPI and OpenMP instrumentation, which might be desirable in some cases.</p>
<p><strong>Measurement Controls</strong></p>
<p><strong>Switching Tracing On/Off:</strong> In addition to instrumenting arbitrary
blocks of code, one can use the VT_ON/ VT_OFF instrumentation calls to
start and stop the recording of events. These constructs can be used to
stop recording of events for a part of the application and later resume
recording. For example, one could not collect trace events during the
initialization phase of an application and turn on tracing for the
computation part.</p>
<p>Furthermore, the &#8220;on/off&#8221; functionality can be used to control the
tracing behavior of VampirTrace, and allows you to trace only parts of
interests. Essentially, then, the amount of trace data can be reduced.</p>
<p>To check whether if tracing is enabled or not, use the call VT_IS_ON.</p>
<p>Please note that stopping and starting the recording of events has to be
performed at the same call stack level. If this is not the case, an
error message will be printed during runtime, and VampirTrace will abort
execution.</p>
<p><strong>Intermediate Buffer Flush:</strong> In addition to an automated buffer flush
when the buffer is filled, it is possible to flush the buffer at any
point of the application. This way you can guarantee that after a manual
buffer flush there will be a sequence of the program with no automatic
buffer flush interrupting. To flush the buffer, you can use the call
VT_BUFFER_FLUSH.</p>
<p><strong>Intermediate Time Synchronisation:</strong> VampirTrace provides several
mechanisms for timer synchronization. In addition, it is also possible
to initiate a timer synchronization at any point of the application by
calling VT_TIMESYNC. Please note that the user has to ensure that all
processes are actual at a synchronized point in the program (e.g., at a
barrier). To use this call, make sure that the enhanced timer
synchronization is activated (set the environment variable
VT_ETIMESYNC).</p>
<p><strong>Intermediate Counter Update:</strong> VampirTrace provides the functionality
to collect the values of arbitrary hardware counters. Chosen counter
values are automatically recorded whenever an event occurs. Sometimes
(e.g., within a long-lasting function) it is desirable to get the
counter values at an arbitrary point within the program. To record the
counter values at any given point, you can call VT_UPDATE_COUNTER.</p>
<p><strong>Note:</strong> For all three languages the instrumented sources have to be
compiled with -DVTRACE. Otherwise the VT * calls are ignored. In
addition, if the sources contain further VampirTrace API calls and only
the calls for measurement controls will be disabled, then the sources
must also be compiled with -DVTRACE_NO_CONTROL.</p>
<p><strong>Tracing Calls to 3rd-Party Libraries</strong></p>
<p>VampirTrace is also capable of tracing calls to third-party libraries
which come with at least one C header file, even without the librarys
source code. If VampirTrace was built with support for library tracing,
the tool vtlibwrapgen can be used to generate a wrapper library to
intercept each call to the actual library functions. This wrapper
library can be linked to the application, or used in combination with
the LD PRELOAD mechanism provided by Linux. The generation of a wrapper
library is done using the vtlibwrapgen command and consists of two
steps. The first step generates a C source file, providing the wrapped
functions of the library header file:</p>
<p>vtlibwrapgen -g SDL -o SDLwrap.c /usr/include/SDL/*.h</p>
<p>This generates the source file <em>SDLwrap.c</em> that contains
wrapper-functions for all library functions found in the header-files
located in <em>/usr/include/SDL/</em>, and instructs VampirTrace to assign
these functions to the new group SDL. The generated wrapper source file
can be edited in order to add manual instrumentation or alter attributes
of the library wrapper. A detailed description can be found in the
generated source file or in the header file <em>vt libwrap.h</em> , which can
be found in the include directory of VampirTrace. To adapt the library
instrumentation it is possible to pass a filter file to the generation
process. The rules are like these for normal VampirTrace
instrumentation, where only 0 (exclude functions) and -1 (generally
include functions) are allowed.</p>
<p>The second step is to compile the generated source file:</p>
<p>vtlibwrapgen &#8211;build &#8211;shared -o libSDLwrap SDLwrap.c</p>
<p>This builds the shared library <em>libSDLwrap.so</em>, which can be linked to
the application or preloaded by using the environment variable LD
PRELOAD:</p>
<p>LD_PRELOAD=$PWD/libSDLwrap.so &lt;executable&gt;</p>
<p><strong>Runtime Measurement</strong></p>
<p>Running a VampirTrace instrumented application should normally result in
an OTF trace file in the current working directory where the application
was executed. If a problem occurs, set the environment variable
VT_VERBOSE to 2 before executing the instrumented application in order
to see control messages of the VampirTrace runtime system which might
help tracking down the problem.</p>
<p>The internal buffer of VampirTrace is limited to 32 MB per process. Use
the environment variables VT_BUFFER_SIZE and VT_MAX_FLUSHES to
increase this limit.</p>
<p><strong>Trace File Name and Location</strong></p>
<p>The default name of the trace file depends on the operating system where
the application is run. On Linux, MacOS and Sun Solaris, the trace file
will be named like the application, e.g., <em>hello.otf</em>for the
executable hello. For other systems, the default name is <em>a.otf</em>.
Optionally, the trace file name can be defined manually by setting the
environment variable VT_FILE_PREFIX to the desired name. The suffix
.<em>otf</em> will be added automatically.</p>
<p>To prevent overwriting of trace files by repetitive program runs, one
can enable unique trace file naming by setting VT_FILE_UNIQUE to yes.
In this case, VampirTrace adds a unique number to the file names as soon
as a second trace file with the same name is created. A *.lock file is
used to count up the number of trace files in a directory. Be aware that
VampirTrace potentially overwrites an existing trace file if you delete
this lock file. The default value of VT_FILE_UNIQUE is no. You can
also set this variable to a number greater than zero, which will be
added to the trace file name. This way you can manually control the
unique file naming.</p>
<p>The default location of the final trace file is the working directory at
application start time. If the trace file will be stored in another
place, use VT_PFORM_GDIR to change the location of the trace file.</p>
<p><strong>Environment Variables</strong></p>
<p>Environment variables can be used to control nearly every aspect of the
measurement of a VampirTrace instrumented executable. (ToDo: link to
CheatSheet and Doku-PDF)</p>
<p><strong>Variable</strong></p>
<p><strong>Purpose</strong></p>
<p><strong>Default</strong></p>
<p>Global Settings</p>
<p>VT_APPPATH</p>
<p>Path to the application executable.</p>
<ul class="simple">
<li></li>
</ul>
<p>VT_BUFFER_SIZE</p>
<p>Size of internal event trace buffer. This is the place where event
records are stored, before being written to a file.</p>
<p>32M</p>
<p>VT_CLEAN</p>
<p>Remove temporary trace files?</p>
<p>yes</p>
<p>VT_COMPRESSION</p>
<p>Write compressed trace files?</p>
<p>yes</p>
<p>VT_FILE_PREFIX</p>
<p>Prefix used for trace filenames.</p>
<ul class="simple">
<li></li>
</ul>
<p>VT_FILE_UNIQUE</p>
<p>Enable unique trace file naming? Set to yes, no, or a numerical ID.</p>
<p>no</p>
<p>VT_MAX_FLUSHES</p>
<p>Maximum number of buffer flushes.</p>
<p>1</p>
<p>VT_MAX_THREADS</p>
<p>Maximum number of threads per process that VampirTrace reserves
resources for.</p>
<p>65536</p>
<p>VT_PFORM_GDIR</p>
<p>Name of global directory to store final trace file in.</p>
<p>./</p>
<p>VT_PFORM_LDIR</p>
<p>Name of node-local directory which can be used to store temporary trace
files.</p>
<p>/tmp/</p>
<p>VT_UNIFY</p>
<p>Unify local trace files afterwards?</p>
<p>yes</p>
<p>VT_VERBOSE</p>
<p>Level of VampirTrace related information messages: Quiet (0), Critical
(1), Information (2)</p>
<p>1</p>
<p>Optional Features</p>
<p>VT_CPUIDTRACE</p>
<p>Enable tracing of CPU ID?</p>
<p>no</p>
<p>VT_ETIMESYNC</p>
<p>Enable enhanced timer synchronization?  Section
[#timer_synchronization [*]]</p>
<p>no</p>
<p>VT_ETIMESYNC_INTV</p>
<p>Interval between two successive synchronization phases in s.</p>
<p>120</p>
<p>VT_IOLIB_PATHNAME</p>
<p>Provides an alternative library to use for LIBC I/O calls.</p>
<ul class="simple">
<li></li>
</ul>
<p>VT_IOTRACE</p>
<p>Enable tracing of application I/O calls?</p>
<p>no</p>
<p>VT_LIBCTRACE</p>
<p>Enable tracing of fork/system/exec calls?</p>
<p>yes</p>
<p>VT_MEMTRACE</p>
<p>Enable memory allocation counter?</p>
<p>no</p>
<p>VT_MODE</p>
<p>Colon-separated list of VampirTrace modes: Tracing (TRACE), Profiling
(STAT).</p>
<p>TRACE</p>
<p>VT_MPICHECK</p>
<p>Enable MPI correctness checking via UniMCI?</p>
<p>no</p>
<p>VT_MPICHECK_ERREXIT</p>
<p>Force trace write and application exit if an MPI usage error is
detected?</p>
<p>no</p>
<p>VT_MPITRACE</p>
<p>Enable tracing of MPI events?</p>
<p>yes</p>
<p>VT_PTHREAD_REUSE</p>
<p>Reuse IDs of terminated Pthreads?</p>
<p>yes</p>
<p>VT_STAT_INV</p>
<p>Length of interval for writing the next profiling record</p>
<p>0</p>
<p>VT_STAT_PROPS</p>
<p>Colon-separated list of event types that will be recorded in profiling
mode: Functions (FUNC), Messages (MSG), Collective Ops. (COLLOP) or all
of them (ALL)</p>
<p>ALL</p>
<p>VT_SYNC_FLUSH</p>
<p>Enable synchronized buffer flush?</p>
<p>no</p>
<p>VT_SYNC_FLUSH_LEVEL</p>
<p>Minimum buffer fill level for synchronized buffer flush in percent.</p>
<p>80</p>
<p>Counters</p>
<p>VT_METRICS</p>
<p>Specify counter metrics to be recorded with trace events as a
colon-separated list of names</p>
<ul class="simple">
<li></li>
</ul>
<p>VT_RUSAGE</p>
<p>Colon-separated list of resource usage counters which will be recorded.</p>
<ul class="simple">
<li></li>
</ul>
<p>VT_RUSAGE_INTV</p>
<p>Sample interval for recording resource usage counters in ms.</p>
<p>100</p>
<p>Filtering, Grouping</p>
<p>VT_DYN_BLACKLIST</p>
<p>Name of blacklist file for Dyninst instrumentation.</p>
<ul class="simple">
<li></li>
</ul>
<p>VT_DYN_SHLIBS</p>
<p>Colon-separated list of shared libraries for Dyninst instrumentation.</p>
<ul class="simple">
<li></li>
</ul>
<p>VT_FILTER_SPEC</p>
<p>Name of function/region filter file.</p>
<p>VT_GROUPS_SPEC</p>
<p>Name of function grouping file.</p>
<ul class="simple">
<li></li>
</ul>
<p>VT_JAVA_FILTER_SPEC</p>
<p>Name of Java specific filter file.</p>
<ul class="simple">
<li></li>
</ul>
<p>VT_GROUP_CLASSES</p>
<p>Create a group for each Java class automatically?</p>
<p>yes</p>
<p>VT_MAX_STACK_DEPTH</p>
<p>Maximum number of stack level to be traced. (0 = unlimited)</p>
<p>0</p>
<p>Demangle, Symbol List</p>
<p>VT_GNU_DEMANGLE</p>
<p>Decode (demangle) low-level symbol names into user-level names?</p>
<p>no</p>
<p>VT_GNU_GETSRC</p>
<p>Retrieve the source code line of functions instrumented automatically
with the GNU interface?</p>
<p>yes</p>
<p>VT_GNU_NMFILE</p>
<p>Name of file with symbol list information.</p>
<ul class="simple">
<li></li>
</ul>
<p>When you use these environment variables, make sure that they have the
same value for all processes of your application on all nodes of your
cluster. Some cluster environments do not automatically transfer your
environment when executing parts of your job on remote nodes of the
cluster, and you may need to explicitly set and export them in batch job
submission scripts.</p>
<p><strong>Influencing Trace Buffer Size</strong></p>
<p>The default values of the environment variables VT_BUFFER_SIZE and
VT_MAX_FLUSHES limit the internal buffer of VampirTrace to 32 MB per
process, and the number of times that the buffer is flushed to 1,
respectively. Events that are to be recorded after the limit has been
reached are no longer written into the trace file. The environment
variables apply to every process of a parallel application, meaning that
applications with n processes will typically create trace files n times
the size of a serial application.</p>
<p>To remove the limit and get a complete trace of an application, set
VT_MAX_FLUSHES to 0. This causes VampirTrace to always write the
buffer to disk when it is full. To change the size of the buffer, use
the environment variable VT_BUFFER_SIZE. The optimal value for this
variable depends on the application which is to be traced. Setting a
small value will increase the memory available to the application, but
will trigger frequent buffer flushes by VampirTrace. These buffer
flushes can significantly change the behavior of the application. On the
other hand, setting a large value, like 2G, will minimize buffer flushes
by VampirTrace, but decrease the memory available to the application. If
not enough memory is available to hold the VampirTrace buffer and the
application data, parts of the application may be swapped to disk,
leading to a significant change in the behavior of the application.</p>
<p>Note that you can decrease the size of trace files significantly by
using the runtime function filtering.</p>
<p><strong>Profiling an Application</strong></p>
<p>Profiling an application collects aggregated information about certain
events during a program run, whereas tracing records information about
individual events. Profiling can therefore be used to get a summary of
the program activity and to detect events that are called very often.
The profiling information can also be used to generate filter rules to
reduce the trace file size.</p>
<p>To profile an application, set the variable VT_MODE to STAT. Setting
VT_MODE to STAT:TRACE tells VampirTrace to perform tracing and
profiling at the same time. By setting the variable VT STAT PROPS, the
user can influence whether functions, messages, and/or collective
operations shall be profiled.</p>
<p><strong>Unification of Local Traces</strong></p>
<p>After a run of an instrumented application, the traces of the single
processes need to be unified in terms of timestamps and event IDs. In
most cases, this happens automatically. If the environment variable
VT_UNIFY is set to no, and in the case of certain other circumstances,
it will be necessary to perform unification of local traces manually. To
do this, use the following command:</p>
<p>vtunify &lt;nproc&gt; &lt;prefix&gt;</p>
<p>If VampirTrace was built with support for OpenMP and/or MPI, it is
possible to speedup the unification of local traces significantly. To
distribute the unificationon multible processes, the MPI parallel
version vtunify-mpi can be used as follows:</p>
<p>mpirun -np &lt;nranks&gt; vtunify-mpi &lt;nproc&gt; &lt;prefix&gt;</p>
<p>Furthermore, both tools vtunify and vtunify-mpi are capable of opening
additional OpenMP threads for unification. The number of threads can be
specified by the OMP_NUM_THREADS environment variable.</p>
<p><strong>Synchronized Buffer Flush</strong></p>
<p>When tracing an application, VampirTrace temporarily stores the recorded
events in a trace buffer. Typically, if a buffer of a process or thread
has reached its maximum fill level, the buffer has to be flushed and
other processes or threads may have to wait for this process or thread.
This will result in an asynchronous runtime behavior.</p>
<p>To avoid this problem, VampirTrace provides a buffer flush in a
synchronized manner. This means that if one buffer has reached its
minimum buffer fill level VT_SYNC_FLUSH_LEVEL, all buffers will be
flushed. This buffer flush is only available at appropriate points in
the program flow. Currently, VampirTrace makes use of all MPI collective
functions associated with MPI_COMM_WORLD. Use the environment variable
VT_SYNC_FLUSH to enable synchronized buffer flush.</p>
<p><strong>Enhanced Timer Synchronization</strong></p>
<p>Especially on cluster environments, where each process has its own local
timer, tracing relies on precisely synchronized timers. Therefore,
VampirTrace provides several mechanisms for timer synchronization. The
default synchronization scheme is a linear synchronization at the very
beginning and very end of a trace run with a master-slave communication
pattern.</p>
<p>However, this way of synchronization can become too imprecise for long
trace runs. Therefore, we recommend the usage of the enhanced timer
synchronization scheme of VampirTrace. This scheme inserts additional
synchronization phases at appropriate points in the program flow.
Currently, VampirTrace makes use of all MPI collective functions
associated with MPI_COMM_WORLD.</p>
<p>To enable this synchronization scheme, a LAPACK library with C wrapper
support has to be provided for VampirTrace, and the environment variable
VT_ETIMESYNC has to be set before the tracing. The length of the
interval between two successive synchronization phases can be adjusted
with VT_ETIMESYNC_INTV. The following LAPACK libraries provide a
C-LAPACK API that can be used by VampirTrace for the enhanced timer
synchronization:</p>
<ul class="simple">
<li>CLAPACK</li>
<li>AMD ACML</li>
<li>IBM ESSL</li>
<li>Intel MKL</li>
<li>SUN Performance Library</li>
</ul>
<p><strong>Note:</strong> Systems equipped with a global timer do not need timer
synchronization.</p>
<p><strong>Note:</strong> It is recommended to combine enhanced timer synchronization
and synchronized buffer flush.</p>
<p><strong>Note:</strong> Be aware that the asynchronous behavior of the application
will be disturbed since VampirTrace makes use of asynchronous MPI
collective functions for timer synchronization and synchronized buffer
flush. Only make use of these approaches if your application does not
rely on an asynchronous behavior! Otherwise, keep this fact in mind
during the process of performance analysis.</p>
<p><strong>Recording Additional Events and Counters</strong></p>
<p><strong>Hardware Performance Counters</strong></p>
<p>If VampirTrace has been built with hardware counter support, it is
capable of recording hardware counter information as part of the event
records. To request the measurement of certain counters, the user is
required to set the environment variable VT_METRICS. The variable
should contain a colon-separated list of counter names or a predefined
platform-specific group.</p>
<p>The user can leave the environment variable unset to indicate that no
counters are requested. If any of the requested counters are not
recognized or the full list of counters cannot be recorded due to
hardware resource limits, program execution will be aborted with an
error message.</p>
<p><strong>PAPI Hardware Performance Counters</strong></p>
<p>If the PAPI library is used to access hardware performance counters,
metric names can be any PAPI preset names or PAPI native counter names.
For example, set</p>
<p>VT_METRICS=PAPI_FP_OPS:PAPI_L2_TCM</p>
<p>to record the number of floating point instructions and level 2 cache
misses.</p>
<p><strong>Resource Usage Counters</strong></p>
<p>The Unix system call getrusage provides information about consumed
resources and operating system events of processes such as user/system
time, received signals, and context switches.</p>
<p>If VampirTrace has been built with resource usage support, it is able to
record this information as performance counters to the trace. You can
enable tracing of specific resource counters by setting the environment
variable VT_RUSAGE to a colon-separated list of counter names. For
example, set</p>
<p>VT_RUSAGE=ru_stime:ru_majflt</p>
<p>to record the system time consumed by each process and the number of
page faults. Alternatively, one can set this variable to the value all
to enable recording of all 16 resource usage counters. Note that not all
counters are supported by all Unix operating systems. Linux 2.6 kernels,
for example, support only resource information for six of them.</p>
<p>The resource usage counters are not recorded at every event. They are
only read if 100 ms have passed since the last sampling. The interval
can be changed by setting VT_RUSAGE_INTV to the number of desired
milliseconds. Setting VT_RUSAGE_INTV to zero leads to sampling
resource usage counters at every event, which may introduce a large
runtime overhead. Note that in most cases the operating system does not
update the resource usage information at the same high frequency as the
hardware performance counters. Setting VT_RUSAGE_INTV to a value less
than 10 ms does not usually improve the granularity.</p>
<p>Be aware that, when using the resource usage counters for multi-threaded
programs, the information displayed is valid for the whole process and
not for each single thread.</p>
<p><strong>Memory Allocation Counter</strong></p>
<p>The GNU LIBC implementation provides a special hook mechanism that
allows intercepting all calls to memory allocation and free functions
(e.g. malloc, realloc, free). This is independent from compilation or
source code access, but relies on the underlying system library.</p>
<p>If VampirTrace has been built with memory-tracing support, VampirTrace
is capable of recording memory allocation information as part of the
event records. To request the measurement of the applications allocated
memory, the user must set the environment variable VT_MEMTRACE to yes.</p>
<p><strong>Note:</strong> This approach to get memory allocation information requires
changing internal function pointers in a non-thread-safe way, so
VampirTrace currently does not support memory tracing for threadable
programs, e.g., programs parallelized with OpenMP or Pthreads!</p>
<p><strong>Pthread API Calls</strong></p>
<p>When tracing applications with Pthreads, only user events and functions
are recorded which are automatically or manually instrumented. Pthread
API functions will not be traced by default. To enable tracing of all
C-Pthread API functions, include the header <em>vt user.h</em> and compile the
instrumented sources with -DVTRACE PTHREAD.</p>
<p>C/C++</p>
<p>#include &#8220;vt_user.h&#8221;</p>
<p>vtcc <strong>-DVTRACE_PTHREAD</strong> hello.c -o hello</p>
<p><strong>I/O Calls</strong></p>
<p>Calls to functions which reside in external libraries can be intercepted
by implementing identical functions and linking them before the external
library. Such &#8220;wrapper functions&#8221; can record the parameters and return
values of the library functions.</p>
<p>If VampirTrace has been built with I/O tracing support, it uses this
technique for recording calls to I/O functions of the standard C
library, which are executed by the application. The following functions
are intercepted by VampirTrace:</p>
<p>close</p>
<p>creat</p>
<p>creat64</p>
<p>dup</p>
<p>dup2</p>
<p>fclose</p>
<p>fcntl</p>
<p>fdopen</p>
<p>fgetc</p>
<p>fgets</p>
<p>flockfile</p>
<p>fopen</p>
<p>fopen64</p>
<p>fprintf</p>
<p>fputc</p>
<p>fputs</p>
<p>fread</p>
<p>fscanf</p>
<p>fseek</p>
<p>fseeko</p>
<p>fseeko64</p>
<p>fsetpos</p>
<p>fsetpos64</p>
<p>ftrylockfile</p>
<p>funlockfile</p>
<p>fwrite</p>
<p>getc</p>
<p>gets</p>
<p>lockf</p>
<p>lseek</p>
<p>lseek64</p>
<p>open</p>
<p>open64</p>
<p>pread</p>
<p>pread64</p>
<p>putc</p>
<p>puts</p>
<p>pwrite</p>
<p>pwrite64</p>
<p>read</p>
<p>readv</p>
<p>rewind</p>
<p>unlink</p>
<p>write</p>
<p>writev</p>
<p>The gathered information will be saved as I/O event records in the trace
file. This feature has to be activated for each tracing run by setting
the environment variable VT_IOTRACE to yes.</p>
<p>This works for both dynamically and statically linked executables. Note
that when linking statically, a warning like the following may be
issued: Using &#8220;dlopen&#8221; in statically linked applications requires at
runtime the shared libraries from the glibc version used for linking.
This is ok as long as the mentioned libraries are available for running
the application.</p>
<p>If youd like to experiment with some other I/O library, set the
environment variable VT_IOLIB_PATHNAME to the alternative one. Beware
that this library must provide all I/O functions mentioned above;
otherwise VampirTrace will abort.</p>
<p><strong>fork/system/exec Calls</strong></p>
<p>If VampirTrace has been built with LIBC trace support, it is capable of
tracing programs which call functions from the LIBC exec family (execl,
execlp, execle, execv, execvp, execve), system, and fork. VampirTrace
records the call of the LIBC function to the trace. This feature works
for sequential (i.e., no MPI or threaded parallelization) programs only.
It works for both dynamically and statically linked executables. Note
that when linking statically, a warning like the following may be
issued: Using &#8220;dlopen&#8221; in statically linked applications requires at
runtime the shared libraries from the glibc version used for linking.
This is ok as long as the mentioned libraries are available for running
the application.</p>
<p>When VampirTrace detects a call of an exec function, the current trace
file is closed before executing the new program. If the executed program
is also instrumented with VampirTrace, it will create a different trace
file. Note that VampirTrace aborts if the exec function returns
unsuccessfully. Calling fork in an instrumented program creates an
additional process in the same trace file.</p>
<p><strong>MPI Correctness Checking Using UniMCI</strong></p>
<p>VampirTrace supports the recording of MPI correctness events, e.g.,
usage of invalid MPI requests. This is implemented by using the
Universal MPI Correctness Interface (UniMCI), which provides an
interface between tools like VampirTrace and existing runtime MPI
correctness checking tools. Correctness events are stored as markers in
the trace file and are visualized by Vampir. If VampirTrace is built
with UniMCI support, the user only has to enable MPI correctness
checking. This is done by merely setting the environment variable
VT_MPICHECK to yes. Further, if your application crashes due to an MPI
error you should set VT_MPICHECK_ERREXIT to yes. This environmental
variable forces VampirTrace to write its trace to disk and exit
afterwards. As a result, the trace with the detected error is stored
before the application might crash.</p>
<p>To install VampirTrace with correctness checking support, it is
necessary to have UniMCI installed on your system. UniMCI in turn
requires you to have a supported MPI correctness checking tool installed
(currently only the tool Marmot is known to have UniMCI support). So,
all in all, you should use the following order to install with
correctness checking support:</p>
<ol class="arabic simple">
<li>Marmot</li>
</ol>
<p><a class="reference external" href="http://www.hlrs.de/organization/av/amt/research/marmot">http://www.hlrs.de/organization/av/amt/research/marmot</a></p>
<ol class="arabic simple">
<li>UniMCI</li>
</ol>
<p><a class="reference external" href="http://www.tu-dresden.de/zih/unimci">http://www.tu-dresden.de/zih/unimci</a></p>
<ol class="arabic simple">
<li>VampirTrace</li>
</ol>
<p><a class="reference external" href="http://www.tu-dresden.de/zih/vampirtrace">http://www.tu-dresden.de/zih/vampirtrace</a></p>
<p>Information on how to install Marmot and UniMCI is given in their
respective manuals. VampirTrace will automatically detect an UniMCI
installation if the unimci-config tool is in path.</p>
<p><strong>User-defined Counters</strong></p>
<p>In addition to the manual instrumentation, the VampirTrace API provides
instrumentation calls which allow recording of program variable values
(e.g., iteration counts, calculation results, ...) or any other
numerical quantity. A user-defined counter is identified by its name,
the counter group it belongs to, the type of its value (integer or
floating-point) and the unit that the value is quoted (e.g.
&#8220;GFlop/sec&#8221;). The VT_COUNT_GROUP_DEF and VT_COUNT_DEF
instrumentation calls can be used to define counter groups and counters:</p>
<p>Fortran</p>
<p>#include &#8220;vt_user.inc&#8221;</p>
<p>integer&nbsp;:: id, gid</p>
<p>VT_COUNT_GROUP_DEF(name, gid)</p>
<p>VT_COUNT_DEF(name, unit, type, gid, id)</p>
<p>C/C++</p>
<p>#include &#8220;vt_user.h&#8221;</p>
<p>unsigned int id, gid;</p>
<p>gid = VT_COUNT_GROUP_DEF(&#8220;name&#8221;);</p>
<p>id = VT_COUNT_DEF(&#8220;name&#8221;, &#8220;unit&#8221;, type, gid);</p>
<p>The definition of a counter group is optional. If no special counter
group is desired, the default group &#8220;User&#8221; can be used. In this case,
set the parameter gid of VT_COUNT_DEF() to VT_COUNT_DEFGROUP. The
third parameter type of VT_COUNT_DEF specifies the data type of the
counter value. To record a value for any of the defined counters, the
corresponding instrumentation call VT_COUNT * VAL must be invoked.</p>
<p><strong>Fortran:</strong></p>
<p><strong>Type</strong></p>
<p>Count call</p>
<p>Data type</p>
<p>VT_COUNT_TYPE_INTEGER</p>
<p>VT_COUNT_INTEGER_VAL</p>
<p>integer (4 byte)</p>
<p>VT_COUNT_TYPE_INTEGER8</p>
<p>VT_COUNT_INTEGER8_VAL</p>
<p>integer (8 byte)</p>
<p>VT_COUNT_TYPE_REAL</p>
<p>VT_COUNT_REAL_VAL</p>
<p>real</p>
<p>VT_COUNT_TYPE_DOUBLE</p>
<p>VT_COUNT_DOUBLE_VAL</p>
<p>double precision</p>
<p><strong>C/C++:</strong></p>
<p><strong>Type</strong></p>
<p>Count call</p>
<p>Data type</p>
<p>VT_COUNT_TYPE_SIGNED</p>
<p>VT_COUNT_SIGNED_VAL</p>
<p>signed int (max. 64-bit)</p>
<p>VT_COUNT_TYPE_UNSIGNED</p>
<p>VT_COUNT_UNSIGNED_VAL</p>
<p>unsigned int (max. 64-bit)</p>
<p>VT_COUNT_TYPE_FLOAT</p>
<p>VT_COUNT_FLOAT_VAL</p>
<p>float</p>
<p>VT_COUNT_TYPE_DOUBLE</p>
<p>VT_COUNT_DOUBLE_VAL</p>
<p>double</p>
<p>The following example records the loop index i:</p>
<p>Fortran</p>
<p>#include &#8220;vt_user.inc&#8221;</p>
<p>program main</p>
<p>integer&nbsp;:: i, cid, cgid</p>
<p>VT_COUNT_GROUP_DEF(loopindex, cgid)</p>
<p>VT_COUNT_DEF(i, #, VT_COUNT_TYPE_INTEGER, cgid, cid)</p>
<p>do i=1,100</p>
<p>&nbsp;&nbsp;&nbsp; VT_COUNT_INTEGER_VAL(cid, i)</p>
<p>end do</p>
<p>end program main</p>
<p>C/C++</p>
<p>#include &#8220;vt_user.h&#8221;</p>
<p>int main() {</p>
<p>&nbsp;&nbsp; unsigned int i, cid, cgid;</p>
<p>&nbsp;&nbsp; cgid = VT_COUNT_GROUP_DEF(loopindex);</p>
<p>&nbsp;&nbsp; cid = VT_COUNT_DEF(&#8220;i&#8221;, &#8220;#&#8221;, VT_COUNT_TYPE_UNSIGNED, cgid);</p>
<p>&nbsp;&nbsp; for( i = 1; i &lt;= 100; i++ ) {</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; VT_COUNT_UNSIGNED_VAL(cid, i);</p>
<p>&nbsp;&nbsp; }</p>
<p>&nbsp;&nbsp; return 0;</p>
<p>}</p>
<p>For all three languages, the instrumented sources have to be compiled
with -DVTRACE. Otherwise, the VT * calls are ignored. Optionally, if
the sources contain further VampirTrace API calls and only the calls for
user-defined counters will be disabled, then the sources have to be
compiled with -DVTRACE_NO_COUNT in addition to -DVTRACE .</p>
<p><strong>User-Defined Markers</strong></p>
<p>In addition to the manual instrumentation, the VampirTrace API provides
instrumentation calls which allow recording of special user information,
which can be used to better identify parts of interest. A user-defined
marker is identified by its name and type.</p>
<p>Fortran</p>
<p>#include &#8220;vt_user.inc&#8221;</p>
<p>integer&nbsp;:: mid</p>
<p>VT_MARKER_DEF(name, type, mid)</p>
<p>VT_MARKER(mid, text)</p>
<p>C/C++</p>
<p>#include &#8220;vt_user.h&#8221;</p>
<p>unsigned int mid;</p>
<p>mid = VT_MARKER_DEF(&#8220;name&#8221;,type);</p>
<p>VT_MARKER(mid, &#8220;text&#8221;);</p>
<p>Types for Fortran/C/C++</p>
<p>VT_MARKER_TYPE_ERROR</p>
<p>VT_MARKER_TYPE_WARNING</p>
<p>VT_MARKER_TYPE_HINT</p>
<p>For all three languages, the instrumented sources have to be compiled
with -DVTRACE. Otherwise, the VT * calls are ignored. Optionally, if
the sources contain further VampirTrace API calls and only the calls for
user-defined markers will be disabled, then the sources have to be
compiled with -DVTRACE_NO_MARKER in addition to -DVTRACE .</p>
<p><strong>Filtering and Grouping</strong></p>
<p>By default, all calls of instrumented functions will be traced;
consequently, the resulting trace files can easily become very large. In
order to decrease the size of a trace, VampirTrace allows the
specification of filter directives before running an instrumented
application. The user can decide on how often an instrumented
function/region should be recorded to a trace file. To use a filter, the
environment variable VT_FILTER_SPEC needs to be defined. It should
contain the path and name of a file with filter directives. Following is
an example of a file containing filter directives:</p>
<p>#VampirTrace region filter specification</p>
<p>#</p>
<p>#call limit definitions and region assignments</p>
<p>#</p>
<p>#syntax: &lt;regions&gt; &#8211; &lt;limit&gt;</p>
<p>#</p>
<p>#regions&nbsp; semicolon-separated list of regions</p>
<p>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (can be wildcards)</p>
<p>#limit&nbsp;&nbsp;&nbsp; assigned call limit</p>
<p>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 = region(s) denied</p>
<p>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -1 = unlimited</p>
<p>#</p>
<p>add;sub;mul;div &#8211; 1000</p>
<p>* &#8211; 3000000</p>
<p>These region filter directives allow the functions add, sub, mul and div
to be recorded at most 1000 times. The remaining functions * will be
recorded at most 3,000,000 times.</p>
<p>Besides creating filter files manually, you can also use the vtfilter
tool to generate them automatically. This tool reads a provided trace
and decides whether a function should be filtered or not, based on the
evaluation of certain parameters.</p>
<p><strong>Rank Specific Filtering</strong></p>
<p>An experimental extension allows rank specific filtering. Use &#64; clauses
to restrict all following filters to the given ranks. The rank selection
must be given as a list of &lt;from&gt; - &lt;to&gt; pairs or single values.</p>
<p>&#64; 4 - 10, 20 - 29, 34</p>
<p>foo;bar &#8211; 2000</p>
<p>* &#8211; 0</p>
<p>The example defines two limits for the ranks 4 - 10, 20 - 29, and 34.</p>
<p><strong>Attention:</strong> The rank specific rules are activated later than usual at
MPI Init, because the ranks are not available earlier. The special MPI
routines MPI Init, MPI Init thread, and MPI Initialized cannot be
filtered in this way.</p>
<p><strong>Function Grouping</strong></p>
<p>VampirTrace allows assigning functions/regions to a group. Groups can,
for instance, be highlighted by different colors in Vampir displays. The
following standard groups are created by VampirTrace:</p>
<p><strong>Group name</strong></p>
<p><strong>Contained functions/regions</strong></p>
<p>MPI</p>
<p>MPI functions</p>
<p>OMP</p>
<p>OpenMP API function calls</p>
<p>OMP_SYNC</p>
<p>OpenMP barriers</p>
<p>OMP_PREG</p>
<p>OpenMP parallel regions</p>
<p>Pthreads</p>
<p>Pthread API function calls</p>
<p>MEM</p>
<p>Memory allocation functions ( Section&nbsp;[#mem_alloc_counter [*]])</p>
<p>I/O</p>
<p>I/O functions ( Section&nbsp;[#io_calls [*]])</p>
<p>LIBC</p>
<p>LIBC fork/system/exec functions ( Section&nbsp;[#execfork [*]])</p>
<p>Application</p>
<p>remaining instrumented functions and source code regions</p>
<p>Additionally, you can create your own groups, if, for example, you wish
to better distinguish different phases of an application. To use
function/region grouping, set the environment variable VT_GROUPS_SPEC
to the path of a file which contains the group assignments. Below is an
example of how to use group assignments:</p>
<p># VampirTrace region groups specification</p>
<p>#</p>
<p># group definitions and region assignments</p>
<p>#</p>
<p># syntax: &lt;group&gt;=&lt;regions&gt;</p>
<p>#</p>
<p># group&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; group name</p>
<p># regions&nbsp;&nbsp;&nbsp;&nbsp; semicolon-separated list of regions</p>
<p>#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (can be wildcards)</p>
<p>#</p>
<p>CALC=add;sub;mul;div</p>
<p>USER=app_*</p>
<p>These group assignments associate the functions add, sub, mul and div
with group &#8220;CALC&#8221;, and all functions with the prefix app are associated
with group &#8220;USER&#8221;.</p>
</div>
<div class="section" id="official-hpcc-results-from-the-acceptance-tests">
<h2>Official HPCC Results from the Acceptance Tests<a class="headerlink" href="#official-hpcc-results-from-the-acceptance-tests" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><a class="reference external" href="#HPCC_Results">1 HPCC Results</a><ul>
<li><a class="reference external" href="#Official_HPCC_Results_from_the_Acceptance_Tests">1.1 Official HPCC Results from the Acceptance
Tests</a><ul>
<li><a class="reference external" href="#Alamo">1.1.1 Alamo</a></li>
<li><a class="reference external" href="#India">1.1.2 India</a></li>
<li><a class="reference external" href="#Hotel">1.1.3 Hotel</a></li>
<li><a class="reference external" href="#XRay">1.1.4 XRay</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference external" href="#HPCC_Configuration">2 HPCC Configuration</a><ul>
<li><a class="reference external" href="#General_Information">2.1 General Information</a><ul>
<li><a class="reference external" href="#India_2">2.1.1 India</a></li>
<li><a class="reference external" href="#Hotel_2">2.1.2 Hotel</a></li>
<li><a class="reference external" href="#Xray_2">2.1.3 Xray</a></li>
</ul>
</li>
<li><a class="reference external" href="#Configuration_Files">2.2 Configuration Files</a><ul>
<li><a class="reference external" href="#India_3">2.2.1 India</a></li>
<li><a class="reference external" href="#Hotel_3">2.2.2 Hotel</a></li>
<li><a class="reference external" href="#XRay_3">2.2.3 XRay</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="hpcc-results">
<h2>HPCC Results<a class="headerlink" href="#hpcc-results" title="Permalink to this headline"></a></h2>
<div class="section" id="id22">
<h3>Official HPCC Results from the Acceptance Tests<a class="headerlink" href="#id22" title="Permalink to this headline"></a></h3>
<div class="section" id="alamo-view-machine-details-manual-alamo">
<h4>Alamo&nbsp;<strong>(`view machine details &lt;/manual/alamo&gt;`__)</strong><a class="headerlink" href="#alamo-view-machine-details-manual-alamo" title="Permalink to this headline"></a></h4>
</div>
<div class="section" id="india-view-machine-details-manual-india">
<h4>India&nbsp;<strong>(`view machine details &lt;/manual/india&gt;`__)</strong><a class="headerlink" href="#india-view-machine-details-manual-india" title="Permalink to this headline"></a></h4>
<p>(*) Calculated using the base frequency of the processor. (Intel Turbo
Boost technology was enabled)</p>
</div>
<div class="section" id="hotel-view-machine-details-manual-hotel">
<h4>Hotel&nbsp;<strong>(`view machine details &lt;/manual/hotel&gt;`__)</strong><a class="headerlink" href="#hotel-view-machine-details-manual-hotel" title="Permalink to this headline"></a></h4>
</div>
<div class="section" id="xray-view-machine-details-manual-xray">
<h4>XRay&nbsp;<strong>(`view machine details &lt;/manual/xray&gt;`__)</strong><a class="headerlink" href="#xray-view-machine-details-manual-xray" title="Permalink to this headline"></a></h4>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
</div>
</div>
<div class="section" id="hpcc-configuration">
<h2>HPCC Configuration<a class="headerlink" href="#hpcc-configuration" title="Permalink to this headline"></a></h2>
<div class="section" id="general-information">
<h3>General Information<a class="headerlink" href="#general-information" title="Permalink to this headline"></a></h3>
<p>What version was used, how as it compiled etc.</p>
<div class="section" id="id23">
<h4>India<a class="headerlink" href="#id23" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li>HPCC version: 1.3.1</li>
<li>Compiler: Intel compiler 11.1.038 with OpenMPI-1.3.1.</li>
<li>Math Library: Intel MKL library.</li>
<li>A few important settings in Makefile:<ul>
<li>CC=mpicc</li>
<li>LINKER=mpicc -mkl</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id24">
<h4>Hotel<a class="headerlink" href="#id24" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li>HPCC version: 1.3.1</li>
<li>Compiler: Intel compiler 11.1.038 with Intel MPI-4.0.0.</li>
<li>Math Library: Intel MKL library.</li>
<li>A few important settings in Makefile:<ul>
<li>CC=mpicc</li>
<li>LINKER=mpicc -mkl</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id25">
<h4>Xray<a class="headerlink" href="#id25" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li>HPCC version: 1.3.1</li>
<li>Compiler: Cray compiler (xt-asyncpe-3.4)</li>
<li>A few important settings in Makefile:</li>
<li>Settings in Makefile: CC=mpicc and LINKER=mpicc -mkl<ul>
<li>CC=cc</li>
<li>CCNOOPT=$(HPL_DEFS) -DLONG_IS_64BITS</li>
<li>CCFLAGS=$(HPL_DEFS) -fast -Minfo=loop -Mneginfo=loop
-DLONG_IS_64BITS -DFFTE_NP=4</li>
<li>CCFLAGS_STREAM=$(HPL_DEFS) -Mnontemporal
-Mprefetch=distance:8,nta -Msafeptr -fastsse -Minfo=loop
-Mneginfo=loop -DLONG_IS_64BITS</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="configuration-files">
<h3>Configuration Files<a class="headerlink" href="#configuration-files" title="Permalink to this headline"></a></h3>
<p>Below are the contents of input files, hpccinf.txt, for HPCC.</p>
<div class="section" id="id26">
<h4>India<a class="headerlink" href="#id26" title="Permalink to this headline"></a></h4>
<div class="highlight-python"><pre>HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out output file name (if any)
8 device out (6=stdout,7=stderr,file)
1 # of problems sizes (N)
409600 Ns
1 # of NBs
112 NBs
0 PMAP process mapping (0=Row-,1=Column-major)
1 # of process grids (P x Q)
32 Ps
32 Qs
16.0 threshold
1 # of panel fact
2 PFACTs (0=left, 1=Crout, 2=Right)
1 # of recursive stopping criterium
4 NBMINs (&gt;= 1)
1 # of panels in recursion
3 NDIVs
1 # of recursive panel fact.
2 RFACTs (0=left, 1=Crout, 2=Right)
1 # of broadcast
3 BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1 # of lookahead depth
0 DEPTHs (&gt;=0)
2 SWAP (0=bin-exch,1=long,2=mix)
64 swapping threshold
0 L1 in (0=transposed,1=no-transposed) form
0 U in (0=transposed,1=no-transposed) form
1 Equilibration (0=no,1=yes)
16 memory alignment in double (&gt; 0)
##### This line (no. 32) is ignored (it serves as a separator). ######
0 Number of additional problem sizes for PTRANS
1200 10000 30000 values of N
0 number of additional blocking sizes for PTRANS
40 9 8 13 13 20 16 32 64 values of NB</pre>
</div>
</div>
<div class="section" id="id27">
<h4>Hotel<a class="headerlink" href="#id27" title="Permalink to this headline"></a></h4>
<div class="highlight-python"><pre>HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out output file name (if any)
8 device out (6=stdout,7=stderr,file)
1 # of problems sizes (N)
338688 Ns
1 # of NBs
112 NBs
0 PMAP process mapping (0=Row-,1=Column-major)
1 # of process grids (P x Q)
32 Ps
32 Qs
16.0 threshold
1 # of panel fact
2 PFACTs (0=left, 1=Crout, 2=Right)
1 # of recursive stopping criterium
4 NBMINs (&gt;= 1)
1 # of panels in recursion
3 NDIVs
1 # of recursive panel fact.
2 RFACTs (0=left, 1=Crout, 2=Right)
1 # of broadcast
3 BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1 # of lookahead depth
0 DEPTHs (&gt;=0)
2 SWAP (0=bin-exch,1=long,2=mix)
64 swapping threshold
0 L1 in (0=transposed,1=no-transposed) form
0 U in (0=transposed,1=no-transposed) form
1 Equilibration (0=no,1=yes)
16 memory alignment in double (&gt; 0)
##### This line (no. 32) is ignored (it serves as a separator). ######
0 Number of additional problem sizes for PTRANS
1200 10000 30000 values of N
0 number of additional blocking sizes for PTRANS
40 9 8 13 13 20 16 32 64 values of NB</pre>
</div>
</div>
<div class="section" id="id28">
<h4>XRay<a class="headerlink" href="#id28" title="Permalink to this headline"></a></h4>
<div class="highlight-python"><pre>HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out output file name (if any)
6 device out (6=stdout,7=stderr,file)
1 # of problems sizes (N)
373056 Ns
1 # of NBs
232 NBs
0 PMAP process mapping (0=Row-,1=Column-major)
1 # of process grids (P x Q)
24 Ps
28 Qs
16.0 threshold
1 # of panel fact
1 PFACTs (0=left, 1=Crout, 2=Right)
1 # of recursive stopping criterium
4 NBMINs (&gt;= 1)
1 # of panels in recursion
2 NDIVs
1 # of recursive panel fact.
2 RFACTs (0=left, 1=Crout, 2=Right)
1 # of broadcast
1 BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1 # of lookahead depth
0 DEPTHs (&gt;=0)
2 SWAP (0=bin-exch,1=long,2=mix)
64 swapping threshold
0 L1 in (0=transposed,1=no-transposed) form
0 U in (0=transposed,1=no-transposed) form
1 Equilibration (0=no,1=yes)
8 memory alignment in double (&gt; 0)
##### This line (no. 32) is ignored (it serves as a separator). ######
0 Number of additional problem sizes for PTRANS
1200 values of N
4 number of additional blocking sizes for PTRANS
23 31 33 63 values of NB

HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out output file name (if any)
8 device out (6=stdout,7=stderr,file)</pre>
</div>
</div>
</div>
</div>
<div class="section" id="id29">
<h2>Foxtrot<a class="headerlink" href="#id29" title="Permalink to this headline"></a></h2>
<p>UF iDataPlex User Manual</p>
<ul class="simple">
<li>foxtrot.futuregrid.org</li>
</ul>
<blockquote>
<div><div class="highlight-python"><pre>ssh username@foxtrot.futuregrid.org</pre>
</div>
</div></blockquote>
<p>where <em>username</em> is your FutureGrid username. You must have an account
on FutureGrid.</p>
<p><strong>2x IBM x3650 M2 management nodes</strong> (fm1, fm2):</p>
<ul class="simple">
<li>2 x 4-Core Intel Xeon E5530 (Nehalem-EP) processors</li>
<li>24GB Main Memory</li>
<li>2 x 146GB 2.5-Inch 6Gbps SAS Disks</li>
</ul>
<p><strong>32x IBM iDataPlex dx360 M2 nodes</strong> (f1 through f2):</p>
<ul class="simple">
<li>2 x 4-Core Intel Xeon E5520 (Nehalem-EP) processors</li>
<li>24GB Main Memory</li>
<li>1 x 500GB 3.5-Inch SATA Disk</li>
</ul>
<p><strong>1x Dell PowerEdge R310 Network monitoring node</strong>
(bwctl.ufl.net.futuregrid.org)</p>
<blockquote>
<div>Storage</div></blockquote>
<hr class="docutils" />
<p><strong>Hardware</strong>:&nbsp; IBM iDataPlex dx360 M3 storage node with 24TB raw
capacity (fs1).</p>
<ul class="simple">
<li>2 x 4-Core Intel Xeon E5520 (Nehalem-EP) processors</li>
<li>24GB Main Memory</li>
<li>12 x 2TB 7200 rpm 3.5-Inch 6Gbps SAS Disks</li>
</ul>
<p><strong>Capacity</strong>:&nbsp; 20TB RAID5</p>
<p><strong>Storage Interconnect</strong>:&nbsp; NFS export over GigE.</p>
<p><strong>Filesystem Type</strong>:&nbsp; XFS</p>
<p><strong>Filesystem Layout</strong>:</p>
<ul class="simple">
<li>Home directories mounted to all foxtrot nodes (f1 thorugh f32) at</li>
</ul>
<p>/N/u/&lt;username&gt;
 Backup is not provided</p>
<p><strong>Hardware</strong></p>
<ul class="simple">
<li>Force10 S50 48-port GigE switch, with 10GigE uplink to Florida</li>
</ul>
<p>LambdaRail
 BLADE Rackswitch 48-port GigE switch (Management Network)</p>
<p><strong>Nimbus</strong></p>
<ul class="simple">
<li>Refer to <a href="#id30"><span class="problematic" id="id31">`</span></a>Nimbus</li>
</ul>
<p>manual. &lt;<a class="reference external" href="https://portal.futuregrid.org/manual/services/nimbus">https://portal.futuregrid.org/manual/services/nimbus</a>&gt;`__</p>
</div>
<div class="section" id="id32">
<h2>India<a class="headerlink" href="#id32" title="Permalink to this headline"></a></h2>
<div class="section" id="iu-idataplex-user-manual">
<h3>IU iDataplex User Manual<a class="headerlink" href="#iu-idataplex-user-manual" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li>india.futuregrid.org</li>
</ul>
<p>&nbsp;i136r.idp.iu.futuregrid.org</p>
<blockquote>
<div><div class="highlight-python"><pre>ssh username@india.futuregrid.org</pre>
</div>
</div></blockquote>
<p>Above, <em>username</em> represents your FutureGrid username. You must have an
account on FutureGrid to log in.</p>
</div>
</div>
<div class="section" id="id33">
<h2>Sierra<a class="headerlink" href="#id33" title="Permalink to this headline"></a></h2>
<div class="section" id="ucsd-idataplex-user-manual">
<h3>UCSD iDataPlex User Manual<a class="headerlink" href="#ucsd-idataplex-user-manual" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li>sierra.futuregrid.org</li>
</ul>
<blockquote>
<div><div class="highlight-python"><pre>ssh sierra.futuregrid.org</pre>
</div>
</div></blockquote>
<p><strong>Hardware</strong>:&nbsp;&nbsp;Two Sun Fire x4540 Servers &#64;&nbsp;48 TB&nbsp;each. &nbsp;Specifications
for each&nbsp;Sun Fire X4540 Server are:</p>
<ul class="simple">
<li>2 x 6-Core AMD Opteron Model 2435, 2.6 GHz Processors</li>
</ul>
<p>&nbsp;32 GB (16 x 2 GB DIMMs) Memory
&nbsp;48 x 1 TB 7200 rpm 3.5-Inch SATA Disks</p>
<p><strong>Capacity</strong>: &nbsp;76.8 TB raid2 and 5.4 TB of raid0 (for scratch)</p>
<p><strong>Storage Interconnect</strong>: &nbsp;Currently mounted to cluster over GigE
ethernet. &nbsp;Our long-term plan is to mount over Infiniband.</p>
<p><strong>Filesystem Type</strong>:&nbsp;&nbsp;ZFS</p>
<p><strong>Filesystem Layout</strong>:</p>
<ul class="simple">
<li>Home directories mounted to Sierra at /N/u/&lt;username&gt;, snapshots</li>
</ul>
<p>taken nightly, quota set at 50 GB
&nbsp;Scratch directories mount to Sierra at /N/scratch/&lt;username&gt;, no
backup, quota at 100 GB
&nbsp;Project directories, software directory mounted to Sierra as
/N/soft, snapshots taken nightly, quota set at 50 GB
&nbsp;Image directory (internal), mounts to Sierra at /images, snapshots
taken nightly, quota set at 6 TB</p>
<p><strong>Overview of ZFS Data Snapshots</strong></p>
<p>A zfs snapshot is a read-only copy of a Solaris ZFS file system or
volume. Snapshots can be created almost instantly and initially consume
no additional disk space within the pool. All users on Sierra have
access to their ZFS hidden file system at</p>
<blockquote>
<div>$HOME/.zfs/</div></blockquote>
<p>ZFS supports the ability to restore lost files with the standard UNIX
copy command. See the example below.</p>
<p>Users are expected to make their own permanent backups of valuable data
on the home file system. ZFS Snapshots are NOT permanent backups. Users
are currently limited to a quota of 50 GB of snapshots.</p>
<p><strong>Example of ZFS Data Snapshot Restore Session</strong></p>
<blockquote>
<div><div class="highlight-python"><pre>$ ls
1G  4G</pre>
</div>
<div class="highlight-python"><pre>$ ls .zfs
snapshot/</pre>
</div>
<div class="highlight-python"><pre>$ ls .zfs/snapshot
SNAPSHOT2009-06-22-1245668520/  SNAPSHOT2009-06-24-1245841320/
SNAPSHOT2009-06-22-1245668674/  SNAPSHOT2009-06-25-1245927720/
SNAPSHOT2009-06-23-1245754920/  SNAPSHOT2009-06-26-1246014120/</pre>
</div>
<div class="highlight-python"><pre>$ rm 1G
$ ls
4G</pre>
</div>
<div class="highlight-python"><pre>$ ls .zfs/snapshot/SNAPSHOT2009-06-26-1246014120/
1G  4G</pre>
</div>
<div class="highlight-python"><pre>$ cp .zfs/snapshot/SNAPSHOT2009-06-26-1246014120/1G .</pre>
</div>
<div class="highlight-python"><pre>$ ls
1G  4G</pre>
</div>
</div></blockquote>
<p>THIS FEATURE IS NOT YET SUPPORTED OFFICIALLY</p>
<p>FutureGrid now supports dynamic provisioning through Moab, and in
Sierra, some instructions are listed below:</p>
<ul class="simple">
<li>The executable tools are installed at /usr/local/bin, and the $PATH</li>
</ul>
<p>should have been all set. So a user could start running from his home
directory.
&nbsp;Command&nbsp;<em>qnodes</em>&nbsp;will list all the nodes and status. As for now,
node s36~s39 is up and running and should be available for
test/experiment.
&nbsp;<em>checknode s36</em>&nbsp;will list info on node s36. Os info can be found
at line such as:</p>
<blockquote>
<div><div class="highlight-python"><pre>Opsys:      statelessrhels5.5  Arch:      x86_64</pre>
</div>
</div></blockquote>
<p>Now we have&nbsp;<em>statelessrhels5.5</em>&nbsp;and&nbsp;<em>statefulrhels5</em>&nbsp;as two options.</p>
<p>&nbsp;If all the four nodes are running stateless os, submit a command like
this:</p>
<blockquote>
<div><div class="highlight-python"><pre>msub -l os=statelessrhels5.5 testcmd.sh</pre>
</div>
</div></blockquote>
<p>will schedule the job in some node.</p>
<p>&nbsp;<em>showq</em>&nbsp;will list the current queue info, and you could see the
submitted job in the&nbsp;<em>active jobs</em>&nbsp;section.
&nbsp;Check job running status using command&nbsp;<em>checkjob &lt;jobid&gt;</em>. The
resource allocation info could be found in some lines like these:</p>
<blockquote>
<div><div class="highlight-python"><pre>Allocated Nodes:
[s36:1]</pre>
</div>
</div></blockquote>
<p>In this case the job is scheduled at node s36.</p>
<p>&nbsp;In the case where all four nodes are running stateless os, submitting
a job like this:</p>
<blockquote>
<div><div class="highlight-python"><pre>msub -l os=statefulrhels5 testcmd.sh</pre>
</div>
</div></blockquote>
<p>will try first to dynamically provision the requested os at some node,
and then schedule the job.</p>
<p>&nbsp;<em>showq</em>&nbsp;once again could list the jobs.
&nbsp;<em>checkjob &lt;jobid&gt;</em>&nbsp;will first tell you the dependency
job&nbsp;<em>provisioning os</em>&nbsp;is not completed.</p>
<blockquote>
<div><div class="highlight-python"><pre>NOTE:  job cannot run  (dependency provision-73 jobsuccessfulcomplete not met)</pre>
</div>
</div></blockquote>
<p>&nbsp;<em>checkjob provision-68(the provision job id)</em>&nbsp;will list the
provisioning status.
&nbsp;Once the provisioning is done,&nbsp;<em>checkjob &lt;jobid&gt;</em>&nbsp;will show the
job is scheduled, and also allow us to see where is it scheduled, for
example s37.
&nbsp;By running&nbsp;<em>checknode s37</em>&nbsp;again, we could see that the running
os was changed from&nbsp;<em>statelessrhels5.5</em>&nbsp;to*statefulrhels5*.</p>
<p>&nbsp;<em>testcmd.sh</em>&nbsp;used in the example:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ cat testcmd.sh
#!/bin/bash
/bin/date
sleep 300
/bin/date</pre>
</div>
</div></blockquote>
</div>
</div>
<div class="section" id="id34">
<h2>Xray<a class="headerlink" href="#id34" title="Permalink to this headline"></a></h2>
<div class="section" id="this-section-is-maintained-by-greg-pike-please-contact-https-portal-futuregrid-org-help-for-more-information">
<h3>This section is maintained by Greg Pike; please contact &nbsp;<a class="reference external" href="https://portal.futuregrid.org/help">https://portal.futuregrid.org/help</a>&nbsp;for more information.<a class="headerlink" href="#this-section-is-maintained-by-greg-pike-please-contact-https-portal-futuregrid-org-help-for-more-information" title="Permalink to this headline"></a></h3>
<p>It will include system specific information relevant to xray.</p>
</div>
<div class="section" id="iu-cray-user-manual">
<h3>IU Cray User Manual<a class="headerlink" href="#iu-cray-user-manual" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li>xray.futuregrid.org</li>
</ul>
<blockquote>
<div><div class="highlight-python"><pre>ssh xray.futuregrid.org</pre>
</div>
</div></blockquote>
<p>For MPI jobs, use cc (pgcc).</p>
<p>For best performance, add the xtpe-barcelona module</p>
<blockquote>
<div><div class="highlight-python"><pre>% module add xtpe-module</pre>
</div>
</div></blockquote>
<ul class="simple">
<li><a class="reference external" href="http://docs.cray.com/cgi-bin/craydoc.cgi?mode=View;id=S-2396-21">http://docs.cray.com/cgi-bin/craydoc.cgi?q=&amp;mode=Search&amp;hw=%22Cray+XT5%22</a></li>
<li><a class="reference external" href="http://docs.cray.com/cgi-bin/craydoc.cgi?mode=View;id=S-2396-21">http://docs.cray.com/cgi-bin/craydoc.cgi?mode=View;id=S-2396-21</a></li>
</ul>
<p>Currently there is only one queue (batch) available to users on the
Cray, and all jobs are automatically routed to that queue.</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span class="n">qstat</span> <span class="o">-</span><span class="n">Q</span>
</pre></div>
</div>
</div></blockquote>
<p>The primary queue for running jobs on Xray is batch. To obtain details
of running jobs and available processors, use the showq command.</p>
<blockquote>
<div><div class="highlight-python"><pre>/opt/moab/default/bin/showq</pre>
</div>
</div></blockquote>
<div class="section" id="submitting-a-job">
<h4>&nbsp;Submitting a job<a class="headerlink" href="#submitting-a-job" title="Permalink to this headline"></a></h4>
<p><strong>MPI run cmd</strong>:&nbsp;&nbsp;aprun</p>
<p>Example&nbsp;<strong>job script (16 processors / 2 nodes):</strong></p>
<blockquote>
<div><div class="highlight-python"><pre>% cat job.sub</pre>
</div>
<div class="highlight-python"><pre>#!/bin/sh
#PBS -l mppwidth=16
#PBS -l mppnppn=8
#PBS -N hpcc-16
#PBS -j oe
#PBS -l walltime=7:00:00
#cd to directory where job was submitted from
cd $PBS_O_WORKDIR
export MPICH_FAST_MEMCPY=1
export MPICH_PTL_MATCH_OFF=1
aprun -n 16 -N 8 -ss -cc cpu hpcc
% qsub job.sub</pre>
</div>
</div></blockquote>
<p>&nbsp;Looking at the Queue</p>
<blockquote>
<div><div class="highlight-python"><pre>% qstat</pre>
</div>
</div></blockquote>
</div>
<div class="section" id="how-do-i-submit-a-job-to-the-cray-xt5m-on-futuregrid">
<h4>How Do I Submit a Job to the Cray XT5m on FutureGrid?<a class="headerlink" href="#how-do-i-submit-a-job-to-the-cray-xt5m-on-futuregrid" title="Permalink to this headline"></a></h4>
<p><a class="reference external" href="http://kb.iu.edu/data/azse.html">http://kb.iu.edu/data/azse.html</a></p>
<p>The XT5m is a 2D mesh of nodes. Each node has two sockets, and each
socket has four cores.</p>
<p>The batch scheduler interfaces with a Cray resource scheduler called
APLS. When you submit a job, the batch scheduler talks to ALPS to find
out what resources are available, and ALPS then makes the reservation.</p>
<p>Currently ALPS is a &#8220;gang scheduler&#8221; and only allows one &#8220;job&#8221; per node.
If a user submits a job in the format&nbsp;aprun -n 1 a.out&nbsp;, ALPS will put
that job on one core of one node and leave the other seven cores empty.
When the next job comes in, either from the same user or a different
one, it will schedule that job to the next node.</p>
<p>If the user submits a job with&nbsp;aprun -n 10 a.out&nbsp;, then the scheduler
will put the first eight tasks on the first node and the next two tasks
on the second node, again leaving six empty cores on the second node.
The user can modify the placement with&nbsp;-N&nbsp;,&nbsp;-S&nbsp;, and&nbsp;-cc&nbsp;.</p>
<p>A user might also run a single job with multiple treads, as with OpenMP.
If a user runs this job&nbsp;aprun -n 1 -d 8 a.out&nbsp;, the job will be
scheduled to one node and have eight threads running, one on each core.</p>
<p>You can run multiple, different binaries at the same time on the same
node, but only from one submission. Submitting a script like this
will&nbsp;not&nbsp;work:</p>
<blockquote>
<div><div class="highlight-python"><pre>OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 0 ./my-binary
OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 1 ./my-binary
OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 2 ./my-binary
OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 3 ./my-binary
OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 4 ./my-binary
OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 5 ./my-binary
OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 6 ./my-binary
OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 7 ./my-binary</pre>
</div>
</div></blockquote>
<p>This will run a job on each core, but&nbsp;not&nbsp;at the same time. To run all
jobs at the same time, you need to first bury all the binaries under
one&nbsp;aprun&nbsp;command:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ more run.sh
./my-binary1
./my-binary2
./my-binary3
./my-binary4
./my-binary5
./my-binary6
./my-binary7
./my-binary8
$ aprun -n 1 run.sh</pre>
</div>
</div></blockquote>
<p>Alternatively, use the command&nbsp;aprun -n 1 -d 8 run.sh&nbsp;. To run multiple
serial jobs, you must build a batch script to divide the number of jobs
into groups of eight, and the</p>
</div>
</div>
</div>
<div class="section" id="id35">
<h2>Alamo<a class="headerlink" href="#id35" title="Permalink to this headline"></a></h2>
<p>Alamo is a 96 node Dell cluster running 2.66 GHz Intel Xeon X5550
processors. &nbsp;The OS is CentOS 5.8 and 6.3. &nbsp;It runs Torque and Moab for
scheduling. &nbsp;Alamo has a QDR IB interconnect and 15 TB of attached disk
storage. &nbsp; Alamo is partitioned into different resources for Nimbus and
HPC. &nbsp;See the Alamo hardware page for more
detail:&nbsp;<a class="reference external" href="https://portal.futuregrid.org/hardware/alamo">https://portal.futuregrid.org/hardware/alamo</a>
.</p>
<p><strong>Nimbus partition -</strong>&nbsp;see nimbus
documentation:&nbsp;<a class="reference external" href="https://portal.futuregrid.org/tutorials/nimbus">https://portal.futuregrid.org/tutorials/nimbus</a></p>
<p><strong>HPC partition</strong></p>
<p>Max cores 584.&nbsp;&nbsp;Submit
<a class="reference external" href="http://%20https://portal.futuregrid.org/help">ticket</a> if you need to
run larger than the 320 limit per user.</p>
<p>Available queues:
&nbsp; &nbsp;short - 24 hours runtime limit
&nbsp; &nbsp;long - 72 hours runtime limit</p>
<p>After registering your .ssh key on the portal, go to
<a class="reference external" href="https://portal.futuregrid.org/manual/access">https://portal.futuregrid.org/manual/access</a>
.</p>
<p>If you key has been uploaded, you can ssh to the login node using the
following command.
<strong>Note</strong>: If you are prompted for a password, your account has not
been set up correctly or the .ssh key has not been propagated.</p>
<p>&nbsp; ssh alamo.futuregrid.org</p>
<p>To submit a test job use the qsub command.</p>
<p>ex. &nbsp;qsub -N job_name -l nodes=1 -q short &nbsp;job_script</p>
<ul class="simple">
<li>qstat - show current jobs in the queue with status</li>
<li>showq - show current running and queued jobs and job id</li>
<li>checkjob -v &lt;jobid&gt; &nbsp; - more detailed information about your job</li>
</ul>
<p>Applications are available via modules. &nbsp;To see a list of available
applications:
&nbsp; module avail</p>
<p>File systems:
&nbsp; &nbsp;/home &nbsp; - Quota enforced home directory, backed up nightly.
&nbsp; &nbsp;/N/work - 6.3 TB work directory, not backed up. NFS mounted from
login node.
&nbsp; &nbsp;/N/images - 11 TB directory for system images, not backed up. NFS
mounted from login node.</p>
<p>Administrator: David Gignac
For issues or questions please
use&nbsp;<a class="reference external" href="https://tickets.futuregrid.org">https://tickets.futuregrid.org</a>
. You can conveniently submit a ticket
via&nbsp;<a class="reference external" href="https://portal.futuregrid.org/help">https://portal.futuregrid.org/help</a>.
To look at your previously submitted tickets you can
use&nbsp;<a class="reference external" href="https://portal.futuregrid.org/tickets">https://portal.futuregrid.org/tickets</a>
.</p>
</div>
<div class="section" id="id36">
<h2>Delta<a class="headerlink" href="#id36" title="Permalink to this headline"></a></h2>
<div class="section" id="gpu-user-manual">
<h3>GPU&nbsp;User Manual<a class="headerlink" href="#gpu-user-manual" title="Permalink to this headline"></a></h3>
<p>FutureGrid&#8217;s supercomputer, Delta&nbsp;(delta.futuregrid.org), is a 16-node
GPU cluster running Red Hat Linux,&nbsp;with TORQUE&nbsp;(also called PBS) and
Moab for job management, and Module to simplify application and
environment configuration.&nbsp;Delta consists of 16 nodes with two 6-core
Intel X5560 processors at 2.8GHz, 192 GB of&nbsp;DDR3 memory, and 15TB
of&nbsp;RAID5 disk storage. Each node supports 2 nVIDIA Tesla C2070 GPUs
with&nbsp;448 processing cores. For details on Delta&#8217;s hardware
configuration, see&nbsp;the
<a class="reference external" href="https://portal.futuregrid.org/hardware/delta">Delta</a>page.</p>
<p>The FutureGrid <em>delta</em> cluster is accessible via&nbsp;a batch queue that
is managed from india (india.futuregrid.org). To use delta
interactively, first log into india:</p>
<div class="highlight-python"><pre>ssh username@india.futuregrid.org</pre>
</div>
<p>Then, on india, the following command lets you use one of the delta
compute nodes:</p>
<div class="highlight-python"><pre>qsub -I -q delta myprg</pre>
</div>
<p>If you want to use delta with your job script, please use</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c">#PBS -q</span>
</pre></div>
</div>
<div class="highlight-python"><pre>::</pre>
</div>
<blockquote>
<div>delta</div></blockquote>
<p>to indicate that you&#8217;d like to use this queue.</p>
<p>For more details about how to manage queues with qsub, see the Delta
manual page.</p>
<p>Utilization of GPU resources&nbsp;on Delta:
1) Utilize GPU&nbsp;node
&nbsp;&nbsp;&nbsp;&nbsp;<a class="reference external" href="https://portal.futuregrid.org/manual/gpu/running-programs-single-gpu">&nbsp;Running Program on&nbsp;single GPU
node</a>
2) Utilize GPU cluster
<a class="reference external" href="https://portal.futuregrid.org/manual/running-mpigpu-program-delta-cluster">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Running MPI/CUDA program on the Delta
cluster</a>
3) Utilize GPU cloud
4) Mixing utilization of&nbsp;GPU&nbsp;and CPU
5) Non-trivial service or applications using&nbsp;GPU</p>
</div>
</div>
<div class="section" id="running-mpi-gpu-program-on-the-delta-cluster">
<h2>Running MPI/GPU program on the Delta cluster<a class="headerlink" href="#running-mpi-gpu-program-on-the-delta-cluster" title="Permalink to this headline"></a></h2>
<p>GPUs provide the ability to use mathematical operations at a fraction
of the cost and with higher performance than on the current generation
of processors. FutureGrid provides the ability to test such an
infrastructure as part of its delta cluster. Here, we provide a
step-by-step guide on how to run a
parallel&nbsp;matrix&nbsp;multiplication&nbsp;program using IntelMPI and CUDA on Delta
machines. The MPI framework&nbsp;distributes the work&nbsp;among compute
nodes,&nbsp;each of which use CUDA&nbsp;to&nbsp;execute&nbsp;the shared workload. We also
provide the&nbsp;complete&nbsp;parallel
matrix&nbsp;multiplication&nbsp;code&nbsp;using&nbsp;MPI/CUDA&nbsp;that&nbsp;has already been tested
on Delta cluster in attachment.</p>
<p><strong>MPI code: pmm_mpi.c</strong></p>
<div class="highlight-python"><pre>#include &lt;mpi.h&gt;

void invoke_cuda_vecadd();

       int main(int argc, char *argv[])
{
          int rank, size;

          MPI_Init (&amp;argc, &amp;argv); /* starts MPI */
          MPI_Comm_rank (MPI_COMM_WORLD, &amp;rank); /* get current process id */
          MPI_Comm_size (MPI_COMM_WORLD, &amp;size); /* get number of processes */
          invoke_cuda_vecadd();  /* the cuda code */
          MPI_Finalize();
 return 0;
}</pre>
</div>
<p><strong>CUDA code: dgemm_cuda.cu</strong></p>
<p>#include &lt;stdio.h&gt;</p>
<p>__global__ void cuda_vecadd(int *array1, int *array2, int
*array3)
{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int index = blockIdx.x * blockDim.x + threadIdx.x;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; array3[index] = array1[index] + array2[index];
}</p>
<p>&nbsp;&nbsp;extern &#8220;C&#8221; void invoke_cuda_vecadd()
&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cudaMalloc((void**) &amp;devarray1, sizeof(int)*10);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cudaMalloc((void**) &amp;devarray2, sizeof(int)*10);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cudaMalloc((void**) &amp;devarray3, sizeof(int)*10);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cudaMemcpy(devarray1, hostarray1, sizeof(int)*10,
cudaMemcpyHostToDevice);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cudaMemcpy(devarray2,&nbsp;hostarray2, sizeof(int)*10,
cudaMemcpyHostToDevice);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cuda_vec_add&lt;&lt;&lt;1, 10&gt;&gt;&gt;(devarray1, devarray2, devarray3);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cudaMemcpy(hostarray3, devarray3, sizeof(int)*10,
cudaMemcpyDeviceToHost);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cudaFree(devarray1);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cudaFree(devarray2);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cudaFree(devarray3);
}</p>
<p>Note: Mixing MPI and CUDA code may cause problems during linking because
of&nbsp;the&nbsp;difference between C and C++ calling conventions. The use of
extern &#8220;C&#8221; around invoke_cuda_code which instructs the nvcc (a wrapper
of c++) compiler to make that function callable from the C runtime.</p>
<p><strong>Compiling the MPI/CUDA&nbsp;program:</strong></p>
<p>Load the Modules
&gt; module load IntelMPI # load Intel MPI
&gt; module load Intel # load icc &gt; module load cuda # load cuda tools
This will load the Intel MPI, the compiler, and the cuda tools. Next
compile the code with</p>
<p>&gt; nvcc -c&nbsp;dgemm_cuda.cu -o dgemm_cuda.o &nbsp; &gt; mpiicc
-o&nbsp;pmm_mpi.c&nbsp;-o&nbsp;pmm_mpi.o
&gt; mpiicc -o mpicuda&nbsp;pmm_mpi.o&nbsp;dgemm_cuda.o -lcudart&nbsp;-lcublas&nbsp;-L
/opt/cuda/lib64 -I /opt/cuda/include</p>
<p>Note:&nbsp;The CUDA compiler nvcc is used only to compile the CUDA&nbsp;source
file, and the IntelMPI compiler&nbsp;mpiicc&nbsp;is&nbsp;used to compile the&nbsp;C&nbsp;code and
do the linking
&nbsp; <strong>Setting Up&nbsp;and Submitting MPI&nbsp;Jobs:</strong></p>
<p>1.&nbsp;qsub -I -l nodes=4 -q delta&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # get&nbsp;4 nodes from FG
2.&nbsp;uniq /var/spool/torque/aux/399286.i136
&gt;&nbsp;gpu_nodes_list&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#create&nbsp;machine file list
3. module load IntelMPI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;# load Intel MPI
4. module load Intel&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # load&nbsp;icc
5. module load&nbsp;cuda&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # load&nbsp;cuda&nbsp;tools
6.&nbsp;mpdboot&nbsp;-r ssh -f&nbsp;gpu_nodes_list -n&nbsp;4&nbsp;&nbsp;# will start an mpd ring
on&nbsp;4 nodes including local host
7. mpiexec -l -machinefile&nbsp;gpu_nodes_list -n&nbsp;4 ./mpicuda&nbsp;10000 1&nbsp;4
#&nbsp;run&nbsp;mpi program&nbsp;using 4 nodes</p>
<p><strong>Comparison between&nbsp;four&nbsp;implementations of&nbsp;sequential&nbsp;matrix
multiplication on Delta:</strong></p>
<p>&nbsp; &nbsp; <img alt="image79" src="https://portal.futuregrid.org/sites/default/files/resize/u28/CUBLAS2-800x280.png" />
<strong>References:</strong> <a class="reference external" href="https://portal.futuregrid.org/sites/default/files/mpi_cuda_mkl.zip">Source Code
Package</a>
[1] High Performance Computing&nbsp;using&nbsp;CUDA,2009 User Group Conference
[2]
<a class="reference external" href="http://www.nvidia.com/content/global/global.php">http://www.nvidia.com/content/global/global.php</a></p>
<p>To get source code: git clone <a class="reference external" href="mailto:git&#37;&#52;&#48;github&#46;com">git<span>&#64;</span>github<span>&#46;</span>com</a>:futuregrid/GPU.git</p>
<p>Compiling source code on Delta machine:</p>
<div class="highlight-python"><pre>module load intelmpi
module load intel
module load cuda
cd mpi_cuda_mkl
make</pre>
</div>
<table border="1" class="docutils">
<colgroup>
<col width="88%" />
<col width="12%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Attachment</th>
<th class="head">Size</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference external" href="https://portal.futuregrid.org/sites/default/files/mpi_cuda_mkl.zip">mpi_cuda_mkl.zip</a></td>
<td>888.92 KB</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="running-programs-on-a-single-gpu">
<h2>Running programs on a single GPU<a class="headerlink" href="#running-programs-on-a-single-gpu" title="Permalink to this headline"></a></h2>
<p><strong>Summary:</strong></p>
<p>GPUs provide the ability&nbsp;to use mathematical operations at a
fraction of the cost and with higher&nbsp;performance than on the
current&nbsp;generation of processors. CUDA is a parallel programming model
and software environment&nbsp;that leverages the parallel computational&nbsp;power
of GPU&nbsp;for non-graphics computing in a fraction of the time required on
a CPU. FutureGrid provides the ability to test&nbsp;such a&nbsp;hardware&nbsp;and
software environment as part of its Delta cluster. Here, we illustrate
some details of data-parallel computational model&nbsp;of CUDA, and&nbsp;then
provide a step-by-step guide on how to&nbsp;make a parallel matrix
multiplication program using CUDA. In the supplied attachment, we also
provide the complete code that has already been tested on Delta node.</p>
<p>&nbsp;&nbsp;&nbsp;<img alt="image80" src="https://portal.futuregrid.org/sites/default/files/resize/u28/cudaarchi_threadsmode-544x300.png" />
Figure&nbsp;1:&nbsp;GPU&nbsp;Kernel and Thread&nbsp;model&nbsp;[1]</p>
<p>&nbsp;<strong>CUDA Kernel&nbsp;and Threads:</strong>
&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The fundamental part of the CUDA code&nbsp;is the kernel program.
Kernel is&nbsp;the&nbsp;function that can&nbsp;be&nbsp;executed&nbsp;in parallel in&nbsp;the GPU
device.&nbsp;A CUDA kernel&nbsp;is executed by&nbsp;an array of CUDA&nbsp;threads. All
threads run the same code.&nbsp;Each thread has&nbsp;an ID that it uses to compute
memory address&nbsp;and make a control decision.&nbsp;CUDA supports to run
thousands of&nbsp;threads on the GPU.&nbsp;CUDA&nbsp;organizes&nbsp;thousands
of&nbsp;threads&nbsp;into a hierarchy of a grid of thread blocks.&nbsp;A grid is a set
of thread blocks that can be processed on the device&nbsp;in parallel. A
thread block is a set of concurrent threads that can cooperate among
themselves through a synchronization barrier and access to a shared
memory space private to the block. Each thread is given a unique thread
ID thread.Idx within its thread block.&nbsp;Each thread block is given a
unique block&nbsp;ID block.Idx within its&nbsp;grid.</p>
<p>&nbsp; <strong>CUDA Kernel&nbsp;code for Matrix Multiplication:</strong></p>
<p>&nbsp;__global__ void&nbsp;matrixMul( float* C, float* A, float* B, int
wA, int wB)
&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// Block index
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int bx = blockIdx.x;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int by = blockIdx.y;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Thread index
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int tx = threadIdx.x;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int ty = threadIdx.y;</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp; // Index of the first sub-matrix of A processed by the block
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int aBegin = wA * BLOCK_SIZE * by;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// Index of the last sub-matrix of A processed by the block
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int aEnd&nbsp;&nbsp; = aBegin + wA - 1;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// Step size used to iterate through the sub-matrices of A
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int aStep&nbsp; = BLOCK_SIZE;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// Index of the first sub-matrix of B processed by the block
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int bBegin = BLOCK_SIZE * bx;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// Step size used to iterate through the sub-matrices of B
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int&nbsp;bStep&nbsp; = BLOCK_SIZE * wB;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;Csub is used to store the element of the block
sub-matrix&nbsp;that is computed by the thread
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;float&nbsp;Csub = 0;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// Loop over all the sub-matrices of A and B&nbsp;required to compute
the block sub-matrix
&nbsp;&nbsp;&nbsp;&nbsp; for (int a = aBegin, b = bBegin; a &lt;= aEnd; a += aStep, b +=
bStep) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Declaration of the shared memory array As used to&nbsp;store
the sub-matrix of A
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Declaration of the shared memory array Bs used to&nbsp;store
the sub-matrix of B
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; As[ty][tx] = A[a + wA * ty +
tx];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bs[ty],[tx] = B[b + wB * ty +
tx];
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Synchronize to make sure the matrices are
loaded</p>
<p>__syncthreads();
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;//&nbsp;multiply two matrices together;&nbsp;each thread computes&nbsp;one
element&nbsp;&nbsp;of&nbsp;&nbsp;sub-matrix
&nbsp;#pragma
unroll
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; for (int k = 0; k &lt; BLOCK_SIZE;
++k)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Csub += As[ty][k]&nbsp;*
Bs[k][tx];</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // Synchronize to make sure that the preceding&nbsp;computation
is done</p>
<p>__syncthreads();</p>
<p>}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;// Write the block sub-matrix to device memory; each thread
only writes one&nbsp;element!
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int c = wB * BLOCK_SIZE * by + BLOCK_SIZE *
bx;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C[c + wB * ty + tx] =
Csub;
}</p>
<p>&nbsp;&nbsp;<img alt="image81" src="https://portal.futuregrid.org/sites/default/files/resize/u28/cudaMemoryArchitecture-500x173.png" />
Figure 2: GPU memory&nbsp;architecture&nbsp;[1][1]&nbsp;[1]</p>
<p>&nbsp;<strong>CUDA Memory Architecture:</strong>
&nbsp;&nbsp;&nbsp;&nbsp;All&nbsp;multiprocessors of the GPU&nbsp;device access a&nbsp;large global
device memory for both gather and scatter operations. This&nbsp;memory is
relatively slow because it does not provide caching. Shared memory is
fast compared to device memory, and normally takes the same amount of
time as required to access registers. Shared memory is local to each
multiprocessor unlike device memory and allows more efficient local
synchronization. It is divided into many parts. Each thread block within
a multiprocessor accesses its own part of shared memory, and this part
of shared memory is not accessible by any other thread block of this
multiprocessor or of some other multiprocessor. All threads within a
thread block that have the same lifetime as the block share this part&nbsp;of
memory for both read and write operations. To declare variables in
shared memory, __shared__ qualifier is used, and to declare in
global memory, __device__ qualifier is used.</p>
<p><strong>CPU code invoke&nbsp;CUDA&nbsp;kernel code:</strong></p>
<p>void&nbsp;invoke_matrixMul(int size){</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int devID;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cudaDeviceProp props;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; checkCudaErrors(cudaGetDevice(&amp;devID));
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; checkCudaErrors(cudaGetDeviceProperties(&amp;props, devID));</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; int block_size = (props.major &lt; 2) ? 16 : 32;
&nbsp;&nbsp;&nbsp; unsigned int uiWA, uiHA,&nbsp;uiWB, uiHB, uiWC,&nbsp;uiHC;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; uiWA = uiHA=&nbsp;uiWB =&nbsp;uiHB =&nbsp;uiWC =&nbsp;uiHC;</p>
<p>&nbsp;&nbsp;&nbsp; // allocate host memory for matrices A and B
&nbsp;&nbsp;&nbsp; unsigned int size_A = uiWA * uiHA;
&nbsp;&nbsp;&nbsp; unsigned int mem_size_A = sizeof(float) * size_A;
&nbsp;&nbsp;&nbsp; float* h_A = (float*)malloc(mem_size_A);
&nbsp;&nbsp;&nbsp; unsigned int size_B = uiWB * uiHB;
&nbsp;&nbsp;&nbsp; unsigned int mem_size_B = sizeof(float) * size_B;
&nbsp;&nbsp;&nbsp; float* h_B = (float*)malloc(mem_size_B);</p>
<p><strong>&nbsp;&nbsp;&nbsp;&nbsp;</strong>// initialize host memory
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; srand(2012);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; randomInit(h_A, size_A);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; randomInit(h_B, size_B);</p>
<p>&nbsp;&nbsp;&nbsp; // allocate device memory
&nbsp;&nbsp;&nbsp; float* d_A, *d_B, *d_C;
&nbsp;&nbsp;&nbsp; unsigned int size_C = uiWC * uiHC;
&nbsp;&nbsp;&nbsp; unsigned int mem_size_C = sizeof(float) * size_C;</p>
<p>&nbsp;&nbsp;&nbsp; // allocate host memory for the result
&nbsp;&nbsp;&nbsp; float* h_C&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = (float*) malloc(mem_size_C);
&nbsp;&nbsp;&nbsp; float* h_CUBLAS = (float*)
malloc(mem_size_C);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; checkCudaErrors(cudaMalloc((void**) &amp;d_A,
mem_size_A));
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; checkCudaErrors(cudaMalloc((void**) &amp;d_B,
mem_size_B));
&nbsp;&nbsp;&nbsp; // copy host memory to
device
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; checkCudaErrors(cudaMemcpy(d_A, h_A, mem_size_A,
cudaMemcpyHostToDevice) );
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; checkCudaErrors(cudaMemcpy(d_B, h_B, mem_size_B,
cudaMemcpyHostToDevice) );
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;checkCudaErrors(cudaMalloc((void**) &amp;d_C,
mem_size_C));
&nbsp;&nbsp;&nbsp; // setup execution
parameters
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; dim3 threads(block_size,
block_size);
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; dim3 grid(uiWC / threads.x, uiHC /
threads.y);</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp; //Performs warmup operation using matrixMul CUDA
kernel
&nbsp;&nbsp;&nbsp; if (block_size 16) {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; matrixMul&lt;16&gt;&lt;&lt;&lt; grid, threads &gt;&gt;&gt;(d_C, d_A,
d_B, uiWA, uiWB);
&nbsp;&nbsp;&nbsp; } else
{
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; matrixMul&lt;32&gt;&lt;&lt;&lt; grid, threads &gt;&gt;&gt;(d_C, d_A, d_B,
uiWA, uiWB);</p>
<p>}</p>
<p>cudaDeviceSynchronize();</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;// clean up
memory</p>
<p>free(h_A);</p>
<p>free(h_B);
&nbsp;&nbsp;&nbsp; free(h_C);
&nbsp;}</p>
<p><strong>References:</strong>
[1]&nbsp;High&nbsp;Performance&nbsp;Computing&nbsp;with&nbsp;CUDA,&nbsp;2009&nbsp;User&nbsp;Group&nbsp;Conference
[2]&nbsp;<a class="reference external" href="http://www.nvidia.com/content/global/global.php">http://www.nvidia.com/content/global/global.php</a></p>
<p>source&nbsp;code:&nbsp;git clone&nbsp;<a class="reference external" href="mailto:git&#37;&#52;&#48;github&#46;com">git<span>&#64;</span>github<span>&#46;</span>com</a>:futuregrid/GPU.git</p>
<p>Usage:
module load cuda
module load&nbsp;intel
&nbsp;&nbsp;nvcc&nbsp;-c matrixMul.cu -L/opt/cuda/lib64&nbsp;-lcudart</p>
<table border="1" class="docutils">
<colgroup>
<col width="89%" />
<col width="11%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Attachment</th>
<th class="head">Size</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference external" href="https://portal.futuregrid.org/sites/default/files/matrixMul_0.zip">matrixMul.zip</a></td>
<td>3.13 KB</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="c-means-clustering-using-cuda-on-gpu">
<h2>C-means clustering using CUDA on GPU<a class="headerlink" href="#c-means-clustering-using-cuda-on-gpu" title="Permalink to this headline"></a></h2>
<p><strong>Summary:</strong>
The computational&nbsp;demands for multivariate clustering&nbsp;are
increasing&nbsp;rapidly, and therefore&nbsp;processing large data sets is
time&nbsp;consuming on&nbsp;a single CPU. To address the computational demands,&nbsp;we
implemented the cmeans&nbsp;clustering&nbsp;algorithm, using&nbsp;the&nbsp;NVIDIA&#8217;s CUDA&#8217;s
framework and the latest GPU&nbsp;devices on the Delta machine.</p>
<p><strong>Fuzzy C-Means Clustering</strong>
&nbsp;Fuzzy c-means&nbsp;is an algorithm&nbsp;of&nbsp;clustering which allows one&nbsp;element
to belong to two or more clusters with&nbsp;different probability.&nbsp;This
method&nbsp;is frequently used in multivariate clustering.&nbsp;This algorithm&nbsp;is
based on minimization of the following objective function: <img alt="image82" src="https://portal.futuregrid.org/sites/default/files/u28/cmeans_objective_function.gif" />
Here, M&nbsp;is a real number greater than 1, N is the number&nbsp;of&nbsp;elements,
Uij is the value&nbsp;of membership of&nbsp;Xi&nbsp;in cluster Cj,&nbsp; xi is the ith of
d-dimensional measured data, cj is the d-dimension center of the
cluster, and ||Xi-Cj|| is any norm expressing the similarity between
any measured data and the center. &nbsp;Fuzzy partitioning is&nbsp;performed
through an iterative optimization of the objective function shown above.
Within each&nbsp;iteration,&nbsp;the algorithm&nbsp;updates&nbsp;the&nbsp;membership&nbsp;uij and the
cluster centers cj by:
<a href="#id84"><span class="problematic" id="id85">|image83|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |image84|</span></a>
This iteration will stop when <img alt="image85" src="https://portal.futuregrid.org/sites/default/files/u28/cmeans_stop_condition.gif" />, where&nbsp;&#8216;e&#8217;&nbsp;is a termination
criterion between 0 and 1, and k represents the iteration steps.
<strong>**Algorithm of CUDA C-means:</strong></p>
<blockquote>
<div>1**) Copy data to GPU</div></blockquote>
<ol class="arabic simple" start="2">
<li>DistanceMatrix kernel</li>
<li>MembershipMatrix kernel</li>
<li>UpdateCenters kernel, copy partial centers to host from GPUs</li>
<li>ClusterSizes kernel, copy cluster sizes to host from each GPU</li>
</ol>
<p>6) Aggregate partial cluster centers and reduce
10) Compute difference between current cluster centers and previous
iteration.
11) Compute cluster distance&nbsp;and&nbsp;memberships using final centers.</p>
<p>&nbsp;&nbsp;<strong>&nbsp;&nbsp;CUDA kernels of C-means program:</strong>
1) DistanceMatrix
2)&nbsp;MembershipMatrix
3)&nbsp;UpdateCetners
4)&nbsp;ClusterSizes</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;<strong>CUDA&nbsp;C-means performance on Delta:</strong>
<img alt="image86" src="https://portal.futuregrid.org/sites/default/files/resize/u28/cmeansPerformance2-600x178.png" />
Figure&nbsp;1:&nbsp;C-means performance&nbsp;using GPU and&nbsp;CPU
&nbsp;&nbsp;Reference:
&nbsp;&nbsp;[1]
<a class="reference external" href="http://en.wikipedia.org/wiki/Cluster_analysis">http://en.wikipedia.org/wiki/Cluster_analysis</a>
&nbsp;&nbsp;[2] Scalable Data Clustering using GPU&nbsp;Clusters,&nbsp; Andrew
Pangborn,&nbsp;Gregor von Laszewski</p>
<p>Average: Select ratingPoorOkayGoodGreatAwesome</p>
<p>Your rating: None Average: 4 (1 vote)</p>
</div>
<div class="section" id="scalemp-vsmp">
<h2>ScaleMP vSMP<a class="headerlink" href="#scalemp-vsmp" title="Permalink to this headline"></a></h2>
<p>FutureGrid provides a new experimental distributed large memory SMP
machine.</p>
</div>
<div class="section" id="accessing-scalemp">
<h2>Accessing ScaleMP:<a class="headerlink" href="#accessing-scalemp" title="Permalink to this headline"></a></h2>
<p>Access of the ScaleMP vSMP machine is managed through the job queueing
system on India. Specifically, you must submit a job to the scalemp
queue on the India cluster. If you haven&#8217;t already, first log into India
and prepare your environment path:</p>
<div class="section" id="id37">
<h3>Submitting a job:<a class="headerlink" href="#id37" title="Permalink to this headline"></a></h3>
<p>From here, you are now able to submit to the scalemp queue:</p>
<p>OR run a job interactively</p>
</div>
</div>
<div class="section" id="developing-a-job-script">
<h2>Developing a job script:<a class="headerlink" href="#developing-a-job-script" title="Permalink to this headline"></a></h2>
<p>As the ScaleMP vSMP machine is a unique and tool, it requires some
simple but special configuration in order to take full advantage of its
capabilities. &nbsp;This customization depends on the type of application you
are looking to run, so please consult the configuration type that best
fits your application.</p>
<div class="section" id="mpi">
<h3>MPI:<a class="headerlink" href="#mpi" title="Permalink to this headline"></a></h3>
<p>Below is an example script to run a MPI job. &nbsp;Please note that we
currently only support using MPICH2. &nbsp;Using OpenMPI and IntelMPI is
possible, however we do not currently support such usage at this time.
&nbsp;See /opt/ScaleMP/examples/ for more information.</p>
</div>
<div class="section" id="openmp">
<h3>OpenMP:<a class="headerlink" href="#openmp" title="Permalink to this headline"></a></h3>
<p>Below is an example script for running your OpenMP code on the scalemp
machine. &nbsp;See /opt/ScaleMP/examples/OpenMP for more information</p>
</div>
<div class="section" id="threaded">
<h3>Threaded:<a class="headerlink" href="#threaded" title="Permalink to this headline"></a></h3>
<p>Below is an example script for running a job using Pthreads.&nbsp;See
/opt/ScaleMP/examples/Pthread for more information</p>
</div>
<div class="section" id="throughput">
<h3>Throughput:<a class="headerlink" href="#throughput" title="Permalink to this headline"></a></h3>
<p>Below is an example script to run a throughput job.&nbsp;See
/opt/ScaleMP/examples/ for more information.</p>
<hr class="docutils" />
<p>Below is an example script to run a serial job. &nbsp;See
/opt/ScaleMP/examples/ for more information.</p>
</div>
</div>
<div class="section" id="iaas-infrastructure-as-a-service">
<h2>IaaS - Infrastructure as a Service<a class="headerlink" href="#iaas-infrastructure-as-a-service" title="Permalink to this headline"></a></h2>
<p>This chapter contains information in regards to Infrastructure as a
Service offerings on FutureGrid</p>
</div>
<div class="section" id="using-iaas-clouds-on-futuregrid">
<h2>Using IaaS Clouds on FutureGrid<a class="headerlink" href="#using-iaas-clouds-on-futuregrid" title="Permalink to this headline"></a></h2>
<p>Infrastructure-as-a-Service (IaaS) cloud computing encompasses
techniques that have driven major recent advances in information
technology supporting elastic, on-demand, &#8220;pay as you go&#8221; computing as a
service. Key technologies behind IaaS cloud computing are resource
virtualization, as well as cloud middleware that enables the management
of clusters of virtualized resources through service interfaces.</p>
<p>The FutureGrid testbed provides capabilities that allow users to
experiment with open-source cloud middleware and virtualization
platforms, and there are different ways you may want to use these
platforms in the testbed. This page guides you in selecting from
FutureGrid capabilities best suited to your goals, and provides links to
respective tutorials:</p>
<blockquote>
<div>Nimbus Clouds</div></blockquote>
<hr class="docutils" />
<p>Nimbus is an open-source service package that allows users to run
virtual machines on FutureGrid hardware. You can easily upload your own
VM image or customize an image provided by us. When you boot a VM, it is
assigned a public IP address (and/or an optional private address), and
you are authorized to log in as root via SSH. You can then run services,
perform computations, and configure the system as desired.</p>
<p>Nimbus is available across various FutureGrid sites, and there are
two open-source hypervisors in use in FutureGrid Nimbus clouds: Xen and
KVM. Nimbus in FutureGrid is the recommended platform if you are
interested in experiments within a cloud, across clouds, or in those not
amenable to para-virtualization, as the Nimbus/KVM cloud (alamo)
supports &#8220;classic&#8221; virtual machines.</p>
<p>For tutorials on getting started with Nimbus, see:</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/nimbus">Using Nimbus on
FutureGrid</a>&nbsp;[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/nm2">Nimbus One-click Cluster
Guide</a>&nbsp;[intermediate]</li>
</ul>
<div class="section" id="openstack-clouds">
<h3>OpenStack Clouds<a class="headerlink" href="#openstack-clouds" title="Permalink to this headline"></a></h3>
<p><a class="reference external" href="http://www.openstack.org/">OpenStack</a>&nbsp;is a recently open-sourced,
IaaS cloud computing platform founded by Rackspace Hosting and NASA, and
used widely in industry. It includes three components: Compute(Nova),
Object Storage (Swift), and Image Service (Glance). OpenStack Nova
supports an&nbsp; Amazon Web Services (AWS) complaint EC2-based web service
interface for interacting with the Cloud service, and can be used with
the same client-side &#8220;eucatools&#8221; that is used with Eucalyptus.</p>
<p>FutureGrid currently features the OpenStack Nova compute cloud.
OpenStack Nova in FutureGrid is useful if you are interested in
experiments within a cloud, and in comparison of cloud middleware
stacks.</p>
<p>For tutorials on getting started with OpenStack, see:</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/openstack">Using OpenStack Nova on
FutureGrid</a>&nbsp;[novice]<ul>
<li>This tutorial targets all users of OpenStack in FutureGrid; it
describes how to get started with FutureGrid OpenStack resources.</li>
</ul>
</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/os1">Running an OpenStack virtual appliance on
FutureGrid</a>&nbsp;[novice]<ul>
<li>This tutorial targets users interested in education and training
on OpenStack internals; it describes how to run a virtual, private
OpenStack deployment as an appliance.</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="eucalyptus-clouds">
<h3>Eucalyptus Clouds<a class="headerlink" href="#eucalyptus-clouds" title="Permalink to this headline"></a></h3>
<p>Eucalyptus is an open-source software platform that implements
IaaS-style cloud computing. Eucalyptus provides an Amazon Web Services
(AWS) complaint EC2-based web service interface for interacting with the
Cloud service. Additionally, Eucalyptus provides services such as the
AWS Complaint Walrus and a user interface for managing users and
images.</p>
<p>Eucalyptus is also available on distributed FutureGrid resources.
Eucalyptus in FutureGrid is useful if&nbsp; you are interested in experiments
within a cloud, across clouds, and in comparison of cloud middleware
stacks.</p>
<p>For tutorials on getting started with Eucalyptus, see:</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/eucalyptus3">Using&nbsp;Eucalyptus on
FutureGrid</a>&nbsp;[novice]</li>
</ul>
</div>
<div class="section" id="virtual-appliances-for-training-and-education">
<h3>Virtual Appliances for Training and Education<a class="headerlink" href="#virtual-appliances-for-training-and-education" title="Permalink to this headline"></a></h3>
<p>The IaaS cloud stacks on FutureGrid enable the use of &#8220;virtual
appliances&#8221; as an environment where hands-on, executable educational and
training modules can be created, shared, and leveraged by the FutureGrid
community. With these appliances, students are able to deploy virtual
machines and virtual private clusters, where they are able to experiment
with various Grid and cloud computing middleware stacks.</p>
<p>For tutorials on getting started with educational virtual appliances,
see:</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/ga1">Running a Grid Appliance on your
desktop</a>&nbsp;&nbsp;[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/ga9">Running a Grid Appliance on
FutureGrid</a>&nbsp;[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/ga8">Running Condor tasks on the Grid
Appliance</a>&nbsp;[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/mp1">Running MPI tasks on the Grid
Appliance</a>&nbsp;[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/ga10">Running Hadoop&nbsp;tasks on the Grid
Appliance</a>&nbsp;[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/os1">Running an OpenStack virtual appliance on
FutureGrid</a>&nbsp;[novice]</li>
</ul>
</div>
</div>
<div class="section" id="using-nimbus-on-futuregrid">
<h2>Using Nimbus on FutureGrid<a class="headerlink" href="#using-nimbus-on-futuregrid" title="Permalink to this headline"></a></h2>
<div class="section" id="image87">
<h3><a class="reference external" href="http://www.nimbusproject.org/"><img alt="image87" src="https://portal.futuregrid.org/sites/default/files/images/nimbus_logo.png" /></a><a class="headerlink" href="#image87" title="Permalink to this headline"></a></h3>
</div>
<div class="section" id="what-is-nimbus">
<h3>What is Nimbus?<a class="headerlink" href="#what-is-nimbus" title="Permalink to this headline"></a></h3>
<p>Nimbus is an open source service package that allows users to run
virtual machines on FutureGrid hardware. You can easily upload your own
VM image or customize an image provided by us. When you boot a VM, it is
assigned a public IP address (and/or an optional private address); you
are authorized to log in as root via SSH. You can then run services,
perform computations, and configure the system as desired. After using
and configuring the VM, you can save the modified VM image back to the
Nimbus image repository.</p>
</div>
<div class="section" id="nimbus-on-futuregrid">
<h3>Nimbus on FutureGrid<a class="headerlink" href="#nimbus-on-futuregrid" title="Permalink to this headline"></a></h3>
<p>Nimbus is installed on four FutureGrid clusters:</p>
<ol class="arabic">
<li><dl class="first docutils">
<dt><strong>Hotel</strong>&nbsp;(University of Chicago)</dt>
<dd><p class="first last">41 nodes, 328 cores</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>Foxtrot</strong>&nbsp;(University of Florida)</dt>
<dd><p class="first last">26 nodes, 208 cores</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>Sierra</strong>&nbsp;(San Diego Supercomputer Center)</dt>
<dd><p class="first last">18 nodes, 144 cores</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>Alamo</strong>(Texas Advanced Computing Center)</dt>
<dd><p class="first last">15 nodes, 120 cores</p>
</dd>
</dl>
</li>
</ol>
<p>By default, users are limited to running 16 VMs simultaneously and
claiming two cores per VM. If you have a good reason for this limitation
to be lifted for your account,
contact&nbsp;<a class="reference external" href="https://portal.futuregrid.org/help">https://portal.futuregrid.org/help</a>.</p>
<p>All FutureGrid users are allowed access to Nimbus on all sites.</p>
</div>
<div class="section" id="id38">
<h3>Getting Started<a class="headerlink" href="#id38" title="Permalink to this headline"></a></h3>
<p>Nimbus provides services that can be controlled remotely using a variety
of clients. In this tutorial, we will use a simple command line tool
called the&nbsp;<strong>cloud-client</strong>. If you&#8217;d rather have programmatic
control, the Amazon EC2 protocols&nbsp;<a class="reference external" href="http://www.nimbusproject.org/docs/current/elclients.html">are
supported</a>,
which have a variety of excellent clients&nbsp;available&nbsp;for many
languages.</p>
<div class="section" id="log-into-hotel">
<h4>Log into hotel<a class="headerlink" href="#log-into-hotel" title="Permalink to this headline"></a></h4>
<p>The first step is to ssh into hotel.futuregrid.org.&nbsp; While you can use
Nimbus clients from anywhere in the world, we recommend that you start
on hotel because the correct version of Java is installed there.</p>
<div class="highlight-python"><pre>$ ssh -A hotel.futuregrid.org</pre>
</div>
<p>If this command fails, contact
<a class="reference external" href="https://portal.futuregrid.org/help">https://portal.futuregrid.org/help</a>.
It likely means one of the following:</p>
<ol class="arabic simple">
<li>Your account is not yet set up.</li>
<li>You provide no public key or an corrupted public key.</li>
<li>The private key you are using does not match the public one you
registered with FutureGrid.</li>
</ol>
</div>
<div class="section" id="download-and-install-cloud-client">
<h4>Download and install cloud-client<a class="headerlink" href="#download-and-install-cloud-client" title="Permalink to this headline"></a></h4>
<p>Download the Nimbus cloud client from the&nbsp;<a class="reference external" href="http://www.nimbusproject.org/downloads/">Nimbus
website</a>.</p>
<div class="highlight-python"><pre>$ wget http://www.nimbusproject.org/downloads/nimbus-cloud-client-021.tar.gz</pre>
</div>
<p>Unpack the archive onto your system. You can also use one of the
FutureGrid login nodes directly.</p>
<div class="highlight-python"><pre>$ tar xzf nimbus-cloud-client-021.tar.gz
$ ls nimbus-cloud-client-021/
CHANGES.txt README.txt  conf        lib
LICENSE.txt bin     history     samples
Obtain your Nimbus credentials and configuration files</pre>
</div>
</div>
<div class="section" id="obtain-your-nimbus-credentials-and-configuration-files">
<h4>Obtain Your Nimbus Credentials and Configuration Files<a class="headerlink" href="#obtain-your-nimbus-credentials-and-configuration-files" title="Permalink to this headline"></a></h4>
<p>In your home directory on hotel, you will find the file
nimbus_creds.tar.gz:</p>
<div class="highlight-python"><pre> username@hotel $ ls ~/nimbus_creds.tar.gz
nimbus_creds.tar.gz</pre>
</div>
<p>If your credentials are not present on&nbsp;<strong>Hotel</strong>,
contact&nbsp;<a class="reference external" href="https://portal.futuregrid.org/help">https://portal.futuregrid.org/help</a>.</p>
<p>Back on your system, download and unpack these files into your
cloud-client&#8217;s&nbsp;directory:</p>
<div class="highlight-python"><pre>$ cd nimbus-cloud-client-021/conf/
$ tar xvzf ~/nimbus_creds.tar.gz
usercert.pem
userkey.pem
cloud.properties
hotel.conf
sierra.conf
foxtrot.conf
alamo.conf</pre>
</div>
<p>Now you should have a functional cloud client. To begin, check out the
help text and&nbsp;file.</p>
<div class="highlight-python"><pre>$ cd ../</pre>
</div>
<div class="highlight-python"><pre>$ bin/cloud-client.sh --help</pre>
</div>
</div>
<div class="section" id="id39">
<h4>Check Your ssh Key<a class="headerlink" href="#id39" title="Permalink to this headline"></a></h4>
<p>In order to use Nimbus clouds effectively, you need to have your ssh
public key in a known place so that it can be injected into your VM, and
thus allow you (and only you) root access to your VM. When creating your
FutureGrid account you had to upload an ssh public key.&nbsp; That key can be
found on hotel in the file <em>~/.ssh/authorized_keys</em>.&nbsp; If you were able
to ssh into hotel then this is the public key are are currently using.
Nimbus needs this key to be in the<em>~/.ssh/id_rsa.pub</em>:</p>
<div class="highlight-python"><pre>$ cp ~/.ssh/authorized_keys ~/.ssh/id_rsa.pub</pre>
</div>
<p>Because the security environment can be complicated, cloud-client has an
option to help verify that things are working.&nbsp; Run the following
command to display some information about your security environment:</p>
<div class="highlight-python"><pre>$ ./bin/cloud-client.sh --security</pre>
</div>
</div>
</div>
<div class="section" id="using-the-cloud-client">
<h3>Using the Cloud Client<a class="headerlink" href="#using-the-cloud-client" title="Permalink to this headline"></a></h3>
<div class="section" id="check-out-the-various-futuregrid-clouds">
<h4>Check out the various FutureGrid clouds<a class="headerlink" href="#check-out-the-various-futuregrid-clouds" title="Permalink to this headline"></a></h4>
<p>When the credentials file was untarred in a step above, a
configuration file for each of the four FutureGrid clouds was put in
your <em>conf/</em>directory.&nbsp; Now let&#8217;s take a look at accessing each of
those clouds and seeing what virtual machines are available for use.
This will require two options to cloud client.&nbsp; The first is &#8211;conf ;
this is used to select the cloud you wish to use.&nbsp; Simply provide a path
to the cloud configuration file.&nbsp; This &#8211;conf switch will be used in all
commands to direct cloud-client at the cloud of interest.</p>
<p>The second option is &#8211;list.&nbsp; This will simply provide a listing of
all the available virtual machines:</p>
<div class="highlight-python"><pre>$ bin/cloud-client.sh --conf conf/hotel.conf --list</pre>
</div>
<p>This command should list the available images on the system.&nbsp; Notice the
hello-cloud virtual machine.&nbsp; This is the test image we will use in this
tutorial:</p>
<div class="highlight-python"><pre>[Image] 'hello-cloud'                    Read only
        Modified: Jan 13 2011 @ 14:15   Size: 576716800 bytes (~550 MB)</pre>
</div>
</div>
<div class="section" id="run-a-virtual-machine">
<h4>Run a Virtual Machine<a class="headerlink" href="#run-a-virtual-machine" title="Permalink to this headline"></a></h4>
<p>Next, try to boot a virtual machine:</p>
<div class="highlight-python"><pre>$ bin/cloud-client.sh --conf conf/hotel.conf --run --name hello-cloud --hours 2
Launching workspace.</pre>
</div>
<div class="highlight-python"><pre>Workspace Factory Service:
     https://svc.uc.futuregrid.org:8443/wsrf/services/WorkspaceFactoryService</pre>
</div>
<div class="highlight-python"><pre>::</pre>
</div>
<blockquote>
<div>Creating workspace &#8220;vm-001&#8221;... done.</div></blockquote>
<div class="highlight-python"><pre> IP address: 149.165.148.253
        Hostname: vm-253.uc.futuregrid.org
      Start time: Wed Jul 25 15:44:33 CDT 2012
   Shutdown time: Wed Jul 25 17:44:33 CDT 2012
Termination time: Wed Jul 25 17:46:33 CDT 2012</pre>
</div>
<div class="highlight-python"><pre>Waiting for updates.</pre>
</div>
<div class="highlight-python"><pre>"vm-001" reached target state: Running</pre>
</div>
<p>Once the image is running, you should be able to log into it with SSH.
Note that you may need to wait another minute or so before you can
actually get it, as the system needs time to boot and start services.
Log in as the root user, and connect to the host printed out by the run
command.&nbsp; Note that you <strong>must</strong>run this command in a location that
has access to your private key.&nbsp; This means it must be in the <em>~/.ssh/</em>
directory on the file system from where you launch this command, or you
must have used the -A option to ssh when logging into hotel (as is shown
above).</p>
<div class="highlight-python"><pre>$ ssh root@vm-253.uc.futuregrid.org</pre>
</div>
</div>
<div class="section" id="create-a-new-vm-image">
<h4>Create a New VM Image<a class="headerlink" href="#create-a-new-vm-image" title="Permalink to this headline"></a></h4>
<p>Once you have a root shell on your VM, you may modify it as through it
were a real machine.&nbsp; Here we encourage you to make some changes.
Create a new user, install some additional software, or simply create a
text file in the root user&#8217;s account:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># touch /root/CHANGE</span>
<span class="c"># exit</span>
</pre></div>
</div>
</div>
<div class="section" id="save-the-changes-to-a-new-vm">
<h4>Save the Changes to a New VM<a class="headerlink" href="#save-the-changes-to-a-new-vm" title="Permalink to this headline"></a></h4>
<p>Now that you have modified the VM ,you can save it back into your
personal repository.&nbsp; To do this, you will use the &#8211;save and &#8211;newname
options.&nbsp; You will also need the VM handle as it was displayed in the
output from the run command.&nbsp; If you have forgotten what this was, you
can use the &#8211;status option to find it:</p>
<div class="highlight-python"><pre>$ ./bin/cloud-client.sh --conf conf/hotel.conf --status
Querying for ALL instances.

[*] - Workspace #32292. 149.165.148.253 [ vm-253.uc.futuregrid.org ]
 State: Running
 Duration: 120 minutes.
 Start time: Wed Jul 25 15:44:33 CDT 2012
 Shutdown time: Wed Jul 25 17:44:33 CDT 2012
 Termination time: Wed Jul 25 17:46:33 CDT 2012
 *Handle: vm-001
 Image: hello-cloud</pre>
</div>
<p>Note the handle <em>vm-001</em>.&nbsp; To save the VM for future use, run the
following command:</p>
<div class="highlight-python"><pre>$ ./bin/cloud-client.sh --conf conf/hotel.conf --save --newname myvm --handle vm-001

Saving workspace.
  - Workspace handle (EPR): '/N/u/bresnaha/nimbus-cloud-client-021/history/vm-001/vw-epr.xml'
  - New name: 'myvm'

Waiting for updates.

The image has successfully been transferred to your repository directory.

Finalizing the deployment now (terminating the resource lease).</pre>
</div>
<p>Do another listing of that cloud and you will see your VM is now
available for launch:</p>
<div class="highlight-python"><pre>$ ./bin/cloud-client.sh --conf conf/hotel.conf --list
[Image] 'myvm'                           Read/write
        Modified: Jul 25 2012 @ 20:49   Size: 576716800 bytes (~550 MB)

----

[Image] 'hello-cloud'                    Read only
        Modified: Apr 8 2011 @ 13:56   Size: 576716800 bytes (~550 MB)</pre>
</div>
</div>
<div class="section" id="launch-your-new-vm">
<h4>Launch Your New VM<a class="headerlink" href="#launch-your-new-vm" title="Permalink to this headline"></a></h4>
<p>You can now launch your new VM just like you did the hello-cloud VM
above, simply changing the name from <em>hello-cloud</em> to <em>myvm&nbsp;</em>:</p>
<div class="highlight-python"><pre>$ ./bin/cloud-client.sh --conf conf/hotel.conf --run --name myvm --hours 2

Launching workspace.

Workspace Factory Service:
 https://svc.uc.futuregrid.org:8443/wsrf/services/WorkspaceFactoryService

Creating workspace "vm-002"... done.


 IP address: 149.165.148.151
 Hostname: vm-151.uc.futuregrid.org
 Start time: Wed Jul 25 15:58:31 CDT 2012
 Shutdown time: Wed Jul 25 17:58:31 CDT 2012
Termination time: Wed Jul 25 18:08:31 CDT 2012

Waiting for updates.


"vm-002" reached target state: Running

Running: 'vm-002'</pre>
</div>
<p>SSH into the machine and verify that your changes persisted.</p>
</div>
<div class="section" id="terminate-the-vm">
<h4>Terminate the VM<a class="headerlink" href="#terminate-the-vm" title="Permalink to this headline"></a></h4>
<p>Your VM will terminate after its allocated time expires.&nbsp; In our
examples here, this is after 2 hours.&nbsp; However, you may wish to
terminate it earlier.&nbsp; You can do so by again using the &#8211;handle option
as you did in the <em>save a new VM step</em> and the &#8211;terminate option:</p>
<div class="highlight-python"><pre>$ ./bin/cloud-client.sh --conf conf/hotel.conf --terminate --handle vm-002

Terminating workspace.
  - Workspace handle (EPR): '/N/u/bresnaha/nimbus-cloud-client-021/history/vm-002/vw-epr.xml'

Destroying vm-002... destroyed.</pre>
</div>
</div>
</div>
<div class="section" id="virtual-clusters">
<h3>Virtual Clusters<a class="headerlink" href="#virtual-clusters" title="Permalink to this headline"></a></h3>
<p>This is a basic walkthrough of how to run a sample virtual cluster.
For more information on how they work, see
<em>http://www.nimbusproject.org/docs/current/clouds/clusters2.html .</em></p>
<div class="highlight-python"><pre>::</pre>
</div>
<div class="section" id="cluster-definition-file">
<h4>Cluster Definition File<a class="headerlink" href="#cluster-definition-file" title="Permalink to this headline"></a></h4>
<p>For this example, we will use a modification of the sample cluster file
that is distributed with the cloud client.&nbsp; The file can be found at
<em>https://portal.futuregrid.org/sites/default/files/tutorial-cluster.xml_.gz&nbsp;</em>.
Copy the file to where your cloud-client program is located, and unzip
it.&nbsp; Open the file and make note of the following:</p>
<ol class="arabic simple">
<li>There are 2 workspace definitions.</li>
<li>The head node has a quantity of 1 and a base image
base-cluster-cc14.gz.&nbsp; It has the roles of providing a nfs server.</li>
<li>The compute-nodes have the same image, but a quantity of 2.&nbsp; This
means there will be 1 head node and 2 compute-nodes in the virtual
cluster.&nbsp; This has the role of being a nfs client.</li>
</ol>
</div>
<div class="section" id="start-the-cluster">
<h4>Start the Cluster<a class="headerlink" href="#start-the-cluster" title="Permalink to this headline"></a></h4>
<div class="highlight-python"><pre>$ ./bin/cloud-client.sh --conf conf/sierra.conf --run --hours 2 --cluster &lt;path to your cluster document&gt;
SSH known_hosts contained tilde:
 - '~/.ssh/known_hosts' --&gt; '/N/u/bresnaha/.ssh/known_hosts'

Requesting cluster.
 - head-node: image 'base-cluster-cc14.gz', 1 instance
 - compute-nodes: image 'base-cluster-cc14.gz', 2 instances

Context Broker:
 https://s83r.idp.sdsc.futuregrid.org:8443/wsrf/services/NimbusContextBroker

Created new context with broker.

Workspace Factory Service:
 https://s83r.idp.sdsc.futuregrid.org:8443/wsrf/services/WorkspaceFactory...

Creating workspace "head-node"... done.
 - 198.202.120.134 [ vm-40.sdsc.futuregrid.org ]

Creating group "compute-nodes"... done.
 - 198.202.120.135 [ vm-41.sdsc.futuregrid.org ]
 - 198.202.120.136 [ vm-42.sdsc.futuregrid.org ]

Launching cluster-004... done.

Waiting for launch updates.
 - cluster-004: all members are Running
 - wrote reports to '/N/u/bresnaha/tutorial/nimbus-cloud-client-021/history/cluster-004/reports-vm'

Waiting for context broker updates.
 - cluster-004: contextualized
 - wrote ctx summary to '/N/u/bresnaha/tutorial/nimbus-cloud-client-021/history/cluster-004/reports-ctx/CTX-OK.txt'
 - wrote reports to '/N/u/bresnaha/tutorial/nimbus-cloud-client-021/history/cluster-004/reports-ctx'

SSH trusts new key for vm-40.sdsc.futuregrid.org  head-node

SSH trusts new key for vm-41.sdsc.futuregrid.org  compute-nodes #0

SSH trusts new key for vm-42.sdsc.futuregrid.org  compute-nodes #1</pre>
</div>
<p>This command takes a bit of time.&nbsp; What is happening is cloud-client
is instructing Nimbus to start up three VMs on the user&#8217;s behalf.
Information is put into the context broker.&nbsp; When each VM boots, the
context agent is run.&nbsp; The context agent checks in with the context
broker and asks for information reflecting the <em>requires</em> section in the
cluster document; similarly it registers its <em>provides</em> information with
the context broker for other VM context agents to query.&nbsp; The NFS
clients use this mechanism to provide the nfs server with their IP
addresses.&nbsp; The NFS server then gets this information out of the context
broker and uses it to authorize those IP addresses to remotely mount its
disks.</p>
<p>When it is complete, your virtual cluster will be ready to go.</p>
</div>
<div class="section" id="check-out-the-virtual-cluster">
<h4>Check Out the Virtual Cluster<a class="headerlink" href="#check-out-the-virtual-cluster" title="Permalink to this headline"></a></h4>
<p>Now ssh into one of the worker nodes and check out the file system.</p>
<div class="highlight-python"><pre>$ df -h
df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda1             2.9G  1.2G  1.7G  42% /
udev                  1.1G  144K  1.1G   1% /dev
shm                   1.1G     0  1.1G   0% /dev/shm
198.202.120.134:/home
                      2.9G  1.2G  1.7G  42% /home
198.202.120.134:/etc/grid-security/certificates
                      2.9G  1.2G  1.7G  42% /etc/grid-security/certificates</pre>
</div>
<p>Notice the NFS mounted home directory. &nbsp;Touch a file in that
directory.</p>
<div class="highlight-python"><pre>$ echo "Hello FutureGrid" &gt; /home/test_file</pre>
</div>
<p>Now ssh into the other worker node and verify that the test file is
visible to this node as well.</p>
<div class="highlight-python"><pre>$cat /home/test_file
Hello FutureGrid</pre>
</div>
<table border="1" class="docutils">
<colgroup>
<col width="89%" />
<col width="11%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Attachment</th>
<th class="head">Size</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference external" href="https://portal.futuregrid.org/sites/default/files/tutorial-cluster.xml_.gz">tutorial-cluster.xml_.gz</a></td>
<td>342 bytes</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="cloud-quick-start-launch-a-vm-with-1-command">
<h2>Cloud Quick Start : Launch a VM with 1 command<a class="headerlink" href="#cloud-quick-start-launch-a-vm-with-1-command" title="Permalink to this headline"></a></h2>
<p>This page explains how to very simply run a virtual machine (VM) on
FutureGrid Nimbus.&nbsp; The VM launched will mount your home files system on
hotel so data can be exchanged with the VM easily.&nbsp; Users new to cloud
computing, particularly those coming from Grid computing, and those who
are interested in getting a quick hands-on introduction to FutureGrid
clouds, should find this useful.</p>
</div>
<div class="section" id="launch-a-vm-via-nimbus">
<h2>Launch A VM via Nimbus<a class="headerlink" href="#launch-a-vm-via-nimbus" title="Permalink to this headline"></a></h2>
<p>Once you have a FutureGrid Nimbus account and ssh access to
hotel.futuregrid.org, you can easily begin using the cloud.&nbsp; All you
need to do is ssh into hotel with ssh forwarding enabled, and type a
single command:</p>
<div class="highlight-python"><pre>$ ssh -A hotel.futuregrid.org
$ /soft/nimbus/tools/bin/launch-vm.sh
Launching your VM (this may take a minute)...
Success!
Access your VM with: ssh root@vm-156.uc.futuregrid.org
Terminate your VM with: /soft/nimbus/tools/bin/cloudinitd.sh terminate 9ec20044</pre>
</div>
<p>Notice the output from the above command.&nbsp; It tells you how to access
the VM, and how to clean it up when you are finished with it.&nbsp; First
let&#8217;s access the VM and check out its file system.&nbsp; The VM should have
remotely mounted your home file system on FutureGrid.&nbsp; This will allow
you to easily copy data in and out of your VM.</p>
<div class="highlight-python"><pre>$ ssh root@vm-156.uc.futuregrid.org
$ vm-156:~# df -h
Filesystem Size Used Avail Use% Mounted on
tmpfs 1.1G 0 1.1G 0% /lib/init/rw
udev 10M 1.1M 9.0M 11% /dev
tmpfs 1.1G 0 1.1G 0% /dev/shm
rootfs 1.9G 731M 1.1G 42% /
sshfs#bresnaha@hotel.futuregrid.org:
 1000G 0 1000G 0% /mnt/hotel.futuregrid.org

$ ls /mnt/hotel.futuregrid.org
..... &lt;a listing of your FutureGrid home directory&gt;</pre>
</div>
</div>
<div class="section" id="options">
<h2>Options<a class="headerlink" href="#options" title="Permalink to this headline"></a></h2>
<div class="section" id="using-other-nimbus-clouds-on-futuregrid">
<h3>Using Other Nimbus Clouds on FutureGrid<a class="headerlink" href="#using-other-nimbus-clouds-on-futuregrid" title="Permalink to this headline"></a></h3>
<p>By default, the VMs are launched on hotel&#8217;s Nimbus cloud.&nbsp; However,
you can use this script to launch a VM on any cloud to which you have
access, including all of FutureGrid&#8217;s Nimbus clouds, FutureGrid&#8217;s
Eucalytpus clouds, and FutureGrid&#8217;s OpenStack clouds.&nbsp; To run on a
different FutureGrid Nimbus cloud, set the
env&nbsp;<em>CLOUDINITD_IAAS_URL</em>&nbsp;to one of the following:</p>
<ul class="simple">
<li>Hotel:
<a class="reference external" href="https://svc.uc.futuregrid.org:8444">https://svc.uc.futuregrid.org:8444</a></li>
<li>Sierra:
<a class="reference external" href="https://s83r.idp.sdsc.futuregrid.org:8444">https://s83r.idp.sdsc.futuregrid.org:8444</a></li>
<li>Foxtrot:
<a class="reference external" href="https://f1r.idp.ufl.futuregrid.org:9444">https://f1r.idp.ufl.futuregrid.org:9444</a>
(coming soon)</li>
<li>Alamo:
<a class="reference external" href="https://master1.futuregrid.tacc.utexas.edu:8444">https://master1.futuregrid.tacc.utexas.edu:8444</a>
(coming soon)</li>
</ul>
</div>
<div class="section" id="launching-multiple-vms">
<h3>Launching Multiple VMs<a class="headerlink" href="#launching-multiple-vms" title="Permalink to this headline"></a></h3>
<p>To launch many VMs at once, set the env
<em>CLOUDINITD_SSHFS_REPLICA_COUNT</em> to the Number of VMs that you would
like launched</p>
</div>
<div class="section" id="using-with-eucalyptus">
<h3>Using with Eucalyptus<a class="headerlink" href="#using-with-eucalyptus" title="Permalink to this headline"></a></h3>
<p>In order to use the tool with Eucalyptus, the following envs must be
set:</p>
<ul class="simple">
<li>export
CLOUDINITD_IAAS_URL=&#8217;<a class="reference external" href="http://149.165.146.135:8773/services/Eucalyptus">http://149.165.146.135:8773/services/Eucalyptus</a>&#8216;</li>
<li>export CLOUDINITD_IAAS_ACCESS_KEY=&lt;your access key&gt;</li>
<li>export CLOUDINITD_IAAS_SECRET_KEY=&lt;your access secret&gt;</li>
<li>export CLOUDINITD_IAAS_IMAGE=&#8217;emi-5F4F1B49&#8217;</li>
</ul>
<p>You may have trouble registering your keys.&nbsp; This is due to Eucalyptus
not yet supporting the latest version of the EC2 API.&nbsp; To circumvent
this, create a key pair named &#8216;futuregrid&#8217;, and set the env
<em>CLOUDINITD_IAAS_SSHKEYNAME</em>to &#8216;futuregrid&#8217;.&nbsp; Then, set the env
<em>CLOUDINITD_IAAS_SSHKEY</em> to point to the private key associated with
that key name.</p>
</div>
</div>
<div class="section" id="what-happens">
<h2>What Happens<a class="headerlink" href="#what-happens" title="Permalink to this headline"></a></h2>
<p>This script uses the program cloudinit.d
(<a class="reference external" href="http://www.nimbusproject.org/doc/cloudinitd/latest/">http://www.nimbusproject.org/doc/cloudinitd/latest/)</a>.
This is a Nimbus Platform multi-cloud tool for running cloud
applications.&nbsp; In this case, it runs a very simple 1 VM application.&nbsp; It
can launch any debian-based VM and install sshfs on it.&nbsp; Then it runs
sshfs inside of the VM to remotely mount your FutureGrid home directory.</p>
</div>
<div class="section" id="futuregrid-tutorial-nm2-nimbus-one-click-cluster-guide">
<h2>FutureGrid Tutorial NM2 - Nimbus One-Click Cluster Guide<a class="headerlink" href="#futuregrid-tutorial-nm2-nimbus-one-click-cluster-guide" title="Permalink to this headline"></a></h2>
<p><strong>Summary:</strong></p>
<p>This tutorial demonstrates using the Nimbus cloud client to create
auto-configured clusters of virtual machines on Nimbus.</p>
<p><strong>Prerequisites:</strong></p>
<p>Java 1.5+ and a working Nimbus cloud client.</p>
<p><strong>Hands-On Tutorial</strong>:</p>
<p>This tutorial is maintained at the Nimbus website. See&nbsp;<a class="reference external" href="http://www.nimbusproject.org/docs/current/clouds/clusters.html">One Click
Clusters</a>.</p>
<p><strong>Reference Material:</strong></p>
<p><a class="reference external" href="http://www.nimbusproject.org/docs/">Nimbus Documentation</a></p>
<p><strong>Authors:</strong></p>
<p>Tim Freeman, Kate Keahey, David LaBissoniere, John Bresnahan</p>
<p>University of Chicago</p>
</div>
<div class="section" id="using-eucalyptus-on-futuregrid">
<h2>Using Eucalyptus on FutureGrid<a class="headerlink" href="#using-eucalyptus-on-futuregrid" title="Permalink to this headline"></a></h2>
<p>Presentation about this document:
<a class="reference external" href="http://futuregrid.svn.sourceforge.net/viewvc/futuregrid/presentations/software/fg-tutorial-eucalyptus.ppt">fg-tutorial-eucalyptus.ppt</a></p>
<div class="section" id="summary">
<h3><strong>Summary</strong><a class="headerlink" href="#summary" title="Permalink to this headline"></a></h3>
<p>Eucalyptus is a software platform that implements
<a class="reference external" href="http://en.wikipedia.org/wiki/Cloud_computing#Infrastructure_as_a_Service_.28IaaS.29">IaaS-style</a>
cloud computing. Eucalyptus provides an Amazon Web Services
(<a class="reference external" href="http://aws.amazon.com/">AWS</a>) complaint EC2-based web service
interface for interacting with the Cloud service. Additionally,
Eucalyptus provides services such as the AWS Complaint Walrus and a user
interface for managing users and images. The aim of this tutorial is to
give users an introduction of how to interact with Eucalyptus using the
Eucalyptus EC2 Interface
(<a class="reference external" href="https://launchpad.net/euca2ools">Euca2ools</a>). More detailed
documentation can be found in the <a class="reference external" href="http://www.eucalyptus.com/eucalyptus-cloud/iaas">Eucalyptus
Website</a>. A detailed
user guide is also available
<a class="reference external" href="http://www.eucalyptus.com/sites/all/modules/pubdlcnt/pubdlcnt.php?file=/sites/all/files/docs/latest/ug.pdf&amp;nid=296">here</a>.</p>
</div>
<div class="section" id="requirements">
<h3><strong>Requirements</strong><a class="headerlink" href="#requirements" title="Permalink to this headline"></a></h3>
<p>Users get access to the Eucalyptus features using the Eucalyptus EC2
Interface, which is only available for GNU/Linux platforms. Therefore,
users will require a machine with a GNU/Linux installed on it.</p>
<p>As of May 15, 2012, FutureGrid is using <a class="reference external" href="http://www.eucalyptus.com/eucalyptus-cloud/iaas/features">Eucalyptus version
3</a> which
requires <a class="reference external" href="http://www.eucalyptus.com/download/euca2ools">euca2ools
2.0.2</a> and python &gt; 2.4
(available by modules in india and sierra). Make sure to load <strong>&#8220;module
load euca2ools/2.0.2&#8221;</strong> before using Eucalyptus. <a class="reference external" href="http://www.eucalyptus.com/download/euca2ools">euca2ools
2.0.2</a> is part of
Eucalyptus Enterprise package. However, the source is available
<a class="reference external" href="http://bazaar.launchpad.net/~eucalyptus-maintainers/euca2ools/euca2ools-main/revision/">here</a>.</p>
</div>
<div class="section" id="id40">
<h3><strong>Account Creation</strong><a class="headerlink" href="#id40" title="Permalink to this headline"></a></h3>
<p>Eucalyptus 3 is now integrated with our LDAP. Each account is created by
default for all FutureGrid users with valid portal account and project
affiliation. There is no need to apply for accounts.</p>
<p><strong>Obtaining Credentials</strong></p>
<ul class="simple">
<li>Credentials files are created by default and placed in a user&#8217;s home
directory under <em>.futuregrid</em> folder in india and sierra. To leverage
LDAP group, we created separate credentials for each FutureGrid
project. For instance, if you are part of FutureGrid project number
100 and 200, there will be two folders (<strong>fg100</strong> and <strong>fg200</strong>) in
<em>.futuregrid/eucalyptus</em>. Each project folder contains corresponding
credentials zip files. So fg100:username and fg200:username in
Eucalyptus, however, two different identities are tied to the same
LDAP user. A user can also download the credential file from the
Eucalyptus dashboard in
<a class="reference external" href="https://eucalyptus.india.futuregrid.org:8443">india</a> and
<a class="reference external" href="https://eucalyptus.sierra.futuregrid.org:8443">sierra</a>. To log in
to the dashboard, use your FutureGrid project number (in the format
of <em>fgnumber)</em>as &#8220;Account&#8221; and your portal user name as &#8220;User&#8221;.
Then use the FutureGrid portal password. (If you download the file
from the dashboard, by default the system will name the file
<em>euca2-{username}-x509</em>). We are using a slightly different naming
scheme. The credential file will be found under the menu tab
<em>username&#64;fgnumber&nbsp;</em>.)</li>
</ul>
<p>NOTICE: Due to an incompatibility problem introduced during the
upgrade to the Eucalyptus 3.1 system, accessing the dashboard of
Eucalyptus on india is tempararily unavailable. However, your credential
will be in place so you can execute the later steps of this manual.
On Sierra, the access is still available so you could download the
credential zip file. Make sure to put it to Sierra first and then set up
the environment as the following steps.
Please be reminded that the credential for one cluster should/could
not be used in another.</p>
<p><img alt="image88" src="https://portal.futuregrid.org/sites/default/files/resize/euca_fg_login-290x240.png" /></p>
<ul>
<li><p class="first">Find your credential zip file in cd
$HOME/.futuregrid/eucalyptus/fgprojectnumber :</p>
<blockquote>
<div><div class="highlight-python"><pre>$ unzip euca3-{username}-{cluster}-fgprojectnumber.zip</pre>
</div>
</div></blockquote>
</li>
<li><p class="first">Apply the eucarc file:</p>
<div class="highlight-python"><pre>source eucarc</pre>
</div>
</li>
<li><p class="first">If you want to add Eucalyptus environment variables to your .bashrc
then, do this: (if you are planning to switch between different cloud
platforms, it is probably better to use source.</p>
<blockquote>
<div><div class="highlight-python"><pre>$ cat eucarc &gt;&gt; $HOME/.bashrc</pre>
</div>
<div class="highlight-python"><pre>$ source .bashrc</pre>
</div>
</div></blockquote>
</li>
</ul>
<p><strong>Install Euca2tools</strong></p>
<blockquote>
<div>Older versions of euca2ools are not compatible with Eucalyptus 3.
<a class="reference external" href="http://www.eucalyptus.com/download/euca2ools">euca2ools 2.0.2</a>
is part of the Enterprise project and so is not available for all
users. However, the source is available
<a class="reference external" href="http://bazaar.launchpad.net/~eucalyptus-maintainers/euca2ools/euca2ools-main/files">here</a>.</div></blockquote>
</div>
<div class="section" id="resources-overview">
<h3><strong>Resources Overview</strong><a class="headerlink" href="#resources-overview" title="Permalink to this headline"></a></h3>
<p>Eucalyptus is available to FutureGrid Users on the India and Sierra
clusters. As we will see later, when we instantiate a Virtual Machine
(VM), it is needed to select the type of VM Image that we are going to
use. In this sense, the information of the VM Image types available in
each cluster is summarized below:</p>
<div class="highlight-python"><pre>AVAILABILITYZONE euca3india 149.165.146.135 arn:euca:eucalyptus:euca3india:cluster:euca3indiaCC/
AVAILABILITYZONE |- vm types free / max cpu ram disk
AVAILABILITYZONE |- m1.small 0189 / 0192 1 512 5
AVAILABILITYZONE |- c1.medium 0170 / 0171 1 2048 7
AVAILABILITYZONE |- m1.large 0091 / 0094 2 5120 10
AVAILABILITYZONE |- m1.xlarge 0073 / 0075 2 6000 15
AVAILABILITYZONE |- c1.xlarge 0044 / 0047 4 9216 20</pre>
</div>
<div class="highlight-python"><pre>AVAILABILITYZONE euca3sierra 198.202.120.90 arn:euca:eucalyptus:euca3sierra:cluster:euca3sierraCC/
AVAILABILITYZONE |- vm types free / max cpu ram disk
AVAILABILITYZONE |- m1.small 0051 / 0056 1 256 4
AVAILABILITYZONE |- c1.medium 0037 / 0042 1 512 5
AVAILABILITYZONE |- m1.large 0012 / 0014 2 1024 10
AVAILABILITYZONE |- m1.xlarge 0012 / 0014 2 1024 12
AVAILABILITYZONE |- c1.xlarge 0009 / 0014 4 2048 15</pre>
</div>
</div>
<div class="section" id="testing-your-setup">
<h3><strong>Testing Your Setup</strong><a class="headerlink" href="#testing-your-setup" title="Permalink to this headline"></a></h3>
<p>Use euca-describe-availability-zones to test the setup:</p>
<blockquote>
<div><div class="highlight-python"><pre>ssh sierra.futuregrid.org
Last login: Fri May 11 06:39:02 2012 from 129-79-49-230.dhcp-bl.indiana.edu

Welcome to Sierra.FutureGrid.Org

torque/2.4.8 version 2.4.8 loaded
moab version 5.4.0 loaded

$ module load euca2ools/2.0.2
euca2ools version 2.0.2 loaded
$ euca-version
euca2ools 2.0.2
$ source .futuregrid/eucalyptus/fgprojectnumber/eucarc
$ euca-describe-availability-zones
AVAILABILITYZONE euca3sierra 198.202.120.90 arn:euca:eucalyptus:euca3sierra:cluster:euca3sierraCC/</pre>
</div>
</div></blockquote>
<p><strong>Available Images</strong></p>
<p>List the existing images using euca-describe-images:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ euca-describe-images

IMAGE emi-8E1C3B69 euca-centos-2012/euca-centos-2012.1.14-x86_64.img.manifest.xml available public
IMAGE emi-D21D3F6C euca3/ubuntu-natty.img.manifest.xml available public
IMAGE emi-1A413C95 centos/centos.5-3.x86-64.img.manifest.xml available public
IMAGE eki-9F293A6A kernel/vmlinuz-2.6.27.21-0.1-xen.manifest.xml available public
IMAGE emi-0FA13B83 inca/centos5inca3128800784.img.manifest.xml available public
IMAGE emi-A9D33917 ubuntu/ubuntu.9-04.x86-64.img.manifest.xml available public
IMAGE eri-D1513DBA ramdisk/initrd-2.6.27.21-0.1-xen.manifest.xml available public
IMAGE eki-919E3C9A kernel/vmlinuz-2.6.27.21-0.1-xen.manifest.xml available public
IMAGE eri-55FE3F76 ramdisk/initrd.img-2.6.32-5-amd64.manifest.xml available public
IMAGE eri-9DCC3A6B ramdisk/initrd-2.6.27.21-0.1-xen.manifest.xml available public</pre>
</div>
</div></blockquote>
</div>
<div class="section" id="image-deployment">
<h3><strong>Image Deployment</strong><a class="headerlink" href="#image-deployment" title="Permalink to this headline"></a></h3>
<ul>
<li><p class="first">Before deploying a VM, you need to create at least one key pair. This
key pair will be injected into the VM, allowing you to SSH into the
instance. This is done using the euca-add-keypair command:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ euca-add-keypair userkey   &gt;userkey.pem</pre>
</div>
</div></blockquote>
</li>
<li><p class="first">Fix the permissions on the generated private key:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ chmod 0600 userkey.pem</pre>
</div>
</div></blockquote>
</li>
<li><p class="first">Now you can start a VM using one of the pre-existing images. You need
the emi-id of the image you want to start. This was listed in the
output of euca-describe-images command that you saw earlier. Use the
euca-run-instances command to start the VM:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ euca-run-instances -k userkey -n 1   emi-0B951139 -t c1.medium

RESERVATION r-4E730969 archit   archit-default
INSTANCE i-4FC40839 emi-0B951139 0.0.0.0 0.0.0.0 pending userkey   2010-07-20T20:35:47.015Z   eki-78EF12D2eri-5BB61255</pre>
</div>
</div></blockquote>
</li>
<li><p class="first">The euca-describe-instances command can be used to check the status
of the request. The following image was assigned an ip address and is
starting up, as demonstrated by the &#8220;pending&#8221; status:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ euca-describe-instances

RESERVATION r-4E730969 archit   default
INSTANCE i-4FC40839 emi-0B951139 149.165.146.153 10.0.2.194 pending userkey 0
 m1.small 2010-07-20T20:35:47.015Z india eki-78EF12D2 eri-5BB61255</pre>
</div>
</div></blockquote>
</li>
<li><p class="first">Once started, the status will change to &#8220;running&#8221;:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ euca-describe-instances

RESERVATION r-4E730969 archit   default
INSTANCE i-4FC40839 emi-0B951139 149.165.146.153 10.0.2.194 running userkey 0
 m1.small 2010-07-20T20:35:47.015Z india eki-78EF12D2 eri-5BB61255</pre>
</div>
</div></blockquote>
</li>
<li><p class="first">If you need to delete a deployed VM, you can use the
euca-terminate-instances command:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ euca-terminate-instances i-4FC40839</pre>
</div>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="logging-into-the-vm">
<h3><strong>Logging Into the VM</strong><a class="headerlink" href="#logging-into-the-vm" title="Permalink to this headline"></a></h3>
<ul>
<li><p class="first">Create rules to allow access to the VM over ssh and to allow ping:</p>
<blockquote>
<div><p>$ euca-authorize -P tcp -p 22 -s 0.0.0.0/0&nbsp;&nbsp; default
$ euca-authorize -P icmp -t -1:-1 -s 0.0.0.0/0 default</p>
</div></blockquote>
</li>
<li><p class="first">The ssh private key that was generated earlier can now be used to log
in to the VM:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ssh -i userkey.pem root@149.165.146.153

-bash-3.2# uname -a

Linux localhost 2.6.27.21-0.1-xen #1 SMP   2009-03-31 14:50:44 +0200 x86_64 x86_64 x86_64 GNU/Linux</pre>
</div>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="vm-network-info">
<h3><strong>VM Network Info</strong><a class="headerlink" href="#vm-network-info" title="Permalink to this headline"></a></h3>
<ul>
<li><p class="first">The VM itself is visible from outside using the VM public IP. The
internal network will show the VM private IP address:</p>
<blockquote>
<div><div class="highlight-python"><pre>-bash-3.2# /sbin/ifconfig

eth0 Link encap:Ethernet HWaddr D0:0D:33:14:06:40
 inet addr:10.0.2.194 Bcast:10.0.2.255 Mask:255.255.255.192</pre>
</div>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="image-management">
<h3><strong>Image Management</strong><a class="headerlink" href="#image-management" title="Permalink to this headline"></a></h3>
<ul>
<li><p class="first">We will use the example ubuntu 10 image to test uploading images.
Download the gzipped tar ball:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ wgethttp://cloud-images.ubuntu.com/releases/precise/release/ubuntu-12.04-server-cloudimg-amd64.tar.gz</pre>
</div>
</div></blockquote>
</li>
<li><p class="first">Uncompress and untar the archive:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ tar zxf ubuntu-12.04-server-cloudimg-amd64.tar.gz</pre>
</div>
</div></blockquote>
</li>
<li><p class="first">Bundle the image with a kernel and a ramdisk using the
euca-bundle-image command. In this example, we will use the xen
kernel already registered. euca-describe-images returns the kernel
and ramdisk IDs that we need:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ euca-bundle-image -i   precise-server-cloudimg-amd64.img --kernel eki-78EF12D2 --ramdisk   eri-5BB61255

Checking image
Tarring image
Encrypting image
Splitting image...
Part:   precise-server-cloudimg-amd64.img.part.0
Part:   precise-server-cloudimg-amd64.img.part.1
Part:   precise-server-cloudimg-amd64.img.part.2
Part:   precise-server-cloudimg-amd64.img.part.3
Part:   precise-server-cloudimg-amd64.img.part.4
Part:   precise-server-cloudimg-amd64.img.part.5
Part:   precise-server-cloudimg-amd64.img.part.6
Part:   precise-server-cloudimg-amd64.img.part.7
Part:   precise-server-cloudimg-amd64.img.part.8
Part:   precise-server-cloudimg-amd64.img.part.9
Part:   precise-server-cloudimg-amd64.img.part.10
Part:   precise-server-cloudimg-amd64.img.part.11
Part:   precise-server-cloudimg-amd64.img.part.12
Part:   precise-server-cloudimg-amd64.img.part.13
Part:   precise-server-cloudimg-amd64.img.part.14
Part:   precise-server-cloudimg-amd64.img.part.15
Part:   precise-server-cloudimg-amd64.img.part.16
Generating manifest   /tmp/precise-server-cloudimg-amd64.img.manifest.xml</pre>
</div>
</div></blockquote>
</li>
<li><p class="first">Use the generated manifest file to upload the image to Walrus:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ euca-upload-bundle -b ubuntu-image-bucket   -m /tmp/precise-server-cloudimg-amd64.img.manifest.xml

Checking bucket:   ubuntu-image-bucket
Creating bucket:   ubuntu-image-bucket
Uploading manifest   file
Uploading part:   precise-server-cloudimg-amd64.img.part.0
Uploading part:   precise-server-cloudimg-amd64.img.part.1
Uploading part:   precise-server-cloudimg-amd64.img.part.2
Uploading part:   precise-server-cloudimg-amd64.img.part.3
Uploading part:   precise-server-cloudimg-amd64.img.part.4
Uploading part:   precise-server-cloudimg-amd64.img.part.5
Uploading part:   precise-server-cloudimg-amd64.img.part.6
Uploading part:   precise-server-cloudimg-amd64.img.part.7
Uploading part:   precise-server-cloudimg-amd64.img.part.8
Uploading part:   precise-server-cloudimg-amd64.img.part.9
Uploading part:   precise-server-cloudimg-amd64.img.part.10
Uploading part:   precise-server-cloudimg-amd64.img.part.11
Uploading part:   precise-server-cloudimg-amd64.img.part.12
Uploading part:   precise-server-cloudimg-amd64.img.part.13
Uploading part:   precise-server-cloudimg-amd64.img.part.14
Uploading part:   precise-server-cloudimg-amd64.img.part.15
Uploading part:   precise-server-cloudimg-amd64.img.part.16
Uploaded image as   ubuntu-image-bucket/precise-server-cloudimg-amd64.img.manifest.xml</pre>
</div>
</div></blockquote>
</li>
<li><p class="first">Register the uploaded image:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ euca-register   ubuntu-image-bucket/precise-server-cloudimg-amd64.img.manifest.xml

IMAGE emi-FFC3154F</pre>
</div>
</div></blockquote>
</li>
<li><p class="first">The returned image ID can now be used to start instances with
euca-run-instances as described earlier. euca-describe-images also
shows the new image now:</p>
<blockquote>
<div><div class="highlight-python"><pre>$ euca-describe-images

IMAGE emi-FFC3154F   ubuntu-image-bucket/precise-server-cloudimg-amd64.img.manifest.xml archit available public   x86_64 machine eri-5BB61255 eki-78EF12D2
IMAGE emi-0B951139   centos53/centos.5-3.x86-64.img.manifest.xml           admin  available public   x86_64 machine
  ...</pre>
</div>
</div></blockquote>
</li>
<li><p class="first">You can also delete your images:</p>
</li>
</ul>
<div class="highlight-python"><pre>$ euca-deregister emi-FFC3154F</pre>
</div>
</div>
<div class="section" id="status-of-deployments">
<h3>Status of Deployments<a class="headerlink" href="#status-of-deployments" title="Permalink to this headline"></a></h3>
<p>At times, you may ask if the Eucalyptus systems on FutureGrid are
operational. You can find this out by visiting the following:</p>
<p>a) The Outage page
at&nbsp;<a class="reference external" href="https://portal.futuregrid.org/metrics/html/results/realtime.html#total-count-of-running-vm-instances-updated-every-5-seconds">https://portal.futuregrid.org/metrics/html/results/realtime.html#total-count-of-running-vm-instances-updated-every-5-seconds</a>
b) The Real Time Status monitor
at&nbsp;<a class="reference external" href="http://inca.futuregrid.org:8080/inca/jsp/status.jsp?queryNames=Health&amp;xsl=table.xsl&amp;resourceIds=FutureGrid">http://inca.futuregrid.org:8080/inca/jsp/status.jsp?queryNames=Health&amp;xsl=table.xsl&amp;resourceIds=FutureGrid</a>
c) Our Runtime History
at&nbsp;<a class="reference external" href="http://inca.futuregrid.org:8080/inca/jsp/report.jsp?xml=cloudReport.xml">http://inca.futuregrid.org:8080/inca/jsp/report.jsp?xml=cloudReport.xml</a></p>
</div>
</div>
<div class="section" id="using-openstack-on-futuregrid">
<h2>Using OpenStack on FutureGrid<a class="headerlink" href="#using-openstack-on-futuregrid" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="note">
<h2>NOTE:<a class="headerlink" href="#note" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><strong>This tutorial contains material that is customized for the
particular user. The best way to view this tutorial is to &nbsp;ask for a
FutureGrid portal account and to log into the portal. You will also
need to be in a valid FG project to execute the directions.</strong></li>
</ul>
</div>
<div class="section" id="id41">
<h2>Summary<a class="headerlink" href="#id41" title="Permalink to this headline"></a></h2>
<p>We currently have twenty nodes in the FG&nbsp;India cluster dedicated to the
<a class="reference external" href="http://www.openstack.org/software/essex/">Essex</a> release of
<a class="reference external" href="http://www.openstack.org">OpenStack</a>, a collection of open-source
technology that provides scalable, open-source cloud computing software.
OpenStack consists of a series of <a class="reference external" href="http://www.openstack.org/software/">interrelated
projects</a> that deliver various
components for a cloud infrastructure solution. This tutorial provides
an overview of OpenStack Nova installation on FutureGrid, as well as
steps for deploying virtual machines.</p>
</div>
<div class="section" id="id42">
<h2>Getting Started<a class="headerlink" href="#id42" title="Permalink to this headline"></a></h2>
<p>Be sure you have a valid portal account
(<a class="reference external" href="https://portal.futuregrid.org">https://portal.futuregrid.org)</a> and
are part of a valid FG project. If you&#8217;re not part of any project, you
can either create a new one (it will need approval) or join an existing
one with the permission of the Lead.</p>
<p>Once the previous requirements are fulfilled, your OpenStack account is
automatically created.</p>
</div>
<div class="section" id="log-into-india">
<h2>Log into India<a class="headerlink" href="#log-into-india" title="Permalink to this headline"></a></h2>
<table border="1" class="docutils">
<colgroup>
</colgroup>
<tbody valign="top">
</tbody>
</table>
<p>&nbsp;Tip: to login into FutureGrid&nbsp;<a class="reference external" href="https://portal.futuregrid.org/accessing-futuregrid-resources-ssh">see this page</a>   |
+&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-+</p>
<p>The first step is to ssh into india.futuregrid.org using your FG
username:</p>
<div class="highlight-python"><pre>$ ssh gvonlasz@india.futuregrid.org</pre>
</div>
<p>If this command fails,
contact&nbsp;<a class="reference external" href="https://portal.futuregrid.org/help">https://portal.futuregrid.org/help</a>.
It likely means one of the following:</p>
<ol class="arabic simple">
<li>Your account is not yet set up.</li>
<li>You provided no public key, or a corrupted public key.</li>
<li>The private key you are using does not match the public key you
registered with FutureGrid.</li>
</ol>
</div>
<div class="section" id="account-and-credentials">
<h2>Account and Credentials<a class="headerlink" href="#account-and-credentials" title="Permalink to this headline"></a></h2>
<p>As noted above, OpenStack credentials and configuration files are
automatically created for all valid users, and are placed in your home
directory on the India system under the <strong>*.futuregrid/openstack*</strong>
folder. The file name is called
<strong>*openstack-essex-&lt;username&gt;-india.zip*</strong>.&nbsp;The credential&nbsp;.zip&nbsp;fi
le
contains the user keys and novarc
file.&nbsp;The novarc
file contains the
necessary environment variables.</p>
<p>Unzip the credential file:</p>
<div class="highlight-python"><pre>$ unzip ~/.futuregrid/openstack/openstack-essex-gvonlasz-india.zip -d ~/openstack</pre>
</div>
<p>Load Environment variables:</p>
<div class="highlight-python"><pre>$ source ~/openstack/novarc</pre>
</div>
<p><strong>Note:</strong> As both Eucalyptus and OpenStack use the same EC2
environmental variables (i.e. EC2_URL, EC2_ACCESS), using novarc will
overwrite previous Eucalyptus credentials. If you have the eucarc
file
in your home directory, doing a <em>source eucarc</em> will bring back those
variables and overwrite the OpenStack ones. In this way, you can go back
and forth with OpenStack and Eucalyptus.</p>
</div>
<div class="section" id="euca2ools-ec2-client-tools">
<h2>Euca2ools (EC2 client tools)<a class="headerlink" href="#euca2ools-ec2-client-tools" title="Permalink to this headline"></a></h2>
<p>OpenStack services can be controlled using an EC2 interface (only
available for GNU/Linux platforms). In this tutorial, we are going to
use the&nbsp;<a class="reference external" href="http://www.eucalyptus.com/download/euca2ools">Euca2ools</a>
client installed in India.</p>
<div class="highlight-python"><pre>$ module load euca2ools</pre>
</div>
</div>
<div class="section" id="id43">
<h2>Testing Your Setup<a class="headerlink" href="#id43" title="Permalink to this headline"></a></h2>
<p>Use euca-describe-availability-zones to test the setup:</p>
<div class="highlight-python"><pre>$ euca-describe-availability-zones

AVAILABILITYZONE    india   available</pre>
</div>
</div>
<div class="section" id="list-of-common-images">
<h2>List of Common Images<a class="headerlink" href="#list-of-common-images" title="Permalink to this headline"></a></h2>
<p>Following are the current images uploaded in essex:</p>
<div class="highlight-python"><pre>$ euca-describe-images |grep common

IMAGE ami-000000b4 common/precise-server-cloudimg-amd64.img.manifest.xml available private x86_64 machine aki-000000b3 instance-store</pre>
</div>
</div>
<div class="section" id="vm-types">
<h2>VM Types<a class="headerlink" href="#vm-types" title="Permalink to this headline"></a></h2>
<p>The following types of VM are available:</p>
<p>$ nova flavor-list</p>
<div class="line-block">
<div class="line">+&#8212;-+&#8212;&#8212;&#8212;&#8212;-+&#8212;&#8212;&#8212;&#8211;+&#8212;&#8212;+&#8212;&#8212;&#8212;&#8211;+&#8212;&#8212;+&#8212;&#8212;-+&#8212;&#8212;&#8212;&#8212;-+&#8212;&#8212;&#8212;&#8211;+&#8212;&#8212;&#8212;&#8212;-+</div>
<div class="line-block">
<div class="line">| ID | Name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | Memory_MB | Disk | Ephemeral | Swap |</div>
</div>
</div>
<p>VCPUs | RXTX_Factor | Is_Public | extra_specs |
|
+&#8212;-+&#8212;&#8212;&#8212;&#8212;-+&#8212;&#8212;&#8212;&#8211;+&#8212;&#8212;+&#8212;&#8212;&#8212;&#8211;+&#8212;&#8212;+&#8212;&#8212;-+&#8212;&#8212;&#8212;&#8212;-+&#8212;&#8212;&#8212;&#8211;+&#8212;&#8212;&#8212;&#8212;-+
|  | 1&nbsp; | m1.tiny&nbsp;&nbsp;&nbsp;&nbsp; | 512&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 0&nbsp;&nbsp;&nbsp; | 0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
1&nbsp;&nbsp;&nbsp;&nbsp; | 1.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | N/A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | {}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
|  | 2&nbsp; | m1.small&nbsp;&nbsp;&nbsp; | 2048&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 10&nbsp;&nbsp; | 20&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
1&nbsp;&nbsp;&nbsp;&nbsp; | 1.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | N/A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | {}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
|  | 3&nbsp; | m1.medium&nbsp;&nbsp; | 4096&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 10&nbsp;&nbsp; | 40&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
2&nbsp;&nbsp;&nbsp;&nbsp; | 1.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | N/A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | {}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
|  | 4&nbsp; | m1.large&nbsp;&nbsp;&nbsp; | 8192&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 10&nbsp;&nbsp; | 80&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
4&nbsp;&nbsp;&nbsp;&nbsp; | 1.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | N/A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | {}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
|  | 5&nbsp; | m1.xlarge&nbsp;&nbsp; | 16384&nbsp;&nbsp;&nbsp;&nbsp; | 10&nbsp;&nbsp; | 160&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
8&nbsp;&nbsp;&nbsp;&nbsp; | 1.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | N/A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | {}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
|  | 6&nbsp; | m1.special1 | 16384&nbsp;&nbsp;&nbsp;&nbsp; | 50&nbsp;&nbsp; | 1500&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
8&nbsp;&nbsp;&nbsp;&nbsp; | 1.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | N/A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | {}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |
|
+&#8212;-+&#8212;&#8212;&#8212;&#8212;-+&#8212;&#8212;&#8212;&#8211;+&#8212;&#8212;+&#8212;&#8212;&#8212;&#8211;+&#8212;&#8212;+&#8212;&#8212;-+&#8212;&#8212;&#8212;&#8212;-+&#8212;&#8212;&#8212;&#8211;+&#8212;&#8212;&#8212;&#8212;-+</p>
</div>
<div class="section" id="id44">
<h2>Key Management<a class="headerlink" href="#id44" title="Permalink to this headline"></a></h2>
<p>Before you instantiate a VM, you need to create at least one key pair.
This key pair will be injected into the VM, allowing you to SSH into the
instance. This is done using the <em>euca-add-keypair</em> command:</p>
<div class="highlight-python"><pre>$ euca-add-keypair gvonlaszkey1 &gt; gvonlaszkey1.pem</pre>
</div>
<p><strong>Important Tip:&nbsp;</strong>In case you execute this command twice this will
naturally not work in case the key is already added and imported. You
can check this by doing a cat on the .pem file and make sure that there
is a key in ther and not an error! If there is an error, you can either
delete the key and add it again, or you can create a new key name and
use that. Also if you have used that key before and you overwrote it you
will not have access to your images anymore. so be careful and before
issuing the above command think about what you do.</p>
<p>Fix the permissions on the generated private key:</p>
<div class="highlight-python"><pre>$ chmod 0600 gvonlaszkey1.pem</pre>
</div>
<p>Note: Instead of creating a new keypair, you can import a public key
created with a third-party tool using <em>euca-import-keypair</em>. For
instance, if you have your ssh public key in india, you can do the
following:</p>
<div class="highlight-python"><pre>$ euca-import-keypair -f .ssh/id_rsa.pub gvonlaszkey2</pre>
</div>
<p>You can add multiple keys, and here&#8217;s the command to check the list of
your keys:</p>
<div class="highlight-python"><pre>$ chmod 0600 keyname.pem
euca-describe-keypairs
KEYPAIR key1  53:e3:01:c1:70:df:94:ef:59:93:1a:3f:c0:10:a5:34
KEYPAIR key2  07:a5:da:30:b4:55:16:eb:35:54:a2:5a:56:68:f6:cb</pre>
</div>
<div class="section" id="key-pair-verification">
<h3>Key pair verification<a class="headerlink" href="#key-pair-verification" title="Permalink to this headline"></a></h3>
<p>Please verify that you do not have any error in the key file.</p>
</div>
</div>
<div class="section" id="image-instantiation">
<h2>Image Instantiation<a class="headerlink" href="#image-instantiation" title="Permalink to this headline"></a></h2>
<p>At this point, you can start a VM using one of the pre-existing images.
You need the ami-id of the image you want to start. This was listed in
the output of the <em>euca-describe-images</em> command you saw earlier. Use
the <em>euca-run-instances</em> command to start the VM (use the key name you
specified before).</p>
<div class="highlight-python"><pre>$ euca-run-instances -k gvonlaszkey -n 1 ami-000000b4

RESERVATION r-gbs9hpmm 461884eef90047fbb4eb9ec92f22a1e3 default
INSTANCE i-00000a27 ami-000000b4 server-2599 server-2599 pending gvonlaszkey 0 m1.small 2012-07-31T14:54:40.000Z unknown zone</pre>
</div>
<p>The output shows the id of your VM, which in this case is i-00000a27.</p>
<p>This id will be useful to do operations with your VM. You can also see
the status of your VM, which is pending now. You need to wait until the
VM is in running status to be able to log into the VM.</p>
</div>
<div class="section" id="better-server-names">
<h2>Better Server Names<a class="headerlink" href="#better-server-names" title="Permalink to this headline"></a></h2>
<p>Unfortunately, the default use of the euca commands uses the name
server-&lt;number&gt; to identify a started instance. This is often not
desirable, as many users have similar names and it will be difficult to
find your own images when lots of users start images. To fix this, you
van however use the nova commands and say</p>
<p>$ nova rename Server-2599 &lt;yourusername&gt;-001</p>
</div>
<div class="section" id="monitoring-instances">
<h2>Monitoring Instances<a class="headerlink" href="#monitoring-instances" title="Permalink to this headline"></a></h2>
<p>You can monitor the status of the instances by using the
<em>euca-describe-instances</em> command. The public IP is highligthed in
yellow; each VM should have one:</p>
<div class="highlight-python"><pre>$ euca-describe-instances</pre>
</div>
<div class="highlight-python"><pre>RESERVATION r-xfj0nag8 461884eef90047fbb4eb9ec92f22a1e3 default
INSTANCE i-0000090e ami-00000016 149.165.158.157 server-2318 running clegoues 0 m1.medium 2012-07-24T19:39:21.000Z india aki-00000014 ari-00000015

RESERVATION r-8mwsq0n0 461884eef90047fbb4eb9ec92f22a1e3 default
INSTANCE i-000008f2 ami-000000b4 149.165.158.130 server-2290 running clegoues 0 m1.medium 2012-07-24T02:19:38.000Z india aki-00000014 ari-00000015
INSTANCE i-000008f3 ami-000000b4 149.165.158.149 server-2291 running clegoues 1 m1.medium 2012-07-24T02:19:38.000Z india aki-00000014 ari-00000015
INSTANCE i-000008f4 ami-000000b4 149.165.158.156 server-2292 running clegoues 2 m1.medium 2012-07-24T02:19:39.000Z india aki-00000014 ari-00000015

RESERVATION r-p90m3pno 461884eef90047fbb4eb9ec92f22a1e3 default
INSTANCE i-000007e2 ami-000000b4 149.165.158.158 server-2018 running jiaazeng 0 m1.medium 2012-07-15T20:56:16.000Z india aki-00000026
INSTANCE i-000007e3 ami-000000b4 149.165.158.159 server-2019 running jiaazeng 1 m1.medium 2012-07-15T20:56:17.000Z india aki-00000026
...</pre>
</div>
<p>You can monitor or restrict the output simply by checking the status of
your VM:</p>
<div class="highlight-python"><pre>$ euca-describe-instances i-00000a27

RESERVATION r-zvtbbj8j default
INSTANCE i-00000a27 ami-000000b4 server-1854 server-1854 pending gvonlaszkey 0 m1.small 2012-07-09T15:49:46.000Z  unknown zone aki-0000000e ari-0000000f</pre>
</div>
<p>This VM does not have public IP yet. Getting the public IP may take some
time, but it is needed to be able to connect to the VM:</p>
<div class="highlight-python"><pre>$ euca-describe-instances i-00000a27

RESERVATION r-zvtbbj8j default
INSTANCE i-00000a27 ami-000000b4 149.165.158.175 server-1854 running gvonlaszkey 0 m1.small 2012-07-09T15:49:46.000Z unknown zone aki-0000000e ari-0000000f</pre>
</div>
</div>
<div class="section" id="log-into-your-vm">
<h2>Log into your VM<a class="headerlink" href="#log-into-your-vm" title="Permalink to this headline"></a></h2>
<p>The ssh private key that was generated earlier can now be used to log in
to the VM. You also need to indicate the public IP associated with your
VM (use the key name you have specified before).</p>
<div class="highlight-python"><pre>$ ssh -i gvonlaszkey.pem ubuntu@149.165.158.175

$ ssh ubuntu@149.165.158.175 (for imported keys)</pre>
</div>
<p>Note: For some ubuntu images, log in with the user <em>ubuntu,</em> then
<em>sudo</em>.</p>
<p>Exit from the VM to continue with the tutorial.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># exit</span>
</pre></div>
</div>
<div class="section" id="making-a-snapshot-with-nova-client">
<h3>Making a snapshot with nova client<a class="headerlink" href="#making-a-snapshot-with-nova-client" title="Permalink to this headline"></a></h3>
<p>You can make a snapshot of your instance:</p>
<div class="highlight-python"><pre>$ nova image-create &lt;instance name&gt; &lt;snapshot name&gt;
$ euca-describe-images</pre>
</div>
<p>Your snapshot will be listed at the end of the output, and it will be
available in 5 to 10 minutes. There&#8217;s a bug that snapshots are created
as &#8220;snapshot&#8221; whatever you name it. So please remember the image ID.</p>
</div>
</div>
<div class="section" id="nova-volumes-not-available">
<h2>Nova Volumes (Not available)<a class="headerlink" href="#nova-volumes-not-available" title="Permalink to this headline"></a></h2>
<p>Nova-volume provides persistent block storage compatible with Amazons
Elastic Block Store.&nbsp;The storage in the instances is non-persistent and
gets lost when the instance is terminated. Therefore, we need persistent
volumes to keep data generated during instance lifetime after the
instance is terminated. Volumes are accessed via iSCSI, although they
will appear as a new device in your VM.</p>
<div class="section" id="list-available-volumes">
<h3>List available Volumes<a class="headerlink" href="#list-available-volumes" title="Permalink to this headline"></a></h3>
<p>You can see the available volumes by using the <em>euca-describe-volumes</em>
command:</p>
<div class="highlight-python"><pre>$  euca-describe-volumes

VOLUME  vol-00000027  100  india  in-use  2012-06-06T21:39:47.000Z
           ATTACHMENT  vol-00000027  i-0000070f  /dev/vdc  attached
VOLUME  vol-00000028  50  india  available  2012-06-06T21:44:30.000Z
VOLUME  vol-0000002a  30  india  available  2012-06-06T21:45:37.000Z</pre>
</div>
</div>
<div class="section" id="create-a-volume">
<h3>Create a Volume<a class="headerlink" href="#create-a-volume" title="Permalink to this headline"></a></h3>
<p>Create a 1 GB volume in the India zone:</p>
<div class="highlight-python"><pre>$  euca-create-volume -s 1 -z india

VOLUME vol-00000031 1 india creating 2012-07-10T15:15:47.244Z</pre>
</div>
</div>
<div class="section" id="attach-volume">
<h3>Attach Volume<a class="headerlink" href="#attach-volume" title="Permalink to this headline"></a></h3>
<p>A volume can only be attached to one instance. Once the volume is
attached to a VM, <em>euca-describe-volumes</em> will show its status as
attached.</p>
<p>Attach a volume to a running instance:</p>
<div class="highlight-python"><pre>$  euca-attach-volume -i i-00000a27 -d /dev/vdc vol-0000031</pre>
</div>
<p>After this command is executed, an additional SCSI disk is created in
the instance.&nbsp;Although we specified the device, it may differ if that
device already exists (look into <em>/dev</em> or <em>/var/log/syslog</em> to find the
new device).</p>
</div>
<div class="section" id="using-the-new-disk">
<h3>Using the new Disk<a class="headerlink" href="#using-the-new-disk" title="Permalink to this headline"></a></h3>
<p>Log into the VM again (use the key name you specified before):</p>
<div class="highlight-python"><pre>$ ssh -i gvonlaszkey.pem ubuntu@149.165.158.175</pre>
</div>
<p>Format the disk&nbsp;(skip this step if you want to reuse data stored):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># mkfs /dev/vdc</span>
</pre></div>
</div>
<p>Mount the disk:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># mount /dev/vdc /mnt</span>
</pre></div>
</div>
<p>You now have the new disk mounted in your system. In this way, you can
use it as a normal directory to store information. However, the
information stored there will be kept after you terminate the VM.</p>
<p>Exit from the VM to continue with the tutorial:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># exit</span>
</pre></div>
</div>
</div>
<div class="section" id="detach-volumes">
<h3>Detach Volumes<a class="headerlink" href="#detach-volumes" title="Permalink to this headline"></a></h3>
<p>Volumes are automatically detached when the instance is terminated.</p>
<p>To detach a volume:</p>
<div class="highlight-python"><pre>$  euca-detach-volume vol-00000031</pre>
</div>
<p>If you detach the volume while the instance is running, and with disk
mounted, it loses access to the disk. Thus, you need to make sure that
you umount (umount /mnt) the disk before you detach the volume.&nbsp;If you
terminate the instance, the volume is automatically detached.</p>
</div>
</div>
<div class="section" id="volume-snapshots">
<h2>Volume Snapshots<a class="headerlink" href="#volume-snapshots" title="Permalink to this headline"></a></h2>
<p>Snapshots are useful to create backups or replicate volumes in different
zones.</p>
<div class="section" id="create-snapshot">
<h3>Create Snapshot<a class="headerlink" href="#create-snapshot" title="Permalink to this headline"></a></h3>
<div class="highlight-python"><pre>$  euca-create-snapshot -d 'Testing snapshot' vol-00000027

SNAPSHOT    snap-00000001    vol-00000027    creating    2012-07-16T14:22:21.728Z    0%    Testing snapshot</pre>
</div>
</div>
<div class="section" id="list-snapshot">
<h3>List Snapshot<a class="headerlink" href="#list-snapshot" title="Permalink to this headline"></a></h3>
<div class="highlight-python"><pre>$  euca-describe-snapshots

SNAPSHOT    snap-00000001    vol-00000027    available    2012-07-16T14:22:21.000Z    100%</pre>
</div>
</div>
<div class="section" id="create-volume-from-snapshot-not-yet-functional-in-openstack-essex">
<h3>Create Volume from Snapshot (not yet functional in OpenStack Essex)<a class="headerlink" href="#create-volume-from-snapshot-not-yet-functional-in-openstack-essex" title="Permalink to this headline"></a></h3>
<p>The snapshot must be in available status (100% completed). The new
volume can be bigger if desired, and you can also create this volume in
a different zone (<em>-z</em> option).</p>
<p>To create a 2 GB volume from snapshot:</p>
<div class="highlight-python"><pre>$  euca-create-volume -s 2 --snapshot snap-00000001 -z india

VOLUME    vol-00000032    2    snap-00000001    india    creating    2012-07-16T14:47:07.916Z</pre>
</div>
</div>
</div>
<div class="section" id="image-registration">
<h2>Image Registration<a class="headerlink" href="#image-registration" title="Permalink to this headline"></a></h2>
<p>We will use an CentOS 5 image to test the image registration:</p>
<div class="highlight-python"><pre>$ wget i120/test-image/centos5.tgz</pre>
</div>
<p>Uncompress and untar the archive:</p>
<div class="highlight-python"><pre>$ tar xvfz centos5.tgz</pre>
</div>
<p>Bundle the image with a kernel and a ramdisk using the
<em>euca-bundle-image</em> command. In this example, we will use the KVM kernel
already registered. <em>euca-describe-images</em> returns the kernel and
ramdisk IDs that we need.</p>
<div class="highlight-python"><pre>$ euca-bundle-image -i centos5.img --kernel aki-0000000e --ramdisk ari-0000000f</pre>
</div>
<div class="highlight-python"><pre>Checking image
Encrypting image
Splitting image...
Part: centos5.img.part.00
...
Part: centos5.img.part.35
Generating manifest /tmp/centos5.img.manifest.xml</pre>
</div>
<p>Use the generated manifest file to upload the image. You need to specify
a bucket name; it can be anything you want:</p>
<div class="highlight-python"><pre>$ euca-upload-bundle -b gvonlasz-bucket -m /tmp/centos5.img.manifest.xml

Checking bucket: gvonlasz-bucket
Creating bucket: gvonlasz-bucket
Uploading manifest file
Uploading part: centos5.img.part.00
...
Uploading part: centos5.img.part.35

Uploaded image as gvonlasz-bucket/centos5.img.manifest.xml</pre>
</div>
<p>Register the upload image:</p>
<div class="highlight-python"><pre>$ euca-register gvonlasz-bucket/centos5.img.manifest.xml

IMAGE  ami-00000033</pre>
</div>
<p>The returned image ID can now be used to start instances
with<em>euca-run-instances</em> as described earlier. However, you cannot run
instances until the image is in available status. You can check the
status using&nbsp;<em>euca-describe-images</em>:</p>
<div class="highlight-python"><pre>$ euca-describe-instances ami00000033 IMAGE ami-00000033 gvonlasz-bucket/centos5.img.manifest.xml available private x86_64 machine aki-0000000e ari-0000000f instance-store</pre>
</div>
</div>
<div class="section" id="delete-your-images">
<h2>Delete your images<a class="headerlink" href="#delete-your-images" title="Permalink to this headline"></a></h2>
<div class="highlight-python"><pre>$ euca-deregister ami-00000033</pre>
</div>
</div>
<div class="section" id="terminate-your-vms">
<h2>Terminate your VMs<a class="headerlink" href="#terminate-your-vms" title="Permalink to this headline"></a></h2>
<div class="highlight-python"><pre>$ euca-terminate-instances i-00000a27</pre>
</div>
</div>
<div class="section" id="limitations">
<h2>Limitations<a class="headerlink" href="#limitations" title="Permalink to this headline"></a></h2>
<p>Our current installation has the following limitations. We are working
on finding a fix:</p>
<ol class="arabic">
<li><p class="first">Instances cannot ping their own IP address from within the instance
(it is pingable and reachable from the outside). The private IP
(which can be found via ifconfig) is pingable.</p>
<dl class="docutils">
<dt><a class="reference external" href="mailto:ubuntu&#37;&#52;&#48;server-837">ubuntu<span>&#64;</span>server-837</a>:~$ ifconfig</dt>
<dd><p class="first last">eth0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Ethernet&nbsp; HWaddr fa:16:3e:00:e5:2a
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; inet addr:10.1.2.16&nbsp; Bcast:10.1.2.255</p>
</dd>
<dt>Mask:255.255.255.0</dt>
<dd><p class="first last">inet6 addr: fe80::f816:3eff:fe00:e52a/64 Scope:Link
UP BROADCAST RUNNING MULTICAST&nbsp; MTU:1500&nbsp; Metric:1
RX packets:199 errors:0 dropped:0 overruns:0 frame:0
TX packets:188 errors:0 dropped:0 overruns:0 carrier:0
collisions:0 txqueuelen:1000
RX bytes:32213 (32.2 KB)&nbsp; TX bytes:21676 (21.6 KB)</p>
</dd>
<dt>lo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Link encap:Local Loopback</dt>
<dd><p class="first last">inet addr:127.0.0.1&nbsp; Mask:255.0.0.0
inet6 addr: ::1/128 Scope:Host
UP LOOPBACK RUNNING&nbsp; MTU:16436&nbsp; Metric:1
RX packets:0 errors:0 dropped:0 overruns:0 frame:0
TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
collisions:0 txqueuelen:0
RX bytes:0 (0.0 B)&nbsp; TX bytes:0 (0.0 B)</p>
</dd>
</dl>
</li>
<li><p class="first">In Essex, each instance gets an internal DNS name.
<em>euca-describe-instance</em> will show this name along with the public IP
addresses. If you are building a cluster and need to communicate
among all the nodes in the cluster, you will need to do so with these
names or the private IP addresses:</p>
<dl class="docutils">
<dt><a class="reference external" href="mailto:ubuntu&#37;&#52;&#48;server-837">ubuntu<span>&#64;</span>server-837</a>:~$ ping server-716</dt>
<dd><p class="first last">PING server-716.novalocal (10.1.2.10) 56(84) bytes of data.
64 bytes from server-716.novalocal (10.1.2.10): icmp_req=1 ttl=64</p>
</dd>
<dt>time=5.06 ms</dt>
<dd><p class="first last">^C
&#8212; server-716.novalocal ping statistics &#8212;
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 5.062/5.062/5.062/0.000 ms
<a class="reference external" href="mailto:ubuntu&#37;&#52;&#48;server-837">ubuntu<span>&#64;</span>server-837</a>:~$ ping server-837
PING server-837.novalocal (127.0.1.1) 56(84) bytes of data.
64 bytes from server-837.novalocal (127.0.1.1): icmp_req=1 ttl=64</p>
</dd>
<dt>time=0.032 ms</dt>
<dd><p class="first last">64 bytes from server-837.novalocal (127.0.1.1): icmp_req=2 ttl=64</p>
</dd>
</dl>
<p>time=0.018 ms</p>
</li>
</ol>
</div>
<div class="section" id="troubleshooting">
<h2>Troubleshooting:<a class="headerlink" href="#troubleshooting" title="Permalink to this headline"></a></h2>
<p>Console output will provide you more details about the VM:</p>
<div class="highlight-python"><pre>$ euca-get-console-output &lt;instanceId&gt;</pre>
</div>
<p>For any other issue, please submit a ticket.</p>
<p>style=&#8221;font-size: small;&#8221;&gt;</p>
</div>
<div class="section" id="image-management-and-rain-on-futuregrid">
<h2>Image Management and Rain on FutureGrid<a class="headerlink" href="#image-management-and-rain-on-futuregrid" title="Permalink to this headline"></a></h2>
<p><a class="reference external" href="https://portal.futuregrid.org/kb/document/ayzv">FutureGrid</a> image
management defines the full lifecycle of the images in FutureGrid. It
involves the process of creating, customizing, storing, sharing, and
deploying images for different FutureGrid environments. For information
and instructions on managing your images in FutureGrid, and the Rain
environment, see</p>
<ul class="simple">
<li><a class="reference external" href="http://futuregrid.github.com/rain/quickstart.html">http://futuregrid.github.com/rain</a>&nbsp;and</li>
<li><a class="reference external" href="http://futuregrid.github.com/rain/quickstart.html">http://futuregrid.github.com/rain/quickstart.html</a></li>
</ul>
<table border="1" class="docutils">
<colgroup>
<col width="51%" />
<col width="49%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><a class="reference external" href="http://futuregrid.github.com/rain/quickstart.html">**RAIN Documentation:**</a></th>
<th class="head"><a class="reference external" href="http://futuregrid.github.com/rain/quickstart.html">**RAIN Quickstart:**</a></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference external" href="http://futuregrid.github.com/rain/"><img alt="image91" src="https://portal.futuregrid.org/sites/default/files/resize/u23/Screen%20Shot%202013-03-06%20at%2012.47.32%20PM-201x200.png" /></a></td>
<td><a class="reference external" href="http://futuregrid.github.com/rain/quickstart.html"><img alt="image92" src="https://portal.futuregrid.org/sites/default/files/resize/u23/Screen%20Shot%202013-03-06%20at%2012.51.48%20PM-200x200.png" /></a></td>
</tr>
</tbody>
</table>
<p><a class="reference external" href="http://futuregrid.github.com/rain/quickstart.html">&nbsp;</a></p>
</div>
<div class="section" id="generate-and-register-an-os-image-on-futuregrid-using-the-fg-shell">
<h2>Generate and Register an OS Image on FutureGrid using the FG Shell<a class="headerlink" href="#generate-and-register-an-os-image-on-futuregrid-using-the-fg-shell" title="Permalink to this headline"></a></h2>
<div class="section" id="id45">
<h3>Summary<a class="headerlink" href="#id45" title="Permalink to this headline"></a></h3>
<p>Below, we summarize the different steps needed to create a new image
and register it in a FutureGrid infrastructure. In this case, we will
register the image in OpenStack, but it can be registered in any other
FutureGrid infrastructure, such as Eucalyptus, Nimbus, or HPC. Detailed
information about these tools can be found
in&nbsp;<a class="reference external" href="http://futuregrid.github.com/rain/">http://futuregrid.github.com/rain/</a>.</p>
</div>
<div class="section" id="request-access">
<h3>Request access<a class="headerlink" href="#request-access" title="Permalink to this headline"></a></h3>
<p>Submit a ticket to request
access&nbsp;<a class="reference external" href="https://portal.futuregrid.org/help">https://portal.futuregrid.org/help</a></p>
<p>Please use the subject: &#8220;Request to access FutureGrid Rain&#8221;. Include
in the body &#8220;I would like to obtain access to FutureGrid Rain&#8221;.</p>
<p><em>Note: In future we may just create a checkbox for this request in the
help form.</em></p>
<p>Once you have access, you can continue the tutorial. Typically, it will
take a business day for you to get added (a business day is 9am-5pm EST
Mon-Fri).</p>
</div>
<div class="section" id="id46">
<h3>Log into India<a class="headerlink" href="#id46" title="Permalink to this headline"></a></h3>
<div class="highlight-python"><pre>$ ssh &lt;username&gt;@india.futuregrid.org
$ module load futuregrid</pre>
</div>
</div>
<div class="section" id="start-the-shell">
<h3>Start the Shell<a class="headerlink" href="#start-the-shell" title="Permalink to this headline"></a></h3>
<p>Execute the FG Shell. Please remember to replace &lt;username&gt; with your
portal user name. Then, you will be asked for a password (which is your
portal password):</p>
<div class="highlight-python"><pre>$ fg-shell -u &lt;username&gt;</pre>
</div>
<p>This command will change your shell and the prompt will be fg-rain&gt;. If
your prompt is different, you need to execute &#8220;use rain&#8221;.</p>
</div>
<div class="section" id="generate-the-image">
<h3>Generate the Image<a class="headerlink" href="#generate-the-image" title="Permalink to this headline"></a></h3>
<p>We are going to generate an Ubuntu 12.04 image. The -s parameter
allows you to specify the software you want to install in your image.
Currently, only the software available in the official repositories can
be installed.</p>
<div class="highlight-python"><pre>fg-rain&gt; generate -o ubuntu -v 12.04 -a x86_64 -s wget, openmpi-bin</pre>
</div>
<p>After a while, your image will be stored in the repository, and your
image ID will be printed.</p>
</div>
<div class="section" id="image-repository">
<h3>Image Repository<a class="headerlink" href="#image-repository" title="Permalink to this headline"></a></h3>
<p>We can consult the information of the image by executing the following
(please replace &lt;imageid&gt; with the ID that you got from the previous
command):</p>
<div class="highlight-python"><pre>fg-rain&gt; list * where imgId=&lt;imageId&gt;</pre>
</div>
</div>
<div class="section" id="register-image">
<h3>Register Image<a class="headerlink" href="#register-image" title="Permalink to this headline"></a></h3>
<p>Next, we need to register that image in the infrastructure we want to
use. In this case, we will register the image in OpenStack (if you
followed the FG Openstack tutorial, your novarc will probably be in
~/openstack/novarc). This command will return an &lt;ami-ID&gt;, which is the
ID of the image in OpenStack.</p>
<div class="highlight-python"><pre>fg-rain&gt; register -r &lt;imageId&gt; -s india -v ~/novarc</pre>
</div>
<p>Once you have registered the image, you need to wait until it becames
available. To check the status of the image, you can execute the
following command.</p>
<div class="highlight-python"><pre>fg-rain&gt; cloudlist -s india -v ~/novarc | grep &lt;imageId&gt;</pre>
</div>
<p>The status is the second field. You may experience that this command
takes time to respond when the image is being uploaded (it is an
OpenStack issue).</p>
</div>
<div class="section" id="start-image">
<h3>Start Image<a class="headerlink" href="#start-image" title="Permalink to this headline"></a></h3>
<p>Once the image is in &#8220;available&#8221; status, we can proceed to start a VM
using the ami-ID that we got from the register command. In this case, we
are going in Interative mode, which means that we are going to get
logged into the VM once it is running.</p>
<div class="highlight-python"><pre>fg-rain&gt; launch -i &lt;ami-ID&gt; -s india -v ~/novarc -I</pre>
</div>
</div>
</div>
<div class="section" id="futuregrid-standalone-image-repository">
<h2>FutureGrid Standalone Image Repository<a class="headerlink" href="#futuregrid-standalone-image-repository" title="Permalink to this headline"></a></h2>
<div class="section" id="id47">
<h3>Introduction<a class="headerlink" href="#id47" title="Permalink to this headline"></a></h3>
<p>The FutureGrid image repository is a standalone service that is not
tied to FutureGrid nor to any IaaS infrastructure.&nbsp;This image repository
offers a common&nbsp;interface that can distinguish image types for different
IaaS frameworks like Nimbus, Eucalyptus, and also bare-metal images.
This allows us in FG to include a diverse&nbsp;image set not only contributed
by the FG development team, but also by the user community that
generates such images and wishes to share them. The images can be
described with information about the software stack that is installed on
them including versions, libraries, and available services. This
information is maintained in the catalog and can be&nbsp;searched by users
and/or other FutureGrid services. Users&nbsp;looking for a specific image can
discover available images&nbsp;tting their needs, and nd their location in
the repository by using the catalog interface.</p>
<p>The repository supports different storage systems that allow you to
choose the most appropriate one for you. It includes MySQL, where the
image les are stored directly in the POSIX le&nbsp;system; MongoDB, where
both data and les are stored&nbsp;in the NoSQL database; the OpenStack
Object Store (Swift); and Cumulus from the Nimbus project. For the last
two cases, the data can be stored in either MySQL&nbsp;or in MongoDB. These
storage plugins not only increase the interoperability of the image
repository, but can also be used by the community as templates to create
their own plugins to support other storage systems.</p>
</div>
<div class="section" id="requirement">
<h3>Requirement<a class="headerlink" href="#requirement" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li>Python 2.6 or 2.7</li>
<li>LDAP server for user authentication</li>
</ul>
</div>
<div class="section" id="software">
<h3>Software<a class="headerlink" href="#software" title="Permalink to this headline"></a></h3>
<p>The software is open-source under an Apache 2 license. The image
repository can be found in github&nbsp;as part of our Image Management and
Rain
software&nbsp;<a class="reference external" href="https://github.com/futuregrid/rain">https://github.com/futuregrid/rain</a>&nbsp;.</p>
</div>
<div class="section" id="documentation">
<h3>Documentation<a class="headerlink" href="#documentation" title="Permalink to this headline"></a></h3>
<p>The documentation can be found
in&nbsp;<a class="reference external" href="http://futuregrid.github.com/rain/index.html">http://futuregrid.github.com/rain/index.html</a>.
Since this link provides information about all our tools, in this
section we collect the links that refer to the image repository.</p>
<div class="section" id="id48">
<h4>User Manual<a class="headerlink" href="#id48" title="Permalink to this headline"></a></h4>
<p>Information about the command-line interface is found
in&nbsp;<a class="reference external" href="http://futuregrid.github.com/rain/man-repo.html">http://futuregrid.github.com/rain/man-repo.html</a>.
If you are interested on using the shell, the manual is
in&nbsp;<a class="reference external" href="http://futuregrid.github.com/rain/man-shell.html">http://futuregrid.github.com/rain/man-shell.html</a>.</p>
</div>
<div class="section" id="installation">
<h4>Installation<a class="headerlink" href="#installation" title="Permalink to this headline"></a></h4>
<p>You need to install the complete software, even if you only want to
use the image repository. This information is found
in&nbsp;<a class="reference external" href="http://futuregrid.github.com/rain/install.html">http://futuregrid.github.com/rain/install.html</a>.</p>
</div>
<div class="section" id="configuration">
<h4>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline"></a></h4>
<p>The configuration information can be found
in&nbsp;<a class="reference external" href="http://futuregrid.github.com/rain/configure_futuregrid.html">http://futuregrid.github.com/rain/configure_futuregrid.html</a>.
You will be interested in:</p>
<ul class="simple">
<li>Configuration
files&nbsp;<a class="reference external" href="http://futuregrid.github.com/rain/configure_futuregrid.html#configuration-files">http://futuregrid.github.com/rain/configure_futuregrid.html#configuration-files</a></li>
<li>Setting up LDAP
information&nbsp;<a class="reference external" href="http://futuregrid.github.com/rain/configure_futuregrid.html#setting-up-ldap">http://futuregrid.github.com/rain/configure_futuregrid.html#setting-up-ldap</a></li>
<li>Setting up the Image
Repository&nbsp;<a class="reference external" href="http://futuregrid.github.com/rain/configure_futuregrid.html#setting-up-the-image-repository">http://futuregrid.github.com/rain/configure_futuregrid.html#setting-up-the-image-repository</a></li>
</ul>
</div>
</div>
</div>
<div class="section" id="manual-image-customization">
<h2>Manual Image Customization<a class="headerlink" href="#manual-image-customization" title="Permalink to this headline"></a></h2>
<p>Sometimes users need to further customize their images by installing
their own softwaresoftware which may not be able to be installed with
the image generation tool. In this tutorial, we explain how users can
perform this customization in the images created with the FG image
generation tool. Detailed information about&nbsp;FG image management tools
can be found in
<a class="reference external" href="http://futuregrid.github.com/rain/">http://futuregrid.github.com/rain/</a>.</p>
<div class="section" id="logging-into-india">
<h3>Logging into India<a class="headerlink" href="#logging-into-india" title="Permalink to this headline"></a></h3>
<div class="highlight-python"><pre>$ ssh &lt;username&gt;@india.futuregrid.org
$ module load futuregrid</pre>
</div>
<ul class="simple">
<li></li>
</ul>
</div>
<div class="section" id="requesting-access">
<h3>Requesting access<a class="headerlink" href="#requesting-access" title="Permalink to this headline"></a></h3>
<p>Submit a ticket to request
access&nbsp;<a class="reference external" href="https://portal.futuregrid.org/help">https://portal.futuregrid.org/help</a></p>
</div>
<div class="section" id="obtaining-the-image">
<h3>Obtaining the image<a class="headerlink" href="#obtaining-the-image" title="Permalink to this headline"></a></h3>
<p>You can get your image from our repository, or generate a new one.</p>
<div class="section" id="getting-an-image-from-the-repository">
<h4>Getting an image from the repository<a class="headerlink" href="#getting-an-image-from-the-repository" title="Permalink to this headline"></a></h4>
<div class="highlight-python"><pre>$ fg-repo -u &lt;username&gt; -g &lt;imageId&gt;</pre>
</div>
</div>
<div class="section" id="generating-a-new-image">
<h4>Generating a new image<a class="headerlink" href="#generating-a-new-image" title="Permalink to this headline"></a></h4>
<p>We are going to generate an Ubuntu image. Since we want to modify the
image, we have specified the -g parameter to retrieve the image after it
is generated. By default, it is uploaded to the image repository.</p>
<div class="highlight-python"><pre>$ fg-generate -u jdiaz -o ubuntu -v 12.04 -a x86_64 -s wget, openmpi-bin -g</pre>
</div>
<p>In both cases, we will obtain a tgz file that contains the image file
(.img) and a manifest (.manifest.xml). For example, our image could be
in <em>/N/u/&lt;username&gt;/123123123.tgz</em> .</p>
</div>
</div>
<div class="section" id="customizing-the-image">
<h3>Customizing the image<a class="headerlink" href="#customizing-the-image" title="Permalink to this headline"></a></h3>
<p>To continue with the rest of the tutorial, we need a UNIX machine
where we have root privileges.</p>
<p>Assuming that we are in a UNIX machine with root privileges:</p>
<ol class="arabic simple">
<li>Retrieve the image from India:</li>
</ol>
<div class="highlight-python"><pre>$ scp &lt;username&gt;@india.futuregrid.org:/N/u/&lt;username/123123123.tgz .</pre>
</div>
<ol class="arabic simple" start="2">
<li>Decompress the image:</li>
</ol>
<div class="highlight-python"><pre>$ tar vxfz 123123123.tgz

jdiaz859434.img
jdiaz859434.manifest.xml</pre>
</div>
<p>3. Mount the image. This will mount the image file into a directory.
In this way, we will have access to the OS files that are inside the
image.</p>
<div class="highlight-python"><pre>$ mkdir image
$ sudo mount -o loop jdiaz859434.img image</pre>
</div>
<p>Copy your software into the image directory (if needed). In this case,
I am going to copy a software directory to the tmp directory of the
image:</p>
<div class="highlight-python"><pre>$ cp -r /home/javi/mysoftware image/tmp</pre>
</div>
<p><em>Chroot</em> into the image. This changes the root of the OS to the one of
the image. In this way, evey operation we execute will have effect only
inside the image. After executing this command, you will be the root
users inside the image.</p>
<div class="highlight-python"><pre>$ sudo chroot image</pre>
</div>
<p>Now you can install whatever software you need. You can also use
<em>yum/apt</em> to install packages from the software repository. Remember
that they will be installed inside the image. As example, here I install
tomcat with <em>apt</em> and compile my software with <em>make</em>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># apt-get install tomcat6</span>
<span class="c"># cd /tmp/mysoftware</span>
<span class="c"># make &amp;&amp; make install</span>
</pre></div>
</div>
<p>You can also configure the OS of image to start services during the
boot time. The easiest way to do that is by modifying
the<em>/etc/rc.local</em>file.</p>
<p>Once you have finished customizing your image, you need to exit from
the <em>chroot</em> command, unmount the image, and compress it again:</p>
<div class="highlight-python"><pre># exit
$ sudo umount image
$ tar vxfz mynewimage.tgz jdiaz859434.img jdiaz859434.manifest.xml</pre>
</div>
</div>
<div class="section" id="transfer-the-image-back-to-india">
<h3>Transfer the image back to India<a class="headerlink" href="#transfer-the-image-back-to-india" title="Permalink to this headline"></a></h3>
<div class="highlight-python"><pre>$ scp mynewimage.tgz &lt;username&gt;@india.futuregrid.org:/N/u/&lt;username&gt;/</pre>
</div>
</div>
<div class="section" id="id49">
<h3>Log into India<a class="headerlink" href="#id49" title="Permalink to this headline"></a></h3>
<div class="highlight-python"><pre>$ ssh &lt;username&gt;@india.futuregrid.org
$ module load futuregrid</pre>
</div>
</div>
<div class="section" id="upload-the-image-to-the-repository">
<h3>Upload the image to the repository<a class="headerlink" href="#upload-the-image-to-the-repository" title="Permalink to this headline"></a></h3>
<p>We upload the image to the repository so we can reuse it and share it
with other users. When uploading the image, you can specify some
metadata to describe the properties of the image:</p>
<div class="highlight-python"><pre>$ fg-repo -p mynewimage.tgz "os=Ubuntu12 &amp; arch=x86_64 &amp; description=My new customized image &amp; tag=tomcat, openmpi"</pre>
</div>
<p>This command will provide you the ID your image has in the repository.
This ID is needed for the next step.</p>
</div>
<div class="section" id="register-your-image-in-different-infrastructures">
<h3>Register your image in different infrastructures<a class="headerlink" href="#register-your-image-in-different-infrastructures" title="Permalink to this headline"></a></h3>
<p>You now have your image ready to be registered in the different
FutureGrid infrastructures.&nbsp;To use OpenStack
(<a class="reference external" href="https://portal.futuregrid.org/tutorials/openstack">tutorial</a>) and
Eucalyptus
(<a class="reference external" href="https://portal.futuregrid.org/tutorials/eucalyptus3">tutorial</a>),
you need to indicate the location of you novarc and eucarc file.</p>
<div class="section" id="register-the-image-in-openstack">
<h4>Register the image in Openstack<a class="headerlink" href="#register-the-image-in-openstack" title="Permalink to this headline"></a></h4>
<div class="highlight-python"><pre>$ fg-register -u &lt;username&gt; -r &lt;imageID&gt; -s india -v ~/novarc</pre>
</div>
</div>
<div class="section" id="register-the-image-in-eucalyptus">
<h4>Register the image in Eucalyptus<a class="headerlink" href="#register-the-image-in-eucalyptus" title="Permalink to this headline"></a></h4>
<div class="highlight-python"><pre>$ fg-register -u &lt;username&gt; -r &lt;imageID&gt; -e india -v ~/eucarc</pre>
</div>
</div>
<div class="section" id="register-the-image-in-hpc">
<h4>Register the image in HPC<a class="headerlink" href="#register-the-image-in-hpc" title="Permalink to this headline"></a></h4>
<div class="highlight-python"><pre>$ fg-register -u &lt;username&gt; -r &lt;imageID&gt; -x india</pre>
</div>
<p>Each command will provide you with the ami-ID that your image has in
the specified infrastructure.</p>
</div>
</div>
<div class="section" id="using-your-registered-image">
<h3>Using your Registered Image<a class="headerlink" href="#using-your-registered-image" title="Permalink to this headline"></a></h3>
<div class="section" id="openstack-more-info-in-https-portal-futuregrid-org-tutorials-openstack">
<h4>OpenStack (more info in&nbsp;<a class="reference external" href="https://portal.futuregrid.org/tutorials/openstack">https://portal.futuregrid.org/tutorials/openstack</a>)<a class="headerlink" href="#openstack-more-info-in-https-portal-futuregrid-org-tutorials-openstack" title="Permalink to this headline"></a></h4>
<div class="highlight-python"><pre>source novarc
euca-run-instance -k &lt;keyname&gt; &lt;ami-ID&gt;</pre>
</div>
</div>
<div class="section" id="eucalyptus-more-info-in-https-portal-futuregrid-org-tutorials-eucalyptus3">
<h4>Eucalyptus (more info in&nbsp;<a class="reference external" href="https://portal.futuregrid.org/tutorials/eucalyptus3">https://portal.futuregrid.org/tutorials/eucalyptus3</a>)<a class="headerlink" href="#eucalyptus-more-info-in-https-portal-futuregrid-org-tutorials-eucalyptus3" title="Permalink to this headline"></a></h4>
<div class="highlight-python"><pre>source eucarc
euca-run-instance -k &lt;keyname&gt; &lt;ami-ID&gt;</pre>
</div>
</div>
<div class="section" id="hpc-more-info-in-https-portal-futuregrid-org-tutorials-hpc">
<h4>HPC (more info in&nbsp;<a class="reference external" href="https://portal.futuregrid.org/tutorials/hpc">https://portal.futuregrid.org/tutorials/hpc</a>)<a class="headerlink" href="#hpc-more-info-in-https-portal-futuregrid-org-tutorials-hpc" title="Permalink to this headline"></a></h4>
<p>Provision a machine with our image and go into Interactive mode (you
are logged into the machine):</p>
<div class="highlight-python"><pre>qsub -l os=&lt;ami-ID&gt; -I</pre>
</div>
<p>Provision two machines with our image and execute an script:</p>
<div class="highlight-python"><pre>qsub -l os=&lt;ami-ID&gt; -l nodes=2:ppn=8 myscript.sh</pre>
</div>
</div>
</div>
</div>
<div class="section" id="rain-manual-pages">
<h2>RAIN Manual Pages<a class="headerlink" href="#rain-manual-pages" title="Permalink to this headline"></a></h2>
<p>FutureGrid provides a number of manual pages, listed below.</p>
<p>(For Rain, our up-to-date documentation of rain can be found at
<a class="reference external" href="https://portal.futuregrid.org/doc/rain/index.html">https://portal.futuregrid.org/doc/rain/index.html</a>
and<a class="reference external" href="http://futuregrid.github.com/rain/index.html">http://futuregrid.github.com/rain/index.html</a>
.</p>
</div>
<div class="section" id="fg-repo">
<h2>fg-repo<a class="headerlink" href="#fg-repo" title="Permalink to this headline"></a></h2>
<p>Please see
<a class="reference external" href="http://futuregrid.github.com/rain/man-repo.html">http://futuregrid.github.com/rain/man-repo.html</a></p>
</div>
<div class="section" id="fg-rain">
<h2>fg-rain<a class="headerlink" href="#fg-rain" title="Permalink to this headline"></a></h2>
<p>Please see
<a class="reference external" href="http://futuregrid.github.com/rain/man-rain.html">http://futuregrid.github.com/rain/man-rain.html</a></p>
</div>
<div class="section" id="fg-generate">
<h2>fg-generate<a class="headerlink" href="#fg-generate" title="Permalink to this headline"></a></h2>
<p>Please see
<a class="reference external" href="http://futuregrid.github.com/rain/man-generate.html">http://futuregrid.github.com/rain/man-generate.html</a></p>
</div>
<div class="section" id="fg-register">
<h2>fg-register<a class="headerlink" href="#fg-register" title="Permalink to this headline"></a></h2>
<p>Please see
<a class="reference external" href="http://futuregrid.github.com/rain/man-register.html">http://futuregrid.github.com/rain/man-register.html</a></p>
</div>
<div class="section" id="fg-shell">
<h2>fg-shell<a class="headerlink" href="#fg-shell" title="Permalink to this headline"></a></h2>
<p>Please see
<a class="reference external" href="http://futuregrid.github.com/rain/man-shell.html">http://futuregrid.github.com/rain/man-shell.html</a></p>
</div>
<div class="section" id="fg-portal-manage">
<h2>fg-portal-manage<a class="headerlink" href="#fg-portal-manage" title="Permalink to this headline"></a></h2>
<p>The tool is for admin purpose only. An admin person or an system script
can call this to get user/project information from the portal.</p>
<div class="section" id="name">
<h3>NAME<a class="headerlink" href="#name" title="Permalink to this headline"></a></h3>
<p><strong>fg-portal-manage</strong> - Futuregrid account management script - the portal
portion</p>
</div>
<div class="section" id="synopsis">
<h3>SYNOPSIS<a class="headerlink" href="#synopsis" title="Permalink to this headline"></a></h3>
<div class="highlight-python"><pre>fg-portal-manage
 -h, --help            show this help message and exit
 -n NAME, --name=NAME  querying by name
 -m MAIL, --mail=MAIL  querying by email
 -i UID, --uid=UID     querying by portal uid
 -u USERNAME, --username=USERNAME
                       querying by portal username
 -k, --attrib          specifying attrib(s) to be displayed, inlcuding:
                       firstname, lastname, email, phone, organization,
                       institution_name, citizenship, sshkey, projectall,
                       project
 -l, --ldif            output ldif format of user info(no dn). If provided
                       all attributes will be included in the output
                       disregarding the setting from -k
 -v, --vetted          Show only those vetted users. If omitted all users
                       that meet the creteria will be shown
 -w, --waiting         Show the waiting list in which a user is eligible to
                       get an LDAP account
 -y, --withsshkey      A sub-option for -w, which shows the waiting list, but
                       only for those who have submitted a sshkey
 -p, --project         Get list of project in table view
 -s STATUS, --status=STATUS
                       A sub-option for -p, which specifies project status
                       that would like to be retrieved. Try pending,
                       approved, completed, denied
 -1, --pidonly         A sub-option for -p which prints out only the project</pre>
</div>
</div>
<div class="section" id="description">
<h3>DESCRIPTION<a class="headerlink" href="#description" title="Permalink to this headline"></a></h3>
<p>FutureGrid account admin tool queries user profile info from the portal
and also checks status against the ldap.</p>
<p>To run the script, python 2.7 is needed (if trying on lower version, try
to install the argparse module; it may also work), as well as the
mysqldb module. Depending on the os distribution, the installation could
be different. E.g., in ubuntu, it&#8217;s simply: apt-get install
python-mysqldb .</p>
<p>The futuregrid.cfg file needs to be put into the same directory as the
script, and the dummy config values need to be replaced with those real
ones. (Whoever has access to the portal server should have knowledge on
this. We will not distribute this to anymore other than the portal admin
and sys admin.)</p>
</div>
<div class="section" id="examples">
<h3>EXAMPLES<a class="headerlink" href="#examples" title="Permalink to this headline"></a></h3>
<p>Run this to get the help info:</p>
<div class="highlight-python"><pre>./fg-portal-manage.py -h</pre>
</div>
<p>This will give a table viewed info of user with uid 3:</p>
<div class="highlight-python"><pre>./fg-portal-manage.py -i 3</pre>
</div>
<p>However, the following:</p>
<div class="highlight-python"><pre>./fg-portal-manage.py -i 3 -l</pre>
</div>
<p>will print the long quasi-ldif formatted info.</p>
<p>Other commands:</p>
<div class="highlight-python"><pre>-u for username; -n for realname(first or last); as well as -m for email does similar thing.</pre>
</div>
<p>./fg-portal-manage.py -l</p>
<p>will simply print out all users in the portal in the ldif-like format.</p>
<p>./fg-portal-manage.py -l -v</p>
<p>will print out a similar list but only for (all) those vetted users,
i.e., approved users with an active project.</p>
<p>./fg-portal-manage.py -w</p>
<p>prints out all users who have approved portal account and are members
of an active project(s) (i.e., vetted users), but don&#8217;t have an LDAP
account yet.</p>
<div class="highlight-python"><pre>./fg-portal-manage.py -l -w</pre>
</div>
<p>the same as -w but in long quasi-ldif format.</p>
<div class="highlight-python"><pre>./fg-portal-manage.py -w -y</pre>
</div>
<p>the same as -w, but prints out only those who have submitted a sshkey.</p>
<div class="highlight-python"><pre>./fg-portal-manage.py -p</pre>
</div>
<p>lists all FG projects registered in the portal, in the format of:</p>
<div class="highlight-python"><pre>pid: title|project lead|project manager|status|[members list]</pre>
</div>
<p>project lead/manager and members are represented by the portaluid.</p>
<div class="highlight-python"><pre>./fg-portal-manage.py -p -s approved</pre>
</div>
<p>prints a list of projects but only those in &#8216;approved&#8217; status. Other
valid statuses are: pending, completed, denied.</p>
<div class="highlight-python"><pre>./fg-portal-manage.py -p -s completed -1(number '1')</pre>
</div>
<p>prints a list of completed projects but only display the projectids.</p>
</div>
<div class="section" id="see-also">
<h3><strong>SEE ALSO</strong><a class="headerlink" href="#see-also" title="Permalink to this headline"></a></h3>
</div>
</div>
<div class="section" id="vine">
<h2>ViNe<a class="headerlink" href="#vine" title="Permalink to this headline"></a></h2>
<p>ViNe is available to FutureGrid users on the foxtrot and sierra
clusters, and can be deployed in any cloud on virtual machines (requires
overlay network knowledge).</p>
<p><strong>Overview</strong>
ViNe is a project developed at University of Florida that implements
routing and other communication mechanisms needed to deploy a user-level
virtual network. ViNe is particularly appealing for cloud computing
because it allows the establishment of wide-area virtual networks
supporting symmetric communication among public and private network
resources (even when they are behind firewalls), does not require
changes to either the physical network or the OS of machines, and has
low virtualization overheads. ViNe can provide communication among
FutureGrid and external resources (including those with private IP
addresses) without the need to reconfigure the (FutureGrid) physical
network infrastructure.</p>
<p><strong>Prerequisites</strong>
In order for a host to participate in ViNe overlays, users need root
privilege to adjust operating system routing tables. Currently, users
can get root privilege on the virtual machines (VMs) started on FG
clouds. ViNe routers are available on foxtrot and sierra, and VMs
started through Nimbus can be configured to participate on ViNe
overlays.</p>
<p><strong>Setup</strong>
All VMs must be members of ViNe overlays, independently wether they
are on public or private networks. In order to enable the ViNe
communication, the following command needs to be executed on
participating VMs:</p>
<div class="highlight-python"><pre>wget -P /tmp -N http://www.acis.ufl.edu/vine/enablevine.php
. /tmp/enablevine.sh</pre>
</div>
<p><strong>Tutorial</strong></p>
<p>A step-by-step tutorial on connecting VMs to ViNe overlays is
available at:</p>
<p><a class="reference external" href="https://portal.futuregrid.org/contrib/simple-vine-tutorial">https://portal.futuregrid.org/contrib/simple-vine-tutorial</a></p>
</div>
<div class="section" id="opennebula-2-0-tutorial">
<h2>OpenNebula 2.0 Tutorial<a class="headerlink" href="#opennebula-2-0-tutorial" title="Permalink to this headline"></a></h2>
<p>This document is aimed to be a guide to start using OpenNebula 2.0.
Therefore, if you need more information about any particular OpenNebula
aspect that it is not addressed here, please refer to the official
documentation located in
(<a class="reference external" href="http://www.opennebula.org/documentation:documentation">http://www.opennebula.org/documentation:documentation</a>)</p>
<p>If you are a user that is only interested in test OpenNebula with some
pre-configured VM, you can use this guide
(<a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:vmg">http://www.opennebula.org/documentation:rel2.0:vmg</a>).</p>
<p>ALL users have to add the following lines to their $HOME/.bashrc or
$HOME/.profile file.</p>
<div class="highlight-python"><pre>export PATH=$PATH:$ONE_LOCATION/bin
export ONE_AUTH=$HOME/.one/one_ssh</pre>
</div>
</div>
<div class="section" id="create-a-vm-image-contextualized-for-opennebula">
<h2>Create a VM Image Contextualized for OpenNebula<a class="headerlink" href="#create-a-vm-image-contextualized-for-opennebula" title="Permalink to this headline"></a></h2>
<p>In this section we are going to explain how to create a KVM image and
how to contextualize for OpenNebula.</p>
<div class="section" id="create-vm-image-using-kvm">
<h3>Create VM Image Using KVM<a class="headerlink" href="#create-vm-image-using-kvm" title="Permalink to this headline"></a></h3>
<p>This process is generic and it is not related with OpenNebula directly.
Therefore, this image can be deploy in any machine with KVM and could be
used to create a VM image for any IaaS framework. The first step is
install KVM.</p>
<p>Ubuntu</p>
<div class="highlight-python"><pre>sudo apt-get install qemu kvm

sudo apt-get install libvirt-bin</pre>
</div>
<p>RHEL or CentOS</p>
<div class="highlight-python"><pre>sudo yum install kvm virt-manager</pre>
</div>
<p>Be sure that the virtualization features of your machine are enabled in
the BIOS. If they are, you should be able to load the KVM
modules</p>
<div class="highlight-python"><pre>sudo modprobe kvm-intel
or
sudo modprobe kvm-amd</pre>
</div>
<p>Now, we need to download the ISO that we are going to install in the VM,
in this case Ubuntu iso. You can do it from
<a class="reference external" href="http://ubuntu-releases.cs.umn.edu/10.10/ubuntu-10.10-server-amd64.iso">http://ubuntu-releases.cs.umn.edu/10.10/ubuntu-10.10-server-amd64.iso.</a></p>
<p>Every VM need a disk to be used as a virtual hard drive (1GB is enough
for Ubuntu server). Thus, to create this we use the next command:</p>
<div class="highlight-python"><pre>qemu-img create machine_name.img -f raw 1G</pre>
</div>
<p>After this, we are prepared to boot the VM and install the OS.</p>
<div class="highlight-python"><pre>kvm -m 512 -cdrom /home/user/ubuntu-10.10-server-amd64.iso -boot d machine_name.img</pre>
</div>
<p>This command will start the VM and you can install the OS as in a normal
PC. When you install the OS in the VM you can boot it using this
command:</p>
<div class="highlight-python"><pre>kvm -no-acpi -m 512 machine_name.img</pre>
</div>
</div>
<div class="section" id="contextualize-vm">
<h3>Contextualize VM<a class="headerlink" href="#contextualize-vm" title="Permalink to this headline"></a></h3>
<p>There are two contextualization mechanisms available in OpenNebula: the
automatic IP assignment, and a more generic way to give any file and
configuration parameters. You can use any of them individually, or both.
Nevertheless, we need to prepare the VM image to use this mechanism
(<a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:cong">http://www.opennebula.org/documentation:rel2.0:cong</a>).</p>
<p>First, download the vmcontext.sh&nbsp; file in your machine. This file depend
of your linux distribution.</p>
<div class="highlight-python"><pre>http://dev.opennebula.org/projects/opennebula/repository/show/share/scripts</pre>
</div>
<p>In this address you can find the vmcontext for each supported
distribution.
CentOS</p>
<div class="highlight-python"><pre>wget http://dev.opennebula.org/projects/opennebula/repository/revisions/master/raw/share/scripts/centos-5/net-vmcontext/vmcontext -O vmcontext.sh</pre>
</div>
<p>Ubuntu</p>
<div class="highlight-python"><pre>wget http://dev.opennebula.org/projects/opennebula/repository/revisions/master/raw/share/scripts/ubuntu/net-vmcontext/vmcontext -O vmcontext.sh</pre>
</div>
<p>Copy this file to the VM. To do that you can configure a network
interface in the VM and send the file using ssh. For example if your
machine has the private IP 192.168.1.1, you could assign to the VM the
192.168.1.2. So, execute this command in the VM:</p>
<div class="highlight-python"><pre>sudo ifconfig eht0 192.168.1.2</pre>
</div>
<p>From your machine execute this command to send the file to your VM:</p>
<div class="highlight-python"><pre>scp vmcontext.sh 192.168.1.2:/home/user</pre>
</div>
<p>In the VM copy the file to the /etc/init.d directory:</p>
<div class="highlight-python"><pre>sudo cp vmcontext.sh /etc/init.d
sudo chmod +x /etc/init.d/vmcontext.sh</pre>
</div>
<p>Configure init.d to execute the script during the VM startup
Centos</p>
<div class="highlight-python"><pre>sudo chkconfig --add vmcontext.sh</pre>
</div>
<p>Ubuntu</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ln</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">init</span><span class="o">.</span><span class="n">d</span><span class="o">/</span><span class="n">vmcontext</span><span class="o">.</span><span class="n">sh</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">rc2</span><span class="o">.</span><span class="n">d</span><span class="o">/</span><span class="n">S01vmcontext</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p>Add at the end of this file (this may not be needed):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">ifup</span> <span class="o">-</span><span class="n">a</span>
</pre></div>
</div>
<p>or</p>
<div class="highlight-python"><pre>/etc/init.d/networking restart  #(in ubuntu)
/etc/init.d/network restart     #(in centos)</pre>
</div>
<p>Remove the udev persistent files (ubuntu only)</p>
<div class="highlight-python"><pre>sudo rm -f /etc/udev/rules.d/z25_persistent-net.rules

sudo rm -f /etc/udev/rules.d/z45_persistent-net-generator.rules</pre>
</div>
<p>With all these steps we have configured the automatic IP assignment in
the VM. Now, we need to enable the generic contextualization by copying
these lines to /etc/rc.local.</p>
<div class="highlight-python"><pre>mount -t iso9660 /dev/sr0 /mnt  #(in ubuntu)
mount -t iso9660 /dev/hdc /mnt  #(in centos. Actually this depend on the initrd that you uses. If you use the one provided by Ubuntu then you use the previous line instead of this one)

if [ -f /mnt/context.sh ]; then

      . /mnt/init.sh

fi

umount /mnt

exit 0</pre>
</div>
<p>The file init.sh is different for each distribution and you can find
them in the <a class="reference external" href="http://dev.opennebula.org/projects/opennebula/repository/revisions/master/show/share/scripts">OpenNebula
repository</a>.
You do not need to include it in the VM because it is included
automatically by OpenNebula when you require it in the VM template.</p>
</div>
</div>
<div class="section" id="managing-virtual-machines">
<h2>Managing Virtual Machines<a class="headerlink" href="#managing-virtual-machines" title="Permalink to this headline"></a></h2>
<p><a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:vm_guide">(http://www.opennebula.org/documentation:rel2.0:vm_guide)</a></p>
<p>A Virtual Machine within the OpenNebula system consists of:</p>
<ul class="simple">
<li>A capacity in terms memory and CPU</li>
<li>A set of NICs attached to one or more virtual networks</li>
<li>A set of <strong>disk images</strong>.</li>
<li>A state file (optional) or <strong>recovery file</strong>, that contains the
memory image of a running VM plus some hypervisor specific
information.</li>
</ul>
<p>The above items, plus some additional VM attributes like the OS kernel
and context information to be used inside the VM, are specified in a
<strong>`VM
template &lt;http://www.opennebula.org/documentation:rel2.0:template&gt;`__</strong>
file.</p>
<p>Each VM in OpenNebula is identified by an unique number, the <tt class="docutils literal"><span class="pre">&lt;VID&gt;</span></tt>.
Also, the user can assign it a name in the <a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:template">VM
template</a>,
the default name for each VM is <tt class="docutils literal"><span class="pre">one-&lt;VID&gt;</span></tt>.</p>
<div class="section" id="virtual-machine-template">
<h3>Virtual Machine Template<a class="headerlink" href="#virtual-machine-template" title="Permalink to this headline"></a></h3>
<p>OpenNebula templates are designed to be hypervisor-agnostic, but there
are still some peculiarities to be taken into account, and mandatory
attributes change depending on the target hypervisor. Hypervisor
specific information for this attributes can be found in the drivers
configuration guides:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:xeng">Xen Adaptor</a></li>
<li><a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:kvmg">KVM Adaptor</a></li>
<li><a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:vmwareg">VMware
Adaptor</a></li>
</ul>
<p>OpenNebula has been designed to be easily extended, so any attribute
name can be defined for later use in any OpenNebula module. There are
some pre-defined attributes, though.</p>
<p>Please check the <a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:template">Virtual Machine definition
file</a>
reference for details on all the sections.</p>
<p>A basic VM template to be run using KVM could be this:</p>
<div class="highlight-python"><pre>#---------------------------------------
# VM definition example
#---------------------------------------

NAME = "ttylinux"

CPU    = 0.5
MEMORY = 64

# --- disks ---

DISK = [
  source   = "/srv/cloud/images/ttylinux/ttylinux.img",
  target   = "hda",
  readonly = "no" ]

# --- 1 NIC ---

NIC = [ NETWORK="Red LAN"]

FEATURES=[ acpi="no" ]

# --- VNC server ---

GRAPHICS = [
  type    = "vnc",
  listen  = "127.0.0.1"]</pre>
</div>
<p>In the previous template KVM will asume that the image is RAW. If you
have a image with the qcow2 format, you have to add the following to the
DISK description:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">driver</span> <span class="o">=</span> <span class="s">&quot;qcow2&quot;</span>
</pre></div>
</div>
<p>Assuming we have a VM template called &#8220;ttylinux.one&#8221; describing a
virtual machine. Then, we can allocate the VM in OpenNebula issuing a:</p>
<p>afterwards, the VM can be listed with the <tt class="docutils literal"><span class="pre">list</span></tt> option:</p>
</div>
</div>
<div class="section" id="managing-the-image-repository">
<h2>Managing the Image Repository<a class="headerlink" href="#managing-the-image-repository" title="Permalink to this headline"></a></h2>
<p>The Image Repository system allows OpenNebula administrators and users
to set up images, which can be operative systems or data, to be used in
Virtual Machines easily. These images can be used by several Virtual
Machines simultaneously, and also shared with other users.</p>
<p>This is what a sample template looks like:</p>
<p>To submit the template, you have to issue the command</p>
<p>You can use the <tt class="docutils literal"><span class="pre">oneimage</span> <span class="pre">list</span></tt> command to check the available images
in the repository.</p>
<p>There other commands to manage the images sotored in the Image
Repository:</p>
<ul class="simple">
<li>Use the <tt class="docutils literal"><span class="pre">oneimage</span> <span class="pre">publish</span></tt> and <tt class="docutils literal"><span class="pre">oneimage</span> <span class="pre">unpublish</span></tt> commands to
make your images public for every user to use in their virtual
machines, or private.</li>
<li>Use the <tt class="docutils literal"><span class="pre">oneimage</span> <span class="pre">persistent</span></tt> and <tt class="docutils literal"><span class="pre">oneimage</span> <span class="pre">nonpersistent</span></tt>
commands to make your images persistent (they are not cloned, but
rather used from the original image) or not. A persistent image
cannot be published.</li>
</ul>
<p>The only difference with the previous VM template is that now, you only
need to specify the name of the image that you want to use in the DISK
section.</p>
<p>Assuming that the template containing the previous information is called
template.one, you can deploy the VM using the command:</p>
<p>Once the VM is deployed you can save the changes made to any disk as a
new image. To do so, use the <tt class="docutils literal"><span class="pre">onevm</span> <span class="pre">saveas</span></tt> command. This command
takes three or four arguments: The VM name (or ID), the disk_id to
save, the name of the new image to register, and optionally the image
type.</p>
<p>To know the DISK_ID of the disk you want to save, just take a look at
the <tt class="docutils literal"><span class="pre">onevm</span> <span class="pre">show</span></tt> output for your VM.</p>
<p>The DISK_IDs are assigned in the same order the disks were defined in
the <a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:template">VM
template</a>.</p>
<p>This will register a new image called Ubuntu with Apache and MySQL,
that will be ready as soon as the VM is shut down.</p>
<p>The disks can be saved even if they were defined from a local disk file
(see the <a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:template">VM
template</a>
reference guide for more information on the different kinds of disks).</p>
</div>
<div class="section" id="managing-physical-hosts-and-clusters">
<h2>Managing Physical Hosts and Clusters<a class="headerlink" href="#managing-physical-hosts-and-clusters" title="Permalink to this headline"></a></h2>
<p>(<a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:cluster_guide">http://www.opennebula.org/documentation:rel2.0:cluster_guide</a>)</p>
<p>In order to use your existing physical nodes, you have to add them to
the system as OpenNebula hosts. You need the following information:</p>
<ul class="simple">
<li><em>Hostname</em> of the cluster node or IP</li>
<li><em>Information Driver</em> to be used to monitor the host, e.g. <tt class="docutils literal"><span class="pre">im_kvm</span></tt>.</li>
<li><em>Storage Driver</em> to clone, delete, move or copy images into the host,
e.g. <tt class="docutils literal"><span class="pre">tm_nfs</span></tt>.</li>
<li><em>Virtualization Driver</em> to boot, stop, resume or migrate VMs in the
host, e.g. <tt class="docutils literal"><span class="pre">vmm_kvm</span></tt>.</li>
</ul>
<p><img alt=":!:" src="http://www.opennebula.org/lib/images/smileys/icon_exclaim.gif" /> Before adding a host check that you can ssh to it without being
prompt for a password</p>
<div class="section" id="managing-hosts">
<h3>Managing Hosts<a class="headerlink" href="#managing-hosts" title="Permalink to this headline"></a></h3>
<p>Hosts can be added to the system anytime with the <tt class="docutils literal"><span class="pre">onehost</span></tt> command.
You can add the cluster nodes to be used by OpenNebula.</p>
<p>This is two example of adding a KVM host.</p>
<p>This is an example of adding a XEN host.</p>
<p>Note that in the two previous examples we are using tm_nfs. This means
that OpenNebula is configured to share the VM directory or to use SSH
with no share directory. Other option is the use of LVM. More
information about the diferent storage system supported can be found in
<a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:sm">http://www.opennebula.org/documentation:rel2.0:sm</a></p>
<p>The status of the cluster can be checked with the <tt class="docutils literal"><span class="pre">onehost</span> <span class="pre">list</span></tt>
command:</p>
<p>And specific information about a host with <tt class="docutils literal"><span class="pre">show</span></tt>:</p>
<p>If you want not to use a given host you can temporarily disable it:</p>
<p>A disabled host should be listed with <tt class="docutils literal"><span class="pre">STAT</span> <span class="pre">off</span></tt> by <tt class="docutils literal"><span class="pre">onehost</span> <span class="pre">list</span></tt>.
You can also remove a host permanently with:</p>
<p><img alt=":!:" src="http://www.opennebula.org/lib/images/smileys/icon_exclaim.gif" /> Detailed information of the <tt class="docutils literal"><span class="pre">onehost</span></tt> utility can be found <a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:cli#onehost">in
the Command Line
Reference</a></p>
<p>If you use the <tt class="docutils literal"><span class="pre">onecluster</span> <span class="pre">list</span></tt> command you will see that the
default cluster is created automatically.</p>
<p>You may want to isolate your physical hosts running virtual machines
containing important services for you business, from the virtual
machines running a development version of your software. The OpenNebula
administrator can do so with these commands:</p>
<p>From this point, the newly created machines can use this cluster names
as a <a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:template#placement_section">placement
requirement</a>:</p>
<p>Once your development cycle is finished, this testing and production
clusters may not be useful any more. Let&#8217;s delete the testing cluster.</p>
<p>As you can see, the hosts assigned to the testing cluster have been
moved to the default one.</p>
<p><img alt=":!:" src="http://www.opennebula.org/lib/images/smileys/icon_exclaim.gif" /> Detailed information of the <tt class="docutils literal"><span class="pre">onecluster</span></tt> utility can be found in
the <a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:cli#onecluster">Command Line
Reference</a></p>
</div>
</div>
<div class="section" id="managing-virtual-networks">
<h2>Managing Virtual Networks<a class="headerlink" href="#managing-virtual-networks" title="Permalink to this headline"></a></h2>
<p><a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:vgg">(</a><a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:vgg">http://www.opennebula.org/documentation:rel2.0:vgg</a>)</p>
<p>A cluster node is connected to one or more networks that are available
to the virtual machines through the corresponding bridges. To set up a
virtual networks you just need to know the name of the bridge to bind
the virtual machines to.</p>
<div class="section" id="defining-a-virtual-network">
<h3>Defining a Virtual Network<a class="headerlink" href="#defining-a-virtual-network" title="Permalink to this headline"></a></h3>
<p>OpenNebula allows for the creation of Virtual Networks by mapping them
on top of the physical ones. All Virtual Networks are going to share a
default value for the MAC preffix, set in the <tt class="docutils literal"><span class="pre">oned.conf</span></tt> file.</p>
<p>There are two types of Virtual Networks in OpenNebula:</p>
<ol class="arabic simple">
<li><strong>Fixed</strong>, defines a fixed set of IP-MAC pair addresses</li>
<li><strong>Ranged</strong>, defines a class network.</li>
</ol>
<p><img alt=":!:" src="http://www.opennebula.org/lib/images/smileys/icon_exclaim.gif" /> Virtual Networks created by <tt class="docutils literal"><span class="pre">oneadmin</span></tt> can be used by every
other user.</p>
<div class="section" id="fixed-virtual-network">
<h4>Fixed Virtual Network<a class="headerlink" href="#fixed-virtual-network" title="Permalink to this headline"></a></h4>
<p>For example to create a Fixed Virtual Network, called <tt class="docutils literal"><span class="pre">Public</span></tt> with
the set of public IPs to be used by the VMs, just create a file with the
following contents:</p>
<p>where <strong>LEASES</strong>: Definition of the IP-MAC pairs. If an IP is defined,
and there is no associated MAC, OpenNebula will generate it using the
following rule: <tt class="docutils literal"><span class="pre">MAC</span> <span class="pre">=</span> <span class="pre">MAC_PREFFIX:IP</span></tt>. So, for example, from IP
10.0.0.1 and MAC_PREFFIX 00:16, we get 00:16:0a:00:00:01. Defining only
a MAC address with no associated IP is not allowed.</p>
</div>
<div class="section" id="ranged-virtual-network">
<h4>Ranged Virtual Network<a class="headerlink" href="#ranged-virtual-network" title="Permalink to this headline"></a></h4>
<p>The following is an example of a Ranged Virtual Network template:</p>
<p>where:</p>
<ul class="simple">
<li><strong>NETWORK_ADDRESS</strong>: Base network address to generate IP addresses.</li>
<li><strong>NETWORK_SIZE</strong>: Number of hosts that can be connected using this
network. It can be defined either using a number or a network class
(B or C).</li>
</ul>
</div>
</div>
<div class="section" id="adding-and-deleting-virtual-networks">
<h3>Adding and Deleting Virtual Networks<a class="headerlink" href="#adding-and-deleting-virtual-networks" title="Permalink to this headline"></a></h3>
<p>Once a template for a VN has been defined, the <tt class="docutils literal"><span class="pre">onevnet</span></tt> command can
be used to create it.</p>
<p>To create the previous networks put their definitions in two different
files, <tt class="docutils literal"><span class="pre">public.net</span></tt> and <tt class="docutils literal"><span class="pre">red.net</span></tt>, respectively. Then, execute:</p>
<p>Also, <tt class="docutils literal"><span class="pre">onevnet</span></tt> can be used to query OpenNebula about available VNs:</p>
<p>with <tt class="docutils literal"><span class="pre">USER</span></tt> the owner of the network and <tt class="docutils literal"><span class="pre">#LEASES</span></tt> the number of
IP-MACs assigned to a VM from this network.</p>
<p>We can see the details of a particular VN:</p>
<div class="highlight-python"><pre>$onevnet show 0

VIRTUAL NETWORK 0 INFORMATION
ID:       : 0
UID:      : 0
PUBLIC    : N

VIRTUAL NETWORK TEMPLATE
BRIDGE=br1
LEASES=[ IP=192.168.1.7 ]
LEASES=[ IP=192.168.1.8 ]
LEASES=[ IP=192.168.1.9 ]
LEASES=[ IP=192.168.1.10 ]
NAME=Small LAN
TYPE=FIXED

LEASES INFORMATION
LEASE=[ IP=192.168.1.7, MAC=02:00:c0:a8:01:07, USED=1, VID=54 ]
LEASE=[ IP=192.168.1.8, MAC=02:00:c0:a8:01:08, USED=1, VID=45 ]
LEASE=[ IP=192.168.1.9, MAC=02:00:c0:a8:01:09, USED=0, VID=-1 ]
LEASE=[ IP=192.168.1.10, MAC=02:00:c0:a8:01:0a, USED=0, VID=-1 ]</pre>
</div>
<p>To delete a virtual network just use <tt class="docutils literal"><span class="pre">onevnet</span> <span class="pre">delete</span></tt>. For example to
delete the previous networks:</p>
<p><img alt=":!:" src="http://www.opennebula.org/lib/images/smileys/icon_exclaim.gif" /> Check the <tt class="docutils literal"><span class="pre">onevnet</span></tt> command help or <a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:cli#onevnet">reference
guide</a>
for more options to list the virtual networks.</p>
</div>
<div class="section" id="using-the-leases-within-the-virtual-machine">
<h3>Using the Leases Within the Virtual Machine<a class="headerlink" href="#using-the-leases-within-the-virtual-machine" title="Permalink to this headline"></a></h3>
<p>Hypervisors can attach a specific MAC address to a virtual network
interface, but Virtual Machines need to obtain an IP address. There are
a variety of ways to achieve this within OpenNebula:</p>
<ul class="simple">
<li>Obtain the IP from the MAC address, using the default MAC assignment
schema (<strong>PREFERRED</strong>)</li>
<li>Use the <tt class="docutils literal"><span class="pre">CONTEXT</span></tt> attribute, check the <a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:cong">Contextualization
Guide</a></li>
</ul>
<div class="section" id="configuring-the-virtual-machine-to-use-the-leases">
<h4>Configuring the Virtual Machine to Use the Leases<a class="headerlink" href="#configuring-the-virtual-machine-to-use-the-leases" title="Permalink to this headline"></a></h4>
<p>Please visit the <a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:cong">contextualization
guide</a> to learn
how to configure your Virtual Machines.</p>
</div>
</div>
</div>
<div class="section" id="accounting">
<h2>Accounting<a class="headerlink" href="#accounting" title="Permalink to this headline"></a></h2>
<p>(<a class="reference external" href="http://opennebula.org/documentation:rel2.0:accounting">http://opennebula.org/documentation:rel2.0:accounting</a>)</p>
<div class="section" id="physical-resources">
<h3>Physical Resources<a class="headerlink" href="#physical-resources" title="Permalink to this headline"></a></h3>
<p>Usage of a physical resource can be obtained for a given user or a given
virtual machine. Accounting reports always show aggregated information,
considering possible migration of a virtual machine to between physical
resources.</p>
<p><tt class="docutils literal"><span class="pre">oneacct</span> <span class="pre">host</span></tt> - prints accounting information for the physical
resources</p>
<p>Obtaining the accounting information for user john</p>
<p>The columns are:</p>
<ul class="simple">
<li>XFR: Total transfer time. It includes cloning VM disk images in the
target resource and transfer back of modified images when applicable</li>
<li>RUNN: Total running time. It includes boot, running and saving times</li>
<li>VMS: Total number of virtual machines</li>
</ul>
<p>Obtaining accounting information for host</p>
</div>
<div class="section" id="virtual-resources">
<h3>&nbsp;&nbsp;Virtual Resources<a class="headerlink" href="#virtual-resources" title="Permalink to this headline"></a></h3>
<p>Accounting information for Virtual Machines can be obtained for a given
user or a given virtual machine.</p>
<p>The current version of the virtual resources reports does not consider
possible migration of a virtual machine to between physical resources.</p>
<p><tt class="docutils literal"><span class="pre">oneacct</span> <span class="pre">vm</span></tt> - prints accounting information for virtual machines</p>
<p>Obtaining the accounting information for a given VM</p>
<p>The columns are:</p>
<ul class="simple">
<li>HOST: where the VM has been executed</li>
<li>XFR: Total transfer time in that host</li>
<li>RUNN: Total running time in the host</li>
</ul>
<p>Obtaining the accounting information for a given host</p>
<p>Obtaining the accounting information for a given user</p>
<p>In this last example columns show aggregated transfer and running times.</p>
</div>
</div>
<div class="section" id="user-management">
<h2>User Management<a class="headerlink" href="#user-management" title="Permalink to this headline"></a></h2>
<p>(<a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:users">http://www.opennebula.org/documentation:rel2.0:users)</a></p>
<div class="section" id="adding-new-users">
<h3>Adding New Users<a class="headerlink" href="#adding-new-users" title="Permalink to this headline"></a></h3>
<p>Current Authentication/Authorization module (from now on auth module)
has support for user/password and rsa private/public key authentication
and also user quota support. Here, we are going to talk about the SSH
module and how to add users using SSH keys. More information can be
found in the official documentation:
<a class="reference external" href="http://www.opennebula.org/documentation:rel2.0:users">http://www.opennebula.org/documentation:rel2.0:users</a></p>
<div class="section" id="public-key-extraction-done-by-users">
<h4>Public Key Extraction (Done by Users)<a class="headerlink" href="#public-key-extraction-done-by-users" title="Permalink to this headline"></a></h4>
<p>To create a user compatible with <tt class="docutils literal"><span class="pre">ssh</span></tt> option for authentication the
administrator needs to add the user&#8217;s public key into the database. The
file with public key generated by <tt class="docutils literal"><span class="pre">ssh-keygen</span></tt>
(<tt class="docutils literal"><span class="pre">$HOME/.ssh/id_rsa.pub</span></tt>) is written in a format readable by
<tt class="docutils literal"><span class="pre">openssl</span></tt> so the public key should be extracted by the user and sent
to the administrator. The way to extract it is the following:</p>
<p>The string the user has to send to the administrator to create the user
is written to the console.</p>
</div>
<div class="section" id="user-creation-done-by-the-administrator">
<h4>User Creation (done by the administrator)<a class="headerlink" href="#user-creation-done-by-the-administrator" title="Permalink to this headline"></a></h4>
<p>After the user sends the rsa public key the administration needs to
create a new account in OpenNebula system. This is done in a similar way
as standard user/password users but using the public key provided by the
user and adding a parameter to the command so the password (in this case
the public key) is stored as is in the database. Here is the command to
create a user called <tt class="docutils literal"><span class="pre">test</span></tt> with the previously extracted public key:</p>
<p>After this the administrator can check that the user is in the system:</p>
</div>
<div class="section" id="user-login-done-by-users">
<h4>User Login (Done by Users)<a class="headerlink" href="#user-login-done-by-users" title="Permalink to this headline"></a></h4>
<p>Before performing any action in OpenNebula system the user needs to
login. To do this we must issue this command:</p>
<p>The command on success will print the command needed to execute so
OpenNebula knows where to find the login file generated.</p>
<p>This line can also be added by the user to $HOME/.profile or
$HOME/.bashrc so it is not needed to be executed every new system login.</p>
<p>By default the login file will be valid for one hour. This expire time
can be changed adding another parameter with the number of seconds you
want the login to be valid. For example to create a 2 hours valid login
you can issue this command:</p>
</div>
</div>
<div class="section" id="quota">
<h3>Quota<a class="headerlink" href="#quota" title="Permalink to this headline"></a></h3>
<p>When quota module is enabled it checks for user resource consumption
before letting a VM to created in OpenNebula system. You have to take
into account that all resources for VMs listed by <tt class="docutils literal"><span class="pre">onevm</span> <span class="pre">list</span></tt> for a
user are counted as used. Administrator user has unlimited quota.</p>
<div class="section" id="quotas-database-security-done-by-the-administrator">
<h4>Quotas Database Security (Done by the Administrator)<a class="headerlink" href="#quotas-database-security-done-by-the-administrator" title="Permalink to this headline"></a></h4>
<p>By default user quota database is an <em>sqlite</em> database located at
<tt class="docutils literal"><span class="pre">$ONE_LOCATION/var/auth.db</span></tt>. This database on creation does not have
secure permissions and the administrator may change its permission and
maybe its location. To change its location you can use a full path to
the database in the auth module configuration file:</p>
<p>You may also change its permissions to <tt class="docutils literal"><span class="pre">0600</span></tt> so the unix user that
runs OpenNebula daemons is the only with read/write access permissions.</p>
</div>
<div class="section" id="default-quotas-done-by-the-administrator">
<h4>Default Quotas (Done by the Administrator)<a class="headerlink" href="#default-quotas-done-by-the-administrator" title="Permalink to this headline"></a></h4>
<p>Default quotas for all users are configured as previously stated in
<em>Configuring and Enabling Auth Module</em>. This is the first configuration
you will have to do if you enable quota system as any user without
explicit quotas can only use that amount of resources.</p>
</div>
<div class="section" id="explicit-user-quotas-done-by-the-administrator">
<h4>Explicit User Quotas (Done by the Administrator)<a class="headerlink" href="#explicit-user-quotas-done-by-the-administrator" title="Permalink to this headline"></a></h4>
<p>Setting special quotas for a user can be done using <tt class="docutils literal"><span class="pre">oneauth</span></tt> command.
If you have secured the database only unix user that runs OpenNebula
will be able to update them. The steps to change permissions for a user
are as follows:</p>
<ul class="simple">
<li>Find the user id for the user we want to change limits. We are
searching for user <tt class="docutils literal"><span class="pre">test</span></tt>:</li>
</ul>
<ul class="simple">
<li>The user id for <tt class="docutils literal"><span class="pre">test</span></tt> is 1. Now we use
<tt class="docutils literal"><span class="pre">oneauth</span> <span class="pre">quota</span> <span class="pre">set</span> <span class="pre">&lt;user</span> <span class="pre">id&gt;</span> <span class="pre">&lt;cpu&gt;</span> <span class="pre">&lt;memory&gt;</span></tt> to change user limits:</li>
</ul>
</div>
</div>
</div>
<div class="section" id="paas-platform-as-a-service">
<h2>PaaS - Platform as a Service<a class="headerlink" href="#paas-platform-as-a-service" title="Permalink to this headline"></a></h2>
<p>This chapter contains information in regards to Platform as a Service
offerings on FutureGrid</p>
</div>
<div class="section" id="using-map-reduce-in-futuregrid">
<h2>Using Map/Reduce in FutureGrid<a class="headerlink" href="#using-map-reduce-in-futuregrid" title="Permalink to this headline"></a></h2>
<p>As the computing landscape becomes increasingly data-centric,
data-intensive computing environments are poised to transform scientific
research. In particular, MapReduce based programming models and run-time
systems such as the open-source Hadoop system have increasingly been
adopted by researchers with data-intensive problems, in areas including
bio-informatics, data mining and analytics, and text processing.</p>
<p>FutureGrid&nbsp;provides capabilities that allow users to experiment with
MapReduce applications and middleware, including the widely-used
Hadoop&nbsp;platform and the iterative map/reduce Twister plaftorm. There are
different ways you may want to use MapReduce platforms in the testbed.
This page guides you in selecting from FutureGrid capabilities that are
best suited depending on your goals, and links to respective tutorials:</p>
<blockquote>
<div>MapReduce&nbsp;on Physical Machines</div></blockquote>
<hr class="docutils" />
<p>While there exist MapReduce systems that run on virtual machines,
many dedicated Hadoop deployments run the Hadoop run-time and
data-processing applications on physical machines to avoid I/O
virtualization overheads. Currently, we have two major approaches for
deploying Hadoop on physical machines in FutureGrid: The first uses
&#8220;MyHadoop&#8221;, where Hadoop tasks are instantiated dynamically using an HPC
scheduler (Torque). The second uses &#8220;SalsaHadoop&#8221;, where Hadoop starts
with a &#8216;one-click script&#8217; automatically on obtained HPC nodes and
tasks&nbsp;are submitted to the Hadoop&nbsp;master directly. In addition,
FutureGrid also supports Twister, a lightweight iterative MapReduce
runtime, running on the HPC cluster.</p>
<p>Associated tutorials:</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/hpc">Basic High Performance
Computing</a>&nbsp;[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/running-hadoop-batch-job-using-myhadoop">Running Hadoop as a batch job using
MyHadoop</a>&nbsp;[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-hpc">Running SalsaHadoop&nbsp;(one-click Hadoop) on HPC
environment</a>
[beginner]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-futuregrid-hpc">Running Twister on HPC
environment</a>
[beginner]</li>
</ul>
<blockquote>
<div>MapReduce&nbsp;on Virtual Machines</div></blockquote>
<hr class="docutils" />
<p>Running Hadoop on virtual machines gives users the flexibility to
customize the Hadoop runtime system and any additional middleware as
desired, e.g. for research on novel MapReduce middleware approaches.
Currently, Hadoop images can be deployed on FutureGrid resources in the
following ways:</p>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/eucalyptus">Using</a><a class="reference external" href="https://portal.futuregrid.org/tutorials/eucalyptus">&nbsp;Eucalyptus
on
FutureGrid</a><a class="reference external" href="https://portal.futuregrid.org/tutorials/eucalyptus">&nbsp;[novice]</a></li>
<li><a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-cloud-eucalyptus">Running SalsaHadoop&nbsp;on
Eucalyptus</a>
[intermediate]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/eucalyptus-and-twister-futuregrid">Running FG-Twister on
Eucalyptus</a>&nbsp;[intermediate]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-futuregrid-cloud-eucalyptus">Running Twister on
Eucalyptus</a>
[intermediate]</li>
</ul>
<div class="section" id="education-training-with-mapreduce">
<h3>Education / Training with MapReduce<a class="headerlink" href="#education-training-with-mapreduce" title="Permalink to this headline"></a></h3>
<p>FutureGrid offers educational virtual appliances that allow users to
deploy virtual private clusters where Hadoop tasks can be deployed using
Condor. This approach allows users to not only experiment with Hadoop on
FutureGrid, but also with virtual clusters, on their own resources.
Currently, Hadoop virtual appliances can be deployed on FutureGrid
resources in the following ways:</p>
<ul class="simple">
<li><a class="reference external" href="http://portal.futuregrid.org/tutorials/ga9">Running a Grid Appliance on
FutureGrid</a>&nbsp;[novice]</li>
<li><a class="reference external" href="http://portal.futuregrid.org/tutorials/ga8">Running Condor tasks on the Grid
Appliance</a>&nbsp;[novice]</li>
<li><a class="reference external" href="http://portal.futuregrid.org/tutorials/ga10">Running Hadoop&nbsp;tasks on the Grid
Appliance</a>&nbsp;[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/hadoop-wordcount">Running Hadoop WordCount&nbsp;on
FutureGrid</a>
[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/hadoop-blast">Running Hadoop Blast on
FutureGrid</a> [novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-kmeans">Running Twister Kmeans on
FutureGrid</a> [novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-blast">Running Twister Blast on
FutureGrid</a> [novice]</li>
</ul>
</div>
</div>
<div class="section" id="running-hadoop-as-a-batch-job-using-myhadoop">
<h2>Running Hadoop as a Batch Job using MyHadoop<a class="headerlink" href="#running-hadoop-as-a-batch-job-using-myhadoop" title="Permalink to this headline"></a></h2>
<p><a href="#id86"><span class="problematic" id="id87">|image94|&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |Hadoop logo|</span></a></p>
<p>=</p>
</div>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>MapReduce is a programming model developed by Google<strong>.</strong>Their
definition of MapReduce is as follows: &nbsp;&#8220;MapReduce is a programming
model and an associated implementation for processing and generating
large data sets. Users specify a map function that processes a key/value
pair to generate a set of intermediate key/value pairs, and a reduce
function that merges all intermediate values associated with the same
intermediate key.&#8221; &nbsp;For more information about MapReduce, please see the
Google paper <a class="reference external" href="http://labs.google.com/papers/mapreduce.html">here</a>.</p>
<p>The <a class="reference external" href="http://hadoop.apache.org">Apache&nbsp;Hadoop&nbsp;Projec</a>t provides an
open source implementation of&nbsp;MapReduce&nbsp;and&nbsp;HDFS&nbsp;(Hadoop&nbsp;Distributed
File System).</p>
<p>This tutorial illustrates how to run Apache Hadoop thru the batch
systems on FutureGrid using the MyHadoop tool.</p>
</div>
<div class="section" id="hadoop-on-futuregrid">
<h2>Hadoop on FutureGrid<a class="headerlink" href="#hadoop-on-futuregrid" title="Permalink to this headline"></a></h2>
<p>Hadoop 0.20.2 is currently installed on Alamo, Hotel, India, and Sierra
FutureGrid systems. &nbsp;Please see the <a class="reference external" href="https://portal.futuregrid.org/gettingstarted">Getting Started
guide</a> to get accounts
on those systems.</p>
</div>
<div class="section" id="what-is-myhadoop">
<h2>What is myHadoop?<a class="headerlink" href="#what-is-myhadoop" title="Permalink to this headline"></a></h2>
<p><a class="reference external" href="http://sourceforge.net/projects/myhadoop/">MyHadoop</a>&nbsp;is a set of
scripts that configure and instantiate&nbsp;Hadoop&nbsp;as a batch job.</p>
</div>
<div class="section" id="running-myhadoop-on-futuregrid">
<h2>Running myHadoop on FutureGrid<a class="headerlink" href="#running-myhadoop-on-futuregrid" title="Permalink to this headline"></a></h2>
<p>To run the example, use the following steps.</p>
<ol class="arabic">
<li><p class="first">Log into a FutureGrid system that has myHadoop available. &nbsp;In this
tutorial, we are executing from the Hotel machine.</p>
<div class="highlight-python"><pre>$ ssh hotel.futuregrid.org
This machine accepts SSH public key and One Time Password (OTP) logins only.
If you do not have a public key set up, you will be prompted for a password.
This is *not* your FutureGrid password, but the One Time Password generated from your
OTP token.  Do not type your FutureGrid password, it will not work.  If you do not
have a token or public key, you will not be able to login.
[gvonlasz@login1 ~]$</pre>
</div>
</li>
<li><p class="first">Load the myHadoop module. &nbsp;On some FutureGrid systems, you may also
need to load the &#8220;torque&#8221; module as well if qstat&nbsp;is not already in
your environment.</p>
<div class="highlight-python"><pre>[gvonlasz@login1 ~]$ module load myhadoop
SUN Java JDK version 1.6.0 (x86_64 architecture) loaded
Apache Hadoop Common version 0.20.203.0 loaded
myHadoop version 0.2a loaded
[gvonlasz@login1 ~]$</pre>
</div>
</li>
<li><p class="first">To run the example now, skip to step 9. &nbsp;Otherwise, view the
pbs-example.sh script located in $MY_HADOOP_HOME/pbs-example.sh.
&nbsp;At the top of the file, you will see standard batch directives
indicating which queue to run the Hadoop job, how many nodes, etc.</p>
</li>
<li><div class="first highlight-python"><div class="highlight"><pre><span class="c">#PBS -q batch</span>
<span class="c">#PBS -N hadoop_job</span>
<span class="c">#PBS -l nodes=4:ppn=8</span>
<span class="c">#PBS -o hadoop_run.out</span>
<span class="c">#PBS -e hadoop_run.err</span>
<span class="c">#PBS -V</span>
</pre></div>
</div>
</li>
<li><p class="first">Next, there is a line to load Java via modules under the above lines:</p>
<div class="highlight-python"><pre>module add java</pre>
</div>
</li>
<li><p class="first">In the example script, a temporary directory to store Hadoop
configuration files is specified as&nbsp;${HOME}/myHadoop-config&nbsp;(although
any globally accessible place is fine):</p>
<div class="highlight-python"><pre>#### Set this to the directory where Hadoop configs should be generated
# Don't change the name of this variable (HADOOP_CONF_DIR) as it is
# required by Hadoop - all config files will be picked up from here
#
# Make sure that this is accessible to all nodes
export HADOOP_CONF_DIR="${HOME}/myHadoop-config"</pre>
</div>
</li>
<li><p class="first">The pbs-example.sh script runs the &#8220;wordcount&#8221; program from
the&nbsp;hadoop-0.20.2-examples.jar. &nbsp;There is sample text data from the
<a class="reference external" href="http://www.gutenberg.org/">Project Gutenberg website</a>&nbsp;located a
$MY_HADOOP_HOME/gutenberg.</p>
<div class="highlight-python"><pre>-bash-3.2$ ls $MY_HADOOP_HOME/gutenberg
1342.txt.utf8</pre>
</div>
</li>
<li><p class="first">The following lines create a Data directory in HDFS (directory
specified in $MY_HADOOP_HOME/bin/setenv.sh), copies over the
gutenberg data, executes the Hadoop job, and then copies the output
back your ${HOME}/Hadoop-Outputs directory.</p>
<div class="highlight-python"><pre>#### Run your jobs here
echo "Run some test Hadoop jobs"
$HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR dfs -mkdir Data
$HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR dfs -copyFromLocal $MY_HADOOP_HOME/gutenberg Data
$HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR dfs -ls Data/gutenberg
$HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR jar $HADOOP_HOME/hadoop-0.20.2-examples.jar wordcount Data/gutenberg Outputs
$HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR dfs -ls Outputs
$HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR dfs -copyToLocal Outputs ${HOME}/Hadoop-Outputs</pre>
</div>
</li>
<li><p class="first">Now submit the pbs-example.sh script to Hotel:</p>
<div class="highlight-python"><pre>[gvonlasz@login1 ~]$ qsub $MY_HADOOP_HOME/pbs-example.sh
40256.svc.uc.futuregrid.org</pre>
</div>
</li>
<li><p class="first">The job will take about 5 minutes to complete. &nbsp;To monitor its
status, type &#8216;qstat&#8217;. &nbsp;The &#8220;R&#8221; means the job is running.</p>
<div class="highlight-python"><pre>[gvonlasz@login1 ~]$ qstat
Job id                    Name             User            Time Use S Queue
------------------------- ---------------- --------------- -------- - -----
40256.svc                  hadoop_job       gvonlasz               0 R batch</pre>
</div>
</li>
<li><p class="first">When it is done, the status of the job will be &#8220;C&#8221; meaning the job
has completed (or it will no longer be displayed in qstat output).
&nbsp;You should see a new hadoop_run.out file and an &#8220;Hadoop-Outputs&#8221;
directory :</p>
<div class="highlight-python"><pre>[gvonlasz@login1 ~]$ qstat
Job id                    Name             User            Time Use S Queue
------------------------- ---------------- --------------- -------- - -----
40256.svc                  hadoop_job       gvonlasz       00:00:05 C batch
-bash-3.2$ ls
Hadoop-Outputs hadoop_run.out</pre>
</div>
</li>
<li><p class="first">View results of the word count operation:</p>
<div class="highlight-python"><pre>[gvonlasz@login1 ~]$ head Hadoop-Outputs/part-r-00000
"'After    1
"'My   1
"'Tis  2
"A 12
"About 2
"Ah!   2
"Ah!" 1
"Ah,   1
"All   2
"All!  1</pre>
</div>
</li>
</ol>
<p>Now to run you own custom Hadoop job, make a copy of the
$MY_HADOOP_HOME/pbs-example.sh script and modify the lines described
in Step 7.</p>
</div>
<div class="section" id="persistent-mode">
<h2>Persistent Mode<a class="headerlink" href="#persistent-mode" title="Permalink to this headline"></a></h2>
<p>The above example copies input to local HDFS scratch space you specified
in $MY_HADOOP_HOME/bin/setenv.sh, runs MapReduce, and copies output
from HDFS back to your home directory. &nbsp;This is called non-persistent
mode and is good for small amounts of data. &nbsp;Alternatively, you can run
in persistent mode which is good if you have access to a parallel file
system or have a large amount of data that will not fit in scratch
space. &nbsp;To enable persistent mode, follow the directions in
pbs-example.sh.</p>
</div>
<div class="section" id="customizing-hadoop-settings">
<h2>Customizing Hadoop&nbsp;Settings<a class="headerlink" href="#customizing-hadoop-settings" title="Permalink to this headline"></a></h2>
<p>To modify any of the Hadoop settings
like&nbsp;maximum_number_of_map_task, maximum_number_of_reduce_task,
etc., make you own copy of myHadoop and customize the settings
accordingly. &nbsp;For example:</p>
<ol class="arabic">
<li><p class="first">Copy the $MY_HADOOP_HOME directory to your home directory</p>
<div class="highlight-python"><pre>-bash-3.2$ cp -r $MY_HADOOP_HOME $HOME/myHadoop</pre>
</div>
</li>
<li><p class="first">Then edit $HOME/myHadoop/pbs-example.sh and on line 16, replace it
with:</p>
<div class="highlight-python"><pre>. ${HOME}/myHadoop/bin/setenv.sh</pre>
</div>
</li>
<li><p class="first">Similarly edit $HOME/myHadoop/bin/setenv.sh and on line 4, replace it
with:</p>
<div class="highlight-python"><pre>export MY_HADOOP_HOME=$HOME/myHadoop</pre>
</div>
</li>
<li><p class="first">Customize the settings in the Hadoop files as needed in
$HOME/myHadoop/etc</p>
</li>
<li><p class="first">Submit your copy of pbs-example.sh:</p>
<div class="highlight-python"><pre>-bash-3.2$ qsub $HOME/myHadoop/pbs-example.sh</pre>
</div>
</li>
</ol>
</div>
<div class="section" id="using-a-different-installation-of-hadoop">
<h2>Using a Different Installation of Hadoop<a class="headerlink" href="#using-a-different-installation-of-hadoop" title="Permalink to this headline"></a></h2>
<p>If you would like to use a different version of my Hadoop or have
customized the Hadoop code in some way, you can specify a different
installation of Hadoop by redefining the HADOOP_HOME variable after
$MY_HADOOP_HOME/bin/setenv.sh is called within your own copy of
pbs-example.sh.</p>
<div class="highlight-python"><pre>### Run the myHadoop environment script to set the appropriate variables
#
# Note: ensure that the variables are set correctly in bin/setenv.sh
. /opt/myHadoop/bin/setenv.sh
export HADOOP_HOME=${HOME}/my-custom-hadoop</pre>
</div>
</div>
<div class="section" id="more-information">
<h2>More Information<a class="headerlink" href="#more-information" title="Permalink to this headline"></a></h2>
<p>For more information about how myHadoop works, please see the
documentation in $MY_HADOOP_HOME/docs/myHadoop.pdf</p>
</div>
<div class="section" id="using-salsahadoop-on-futuregrid">
<h2>Using SalsaHadoop on FutureGrid<a class="headerlink" href="#using-salsahadoop-on-futuregrid" title="Permalink to this headline"></a></h2>
<p>PLEASE NOTE: THIS MANUAL PAGE IS A DRAFT, PLEASE PROVIDE FEEDBACK IN
THE COMMENT SECTION.</p>
<div class="section" id="salsahadoop-introduction">
<h3>SalsaHadoop Introduction<a class="headerlink" href="#salsahadoop-introduction" title="Permalink to this headline"></a></h3>
<p>Apache Hadoop is widely used by domain scientists for running their
scientific&nbsp;applications in parallel fashion. For our research
convenience, SalsaHPC research group develops SalsaHadoop, an automatic
method to start Hadoop without worrying the Hadoop configuration, can be
running on any general cluster and multiple machines. SalsaHadoop has
been used by <a class="reference external" href="http://salsahpc.indiana.edu/">SalsaHPC&nbsp;research group</a>
and a graduate-level course <a class="reference external" href="http://salsahpc.indiana.edu/csci-b649-2011/">CSCI B649 Cloud Computing for Data
Intensive Sciences</a>.</p>
</div>
<div class="section" id="running-salsahadoop-on-futuregrid">
<h3>Running SalsaHadoop on FutureGrid<a class="headerlink" href="#running-salsahadoop-on-futuregrid" title="Permalink to this headline"></a></h3>
<p>SalsaHadoop can be run in various modes within FG&nbsp;either in FutureGrid
HPC and FutureGrid Cloud/IaaS environments. The following tutorials
provide step-by-step instructions to use SalsaHadoop on these modes, and
also it shows some examples of running Hadoop applications after
starting Hadoop. In general, the HPC environment is easier if you do not
have experience with IaaS Eucalyptus.</p>
<ul class="simple">
<li>SalsaHadoop on FutureGrid<ul>
<li><a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-hpc">SalsaHadoop with FutureGrid
HPC</a>&nbsp;[recommended]<ul>
<li><a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-hpc#HPC_Nodes">Get HPC compute
nodes</a></li>
<li><a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-hpc#Configuration">Hadoop
Configuration</a></li>
<li><a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-hpc#Verify">Verify Hadoop HDFS and MapReduce Daemon
status</a></li>
</ul>
</li>
<li><a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-cloud-eucalyptus">SalsaHadoop with FutureGrid Cloud
Eucalyptus</a><ul>
<li><a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-cloud-eucalyptus#VM_Nodes">Get VM compute
nodes</a><ul>
<li><a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-cloud-eucalyptus#VM_Nodes_Set">VM Hostname
setting</a></li>
<li><a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-cloud-eucalyptus#Euca_Disk">VM attached disk
configuration</a></li>
</ul>
</li>
<li><a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-cloud-eucalyptus#Configuration">Hadoop
Configuration</a>&nbsp;(same
as above with different masters and slaves hostname)</li>
<li><a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-cloud-eucalyptus#Verify">Verify Hadoop HDFS and MapReduce Daemon
status</a></li>
</ul>
</li>
<li>Run SalsaHadoop Applications<ul>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/one-click-hadoop-wordcount-eucalyptus-futuregrid">Hadoop
WordCount</a></li>
<li><a class="reference external" href="https://portal.futuregrid.org/manual/hadoop-blast">Hadoop
Blast</a></li>
</ul>
</li>
<li>Run Hadoop with static FutureGrid-Bravo&nbsp;HDFS*</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="hadoop-blast">
<h2>Hadoop Blast<a class="headerlink" href="#hadoop-blast" title="Permalink to this headline"></a></h2>
<p>Number:
Author: Tak-Lon Stephen Wu
Improvements:
Version: 0.1
Date: 2011-11-01</p>
<div class="section" id="id50">
<h3>Hadoop Blast<a class="headerlink" href="#id50" title="Permalink to this headline"></a></h3>
<p>BLAST (Basic Local Alignment Search Tool) is one of the most widely used
bioinformatics applications written in C++, and&nbsp;the version we are using
is v2.2.23. Hadoop Blast is an advanced Hadoop program which helps
Blast, a bioinformatics application, utilizes the Computing Capability
of Hadoop. The database used in the following settings is a subset (241
MB) of Non-redundant protein sequence database from
<a class="reference external" href="http://www.ncbi.nlm.nih.gov/staff/tao/URLAPI/blastdb.html">nr</a>
(8.5GB) database.</p>
<p>You can download the <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/source_code/Hadoop-Blast.zip">Hadoop Blast source
code</a>
and customized Blast program and Database archive
(<a class="reference external" href="http://salsahpc.indiana.edu/tutorial/apps/BlastProgramAndDB.tar.gz">BlastProgramAndDB.tar.gz</a>)
from <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/hadoopblast.html">Big Data for Science
tutorial</a>.</p>
<ul class="simple">
<li></li>
<li></li>
</ul>
</div>
<div class="section" id="acknowledge">
<h3>Acknowledge<a class="headerlink" href="#acknowledge" title="Permalink to this headline"></a></h3>
<p>This page was original designed by
<a class="reference external" href="http://salsahpc.indiana.edu/">SalsaHPC</a> group for <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/">Big Data for
Science Workshop</a>, you can see
the original pages
<a class="reference external" href="http://salsahpc.indiana.edu/tutorial/hadoopblast.html">here</a>.</p>
</div>
<div class="section" id="id51">
<h3>Requirement<a class="headerlink" href="#id51" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li>Login to FutureGrid Cluster and obtain compute nodes.
(<a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-hpc#HPC_Nodes">HPC</a>/
<a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-cloud-eucalyptus">Eucalyptus</a>)</li>
<li>Start SalsaHadoop/Hadoop on compute nodes. (<a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-hpc#Configuration">SalsaHadoop
Tutorial</a>)</li>
<li>Download and unzip&nbsp;<a class="reference external" href="http://salsahpc.indiana.edu/tutorial/source_code/Hadoop-Blast.zip">Hadoop Blast source
code</a>
from <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/hadoopwordcount.html">Big Data for Science
tutorial</a>.</li>
<li>Download customized Blast binary and Database archive
<a class="reference external" href="http://salsahpc.indiana.edu/tutorial/apps/BlastProgramAndDB.tar.gz">BlastProgramAndDB.tar.gz</a></li>
<li>Linux command experience.</li>
</ol>
</div>
<div class="section" id="download-hadoop-blast-under">
<h3>1. Download Hadoop Blast under $<a class="headerlink" href="#download-hadoop-blast-under" title="Permalink to this headline"></a></h3>
</div>
<div class="section" id="hadoop-home">
<h3>HADOOP_HOME<a class="headerlink" href="#hadoop-home" title="Permalink to this headline"></a></h3>
<p>Assuming your start SalsaHadoop/Hadoop with setting
$HADOOP_HOME=~/hadoop-0.20.203.0, and is running the master node on
i55. Then, we download the <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/source_code/Hadoop-Blast.zip">Hadoop Blast source
code</a>
and customized Blast program and Database archive
(<a class="reference external" href="http://salsahpc.indiana.edu/tutorial/apps/BlastProgramAndDB.tar.gz">BlastProgramAndDB.tar.gz</a>)
from <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/hadoopblast.html">Big Data for Science
tutorial</a> to
$HADOOP_HOME.</p>
<div class="highlight-python"><pre>[taklwu@i55 ~]$ cd $HADOOP_HOME
[taklwu@i55 hadoop-0.20.203.0]$ wget http://salsahpc.indiana.edu/tutorial/source_code/Hadoop-Blast.zip
[taklwu@i55 hadoop-0.20.203.0]$ wget http://salsahpc.indiana.edu/tutorial/apps/BlastProgramAndDB.tar.gz
[taklwu@i55 hadoop-0.20.203.0]$ unzip Hadoop-Blast.zip</pre>
</div>
</div>
<div class="section" id="prepare-hadoop-blast">
<h3>2. Prepare Hadoop Blast<a class="headerlink" href="#prepare-hadoop-blast" title="Permalink to this headline"></a></h3>
<p>Assuming the program are already stored in $HADOOP_HOME/Hadoop-Blast,
we need to copy the input files, Blast program and Database archive
(<a class="reference external" href="http://salsahpc.indiana.edu/tutorial/apps/BlastProgramAndDB.tar.gz">BlastProgramAndDB.tar.gz</a>)
onto HDFS.</p>
<div class="highlight-python"><pre>[taklwu@i55 hadoop-0.20.203.0]$ bin/hadoop fs -put $HADOOP_HOME/Hadoop-Blast/blast_input HDFS_blast_input
[taklwu@i55 hadoop-0.20.203.0]$ bin/hadoop fs -ls HDFS_blast_input
[taklwu@i55 hadoop-0.20.203.0]$ bin/hadoop fs -copyFromLocal $HADOOP_HOME/BlastProgramAndDB.tar.gz BlastProgramAndDB.tar.gz
[taklwu@i55 hadoop-0.20.203.0]$ bin/hadoop fs -ls BlastProgramAndDB.tar.gz</pre>
</div>
<ul class="simple">
<li>Line 1 push all the blast input files (FASTA formatted queries) onto
HDFS HDFS_blast_input directory from local disk.</li>
<li>Line 2 list the pushed files on HDFS directory &#8220;HDFS_blast_input&#8221;</li>
<li>Line 3 copies the Blast program and database archive
(BlastProgramAndDB.tar.gz) from $HADOOP_HOME onto the HDFS as
distributed caches which will be used later.</li>
<li>Line 4 double check the pushed Blast program and database archive
&#8220;BlastProgramAndDB.tar.gz&#8221; on HDFS</li>
</ul>
</div>
<div class="section" id="execute-hadoop-blast">
<h3>3. Execute Hadoop-Blast<a class="headerlink" href="#execute-hadoop-blast" title="Permalink to this headline"></a></h3>
<p>After deploying those required files onto HDFS, run the Hadoop Blast
program with the following commands:</p>
<div class="highlight-python"><pre>[taklwu@i55 hadoop-0.20.203.0]$ bin/hadoop jar $HADOOP_HOME/Hadoop-Blast/executable/blast-hadoop.jar BlastProgramAndDB.tar.gz \
 bin/blastx /tmp/hadoop-taklwu-test/ db nr HDFS_blast_input HDFS_blast_output '-query #_INPUTFILE_#-outfmt 6 -seg no -out #_OUTPUTFILE_#'</pre>
</div>
<p>Here is the description of the above command:</p>
<div class="highlight-python"><pre>bin/hadoop jar Executable BlastProgramAndDB_on_HDFS bin/blastx Local_Work_DIR db nr HDFS_Input_DIR Unique_HDFS_Output_DIR '-query #_INPUTFILE_#-outfmt 6 -seg no -out #_OUTPUTFILE_#'</pre>
</div>
<table border="1" class="docutils">
<colgroup>
<col width="23%" />
<col width="77%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Parameter</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="row-even"><td>Executable</td>
<td>The full path of the Hadoop-Blast Jar program, e.g. $HADOOP_HOME/apps/Hadoop-Blast/executable/blast-hadoop.jar</td>
</tr>
<tr class="row-odd"><td>BlastProgramAndDB_on_HDFS</td>
<td>The archive name of Blast Program and Database on HDFS, e.g. BlastProgramAndDB.tar.gz</td>
</tr>
<tr class="row-even"><td>Local_Work_DIR</td>
<td>The local directory for storing temporary output of Blast Program, e.g. /tmp/hadoop-test/</td>
</tr>
<tr class="row-odd"><td>HDFS_Input_DIR</td>
<td>The HDFS remote directory where stored input files, e.g. HDFS_blast_input</td>
</tr>
<tr class="row-even"><td>Unique_HDFS_Output_DIR</td>
<td>A Never used HDFS remote directory for storing output files, e.g. HDFS_blast_output</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>If Hadoop is running correctly, it will print hadoop running messages
similar to the following:</p>
<div class="highlight-python"><pre>11/11/01 19:31:08 INFO input.FileInputFormat: Total input paths to process : 16
11/11/01 19:31:08 INFO mapred.JobClient: Running job: job_201111021738_0002
11/11/01 19:31:09 INFO mapred.JobClient: map 0% reduce 0%
11/11/01 19:31:31 INFO mapred.JobClient: map 18% reduce 0%
11/11/01 19:31:34 INFO mapred.JobClient: map 50% reduce 0%
11/11/01 19:31:53 INFO mapred.JobClient: map 75% reduce 0%
11/11/01 19:32:04 INFO mapred.JobClient: map 100% reduce 0%
...
Job Finished in 191.376 seconds</pre>
</div>
</div>
<div class="section" id="monitoring-hadoop">
<h3>3. Monitoring Hadoop<a class="headerlink" href="#monitoring-hadoop" title="Permalink to this headline"></a></h3>
<p>We can also monitor the job status using lynx, a text browser, on i136
based Hadoop monitoring console. Assuming the Hadoop Jobtracker is
running on i55:9003:</p>
<div class="highlight-python"><pre>[taklwu@i136 ~]$ lynx i55:9003</pre>
</div>
<p>In addition, all the outputs will stored in the HDFS output directory
(e.g. HDFS_blast_output).</p>
<div class="highlight-python"><pre>[taklwu@i55 hadoop-0.20.203.0]$ bin/hadoop fs -ls HDFS_blast_output
[taklwu@i55 hadoop-0.20.203.0]$ bin/hadoop fs -cat HDFS_blast_output/*
BG3:2_30MNAAAXX:7:1:981:1318/1  gi|298916876|dbj|BAJ09735.1|    100.00  11      0       0       3       35      9       19      7.0     27.7
BG3:2_30MNAAAXX:7:1:981:1318/1  gi|298708397|emb|CBJ48460.1|    100.00  11      0       0       3       35      37      47      7.0     27.7
BG3:2_30MNAAAXX:7:1:981:1318/1  gi|298104210|gb|ADI54942.1|     100.00  11      0       0       3       35      11      21      7.0     27.7
BG3:2_30MNAAAXX:7:1:981:1318/1  gi|297746593|emb|CBM42053.1|    100.00  11      0       0       3       35      11      21      7.0     27.7
BG3:2_30MNAAAXX:7:1:981:1318/1  gi|297746591|emb|CBM42052.1|    100.00  11      0       0       3       35      11      21      7.0     27.7
BG3:2_30MNAAAXX:7:1:981:1318/1  gi|297746589|emb|CBM42051.1|    100.00  11      0       0       3       35      11      21      7.0     27.7
BG3:2_30MNAAAXX:7:1:981:1318/1  gi|297746587|emb|CBM42050.1|    100.00  11      0       0       3       35      11      21      7.0     27.7
BG3:2_30MNAAAXX:7:1:981:1318/1  gi|297746585|emb|CBM42049.1|    100.00  11      0       0       3       35      11      21      7.0     27.7
BG3:2_30MNAAAXX:7:1:981:1318/1  gi|297746583|emb|CBM42048.1|    100.00  11      0       0       3       35      11      21      7.0     27.7
BG3:2_30MNAAAXX:7:1:981:1318/1  gi|297746581|emb|CBM42047.1|    100.00  11      0       0       3       35      11      21      7.0     27.7

...</pre>
</div>
</div>
<div class="section" id="finishing-the-map-reduce-process">
<h3>5.&nbsp;Finishing the Map-Reduce process<a class="headerlink" href="#finishing-the-map-reduce-process" title="Permalink to this headline"></a></h3>
<p>After finishing the Job, please use the command to kill the HDFS and
Map-Reduce daemon:</p>
<div class="highlight-python"><pre>[taklwu@i55 hadoop-0.20.203.0]$ bin/stop-all.sh</pre>
</div>
</div>
</div>
<div class="section" id="hadoop-wordcount">
<h2>Hadoop WordCount<a class="headerlink" href="#hadoop-wordcount" title="Permalink to this headline"></a></h2>
<dl class="docutils">
<dt>Number:</dt>
<dd>Author: Tak-Lon Stephen Wu
Improvements:</dd>
</dl>
<p>Version: 0.1
Date: 2011-11-01</p>
<div class="section" id="id52">
<h3>Hadoop WordCount<a class="headerlink" href="#id52" title="Permalink to this headline"></a></h3>
<p>WordCount is a simple program which counts the number of occurrences of
each word in a given text input data set. WordCount fits very well with
the MapReduce programming model making it a great eample to understand
the Hadoop Map/Reduce programming style. You can download the <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/source_code/Hadoop-WordCount.zip">WordCount
source
code</a>
from <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/hadoopwordcount.html">Big Data for Science
tutorial</a>.</p>
<ul class="simple">
<li></li>
</ul>
</div>
<div class="section" id="id53">
<h3>Acknowledge<a class="headerlink" href="#id53" title="Permalink to this headline"></a></h3>
<p>This page was original designed by
<a class="reference external" href="http://salsahpc.indiana.edu/">SalsaHPC</a> group for <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/">Big Data for
Science Workshop</a>, you can see
the original pages
<a class="reference external" href="http://salsahpc.indiana.edu/tutorial/hadoopwordcount.html">here</a>.</p>
</div>
<div class="section" id="id54">
<h3>Requirement<a class="headerlink" href="#id54" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li>Login to FutureGrid Cluster and obtain compute nodes.
(<a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-hpc#HPC_Nodes">HPC</a>/
<a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-cloud-eucalyptus">Eucalyptus</a>)</li>
<li>Start SalsaHadoop/Hadoop on compute nodes. (<a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-hpc#Configuration">SalsaHadoop
Tutorial</a>)</li>
<li>Download and unzip <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/source_code/Hadoop-WordCount.zip">WordCount source
code</a>
from <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/hadoopwordcount.html">Big Data for Science
tutorial</a>.</li>
<li>Linux command experience.</li>
</ol>
</div>
<div class="section" id="download-and-unzip-wordcount-under-hadoop-home">
<h3>1. Download and unzip WordCount under $HADOOP_HOME<a class="headerlink" href="#download-and-unzip-wordcount-under-hadoop-home" title="Permalink to this headline"></a></h3>
<p>Assuming your start SalsaHadoop/Hadoop with setting
$HADOOP_HOME=~/hadoop-0.20.203.0, and is running the master node on
i55. Then, we download and unzip the <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/source_code/Hadoop-WordCount.zip">WordCount source
code</a>
from&nbsp; <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/hadoopwordcount.html">Big Data for Science
tutorial</a>
under $HADOOP_HOME.</p>
<div class="highlight-python"><pre>[taklwu@i55 ~]$ cd $HADOOP_HOME
[taklwu@i55 hadoop-0.20.203.0]$ wget http://salsahpc.indiana.edu/tutorial/source_code/Hadoop-WordCount.zip
[taklwu@i55 hadoop-0.20.203.0]$ unzip Hadoop-WordCount.zip</pre>
</div>
</div>
<div class="section" id="execute">
<h3>2. Execute<a class="headerlink" href="#execute" title="Permalink to this headline"></a></h3>
</div>
<div class="section" id="id55">
<h3>Hadoop-WordCount<a class="headerlink" href="#id55" title="Permalink to this headline"></a></h3>
<p>First, we need to uplaod the input files (any text format file) into
Hadoop distributed file system (HDFS):</p>
<div class="highlight-python"><pre>[taklwu@i55 hadoop-0.20.203.0]$ bin/hadoop fs -put $HADOOP_HOME/Hadoop-WordCount/input/ input
[taklwu@i55 hadoop-0.20.203.0]$ bin/hadoop fs -ls input</pre>
</div>
<p>Here, $HADOOP_HOME/Hadoop-WordCount/input/ is the local directory where
the program inputs are stored. The second &#8220;input&#8221; represents the remote
destination directory on the HDFS.</p>
<p>After uploading the inputs into HDFS, run the WordCount program with the
following commands. We assume you have already compiled the word count
program.</p>
<div class="highlight-python"><pre>[taklwu@i55 hadoop-0.20.203.0]$ bin/hadoop jar $HADOOP_HOME/Hadoop-WordCount/wordcount.jar WordCount input output</pre>
</div>
<p>If Hadoop is running correctly, it will print hadoop running messages
similar to the following:</p>
<div class="highlight-python"><pre>WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.
11/11/02 18:34:46 INFO input.FileInputFormat: Total input paths to process : 1
11/11/02 18:34:46 INFO mapred.JobClient: Running job: job_201111021738_0001
11/11/02 18:34:47 INFO mapred.JobClient:  map 0% reduce 0%
11/11/02 18:35:01 INFO mapred.JobClient:  map 100% reduce 0%
11/11/02 18:35:13 INFO mapred.JobClient:  map 100% reduce 100%
11/11/02 18:35:18 INFO mapred.JobClient: Job complete: job_201111021738_0001
11/11/02 18:35:18 INFO mapred.JobClient: Counters: 25
...</pre>
</div>
</div>
<div class="section" id="id56">
<h3>3. Monitoring Hadoop<a class="headerlink" href="#id56" title="Permalink to this headline"></a></h3>
<p>We can also monitor the job status using lynx, a text browser, on i136
based Hadoop monitoring console. Assuming the Hadoop Jobtracker is
running on i55:9003:</p>
<div class="highlight-python"><pre>[taklwu@i136 ~]$ lynx i55:9003</pre>
</div>
</div>
<div class="section" id="check-the-result">
<h3>4.&nbsp;Check the result<a class="headerlink" href="#check-the-result" title="Permalink to this headline"></a></h3>
<p>After finishing the Job, please use the command to check the output:</p>
<div class="highlight-python"><pre>[taklwu@i55 ~]$ cd $HADOOP_HOME
[taklwu@i55 ~]$ bin/hadoop fs -ls output
[taklwu@i55 ~]$ bin/hadoop fs -cat output/*</pre>
</div>
<p>Here, &#8220;output&#8221; is the HDFS directory where the result stored. The result
will look like as following:</p>
<div class="highlight-python"><pre>you." 15
you; 1
you? 2
you?" 23
https://portal.futuregrid.org/salsahadoop-futuregrid-cloud-eucalyptusyoung 42</pre>
</div>
</div>
<div class="section" id="id57">
<h3>5.&nbsp;Finishing the Map-Reduce process<a class="headerlink" href="#id57" title="Permalink to this headline"></a></h3>
<p>After finishing the Job, please use the command to kill the HDFS and
Map-Reduce daemon:</p>
<div class="highlight-python"><pre>[taklwu@i55 hadoop-0.20.203.0]$ bin/stop-all.sh</pre>
</div>
</div>
</div>
<div class="section" id="using-twister-on-futuregrid">
<h2>Using Twister on FutureGrid<a class="headerlink" href="#using-twister-on-futuregrid" title="Permalink to this headline"></a></h2>
<p>PLEASE NOTE: THIS MANUAL PAGE IS A DRAFT, PLEASE PROVIDE FEEDBACK IN
THE COMMENT SECTION.</p>
<div class="section" id="what-is-twister">
<h3>What is Twister?<a class="headerlink" href="#what-is-twister" title="Permalink to this headline"></a></h3>
<p>MapReduce programming model has simplified the implementations of many
data parallel applications. The simplicity of the programming model and
the quality of services provided by many implementations of MapReduce
attract a lot of enthusiasm among parallel computing communities. From
the years of experience in applying MapReduce programming model to
various scientific applications we identified a set of extensions to the
programming model and improvements to its architecture that will expand
the applicability of MapReduce to more classes of
applications.&nbsp;<a class="reference external" href="http://www.iterativemapreduce.org/">Twister</a>&nbsp;is a
lightweight MapReduce runtime we have developed by incorporating these
enhancements.</p>
<p><a class="reference external" href="http://www.iterativemapreduce.org/">Twister</a> provides the following
features to support MapReduce computations. (Twister&nbsp;is developed as
part of&nbsp;<a class="reference external" href="http://www.cs.indiana.edu/%7Ejekanaya/">Jaliya
Ekanayake&#8217;s</a>&nbsp;Ph.D. research
and is supported by
the&nbsp;<strong>`S&nbsp;A&nbsp;L&nbsp;S&nbsp; &lt;http://salsahpc.indiana.edu/&gt;`__</strong><a href="#id58"><span class="problematic" id="id59">**</span></a><strong>**`A &lt;http://salsahpc.indiana.edu/&gt;`__</strong>Team
&#64;&nbsp;<a class="reference external" href="http://www.iub.edu/">IU</a>)</p>
<table border="1" class="docutils">
<colgroup>
<col width="6%" />
<col width="94%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><img alt="image105" src="http://www.iterativemapreduce.org/images/bullet.GIF" /></td>
<td>Distinction on static and variable data</td>
</tr>
<tr class="row-even"><td><img alt="image106" src="http://www.iterativemapreduce.org/images/bullet.GIF" /></td>
<td>Configurable long running (cacheable) map/reduce tasks</td>
</tr>
<tr class="row-odd"><td><img alt="image107" src="http://www.iterativemapreduce.org/images/bullet.GIF" /></td>
<td>Pub/sub messaging based communication/data transfers</td>
</tr>
<tr class="row-even"><td><img alt="image108" src="http://www.iterativemapreduce.org/images/bullet.GIF" /></td>
<td>Efficient support for Iterative MapReduce computations (extremely faster than<a class="reference external" href="http://hadoop.apache.org/">Hadoop</a>&nbsp;or&nbsp;<a class="reference external" href="http://research.microsoft.com/en-us/projects/DryadLINQ/">Dryad/DryadLINQ</a>)</td>
</tr>
<tr class="row-odd"><td><img alt="image109" src="http://www.iterativemapreduce.org/images/bullet.GIF" /></td>
<td>Combine phase to collect all reduce outputs</td>
</tr>
<tr class="row-even"><td><img alt="image110" src="http://www.iterativemapreduce.org/images/bullet.GIF" /></td>
<td>Data access via local disks</td>
</tr>
<tr class="row-odd"><td><img alt="image111" src="http://www.iterativemapreduce.org/images/bullet.GIF" /></td>
<td>Lightweight (~5600 lines of Java code)</td>
</tr>
<tr class="row-even"><td><img alt="image112" src="http://www.iterativemapreduce.org/images/bullet.GIF" /></td>
<td>Support for typical MapReduce computations</td>
</tr>
<tr class="row-odd"><td><img alt="image113" src="http://www.iterativemapreduce.org/images/bullet.GIF" /></td>
<td>Tools to manage data</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="image114">
<h3><img alt="image114" src="http://www.iterativemapreduce.org/images/imrmodel.png" /><a class="headerlink" href="#image114" title="Permalink to this headline"></a></h3>
<p>Iterative MapReduce programming model using Twister</p>
</div>
<div class="section" id="running-twister-on-futuregrid">
<h3>Running Twister on FutureGrid<a class="headerlink" href="#running-twister-on-futuregrid" title="Permalink to this headline"></a></h3>
<p>Twister&nbsp;can be run in various modes within FG either in FutureGrid HPC
and FutureGrid Cloud environment.</p>
<ul class="simple">
<li>Twister on FutureGrid<ul>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-futuregrid-hpc">Twister&nbsp;with&nbsp;FutureGrid
HPC</a><ul>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-futuregrid-hpc#HPC_Nodes">Get HPC compute
nodes</a></li>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-futuregrid-hpc#Twister_Conf">Twister
Configuration</a><ul>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-futuregrid-hpc#Twister_Conf_Download">Download Twister
0.9</a></li>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-futuregrid-hpc#Twister_Conf_Set">Set $TWISTER_HOME and
$JAVA_HOME</a></li>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-futuregrid-hpc#Twister_Conf_PowerMakeUp">Run
TwisterPowerMakeUp.sh</a></li>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-futuregrid-hpc#Twister_Conf_ActiveMQ">Download and start ActiveMQ on specific
nodes</a></li>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-futuregrid-hpc#Twister_Conf_Start">Start
Twister</a></li>
</ul>
</li>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-futuregrid-hpc#Verify">Verify Twister MapReduce Daemon
status</a></li>
</ul>
</li>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-futuregrid-cloud-eucalyptus">Twister&nbsp;with&nbsp;FutureGrid Cloud
Eucalyptus</a><ul>
<li><a class="reference external" href="../../twister-futuregrid-cloud-eucalyptus#VM_Nodes">Get VM compute
nodes</a><ul>
<li><a class="reference external" href="../../twister-futuregrid-cloud-eucalyptus#VM_Nodes_Set">VM Hostname
setting</a></li>
<li><a class="reference external" href="../../twister-futuregrid-cloud-eucalyptus#Euca_Disk">VM attached disk
configuration</a></li>
</ul>
</li>
<li><a class="reference external" href="../../twister-futuregrid-cloud-eucalyptus#Twister_Conf">Twister
Configuration</a><ul>
<li><a class="reference external" href="../../twister-futuregrid-cloud-eucalyptus#Twister_Conf_Download">Download Twister
0.9</a></li>
<li><a class="reference external" href="../../twister-futuregrid-cloud-eucalyptus#Twister_Conf_Set">Set $TWISTER_HOME,&nbsp; $JAVA_HOME and Worker
Nodes</a></li>
<li><a class="reference external" href="../../twister-futuregrid-cloud-eucalyptus#Twister_Conf_PowerMakeUp">Run
TwisterPowerMakeUp.sh</a></li>
<li><a class="reference external" href="../../twister-futuregrid-cloud-eucalyptus#Twister_Conf_ActiveMQ">Download and start ActiveMQ on specific
node</a></li>
<li><a class="reference external" href="../../twister-futuregrid-cloud-eucalyptus#Twister_Conf_Start">Start
Twister</a></li>
</ul>
</li>
<li><a class="reference external" href="../../twister-futuregrid-cloud-eucalyptus#Verify">Verify Twister MapReduce Daemon
status</a></li>
</ul>
</li>
<li>Run Twister Applications<ul>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-kmeans">Twister
Kmeans</a></li>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-blast">Twister Blast</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="papers-and-presentations">
<h3>Papers and Presentations<a class="headerlink" href="#papers-and-presentations" title="Permalink to this headline"></a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="3%" />
<col width="97%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><img alt="image121" src="http://www.iterativemapreduce.org/images/bullet.GIF" /></td>
<td>Jaliya Ekanayake, Hui Li, Bingjing Zhang, Thilina Gunarathne, Seung-Hee Bae, Judy Qiu, Geoffrey Fox,&nbsp;<a class="reference external" href="http://www.iterativemapreduce.org/hpdc-camera-ready-submission.pdf">Twister: A Runtime for Iterative MapReduce</a>,&#8221; The First International Workshop on MapReduce and its Applications (MAPREDUCE&#8216;10) - HPDC2010</td>
</tr>
<tr class="row-even"><td><img alt="image122" src="http://www.iterativemapreduce.org/images/bullet.GIF" /></td>
<td>Jaliya Ekanayake, (Advisor: Geoffrey Fox)&nbsp;<a class="reference external" href="http://grids.ucs.indiana.edu/ptliupages/publications/SC09-abstract-jaliya-ekanayake.pdf">Architecture and Performance of Runtime Environments for Data Intensive Scalable Computing</a>, Doctoral Showcase, SuperComputing2009. (<a class="reference external" href="http://www.slideshare.net/jaliyae/architecture-and-performance-of-runtime-environments-for-data-intensive-scalable-computing-2653554">Presentation</a>)</td>
</tr>
<tr class="row-odd"><td><img alt="image123" src="http://www.iterativemapreduce.org/images/bullet.GIF" /></td>
<td>Jaliya Ekanayake, Atilla Soner Balkir, Thilina Gunarathne, Geoffrey Fox, Christophe Poulain, Nelson Araujo, Roger Barga,&nbsp;<a class="reference external" href="http://grids.ucs.indiana.edu/ptliupages/publications/eScience09-camera-ready-submission.pdf">DryadLINQ for Scientific Analyses</a>, Fifth IEEE International Conference on e-Science (eScience2009), Oxford, UK.</td>
</tr>
<tr class="row-even"><td><img alt="image124" src="http://www.iterativemapreduce.org/images/bullet.GIF" /></td>
<td>Jaliya Ekanayake, Geoffrey Fox,&nbsp;<a class="reference external" href="http://grids.ucs.indiana.edu/ptliupages/publications/cloud_handbook_final-with-diagrams.pdf">High Performance Parallel Computing with Clouds and Cloud Technologies</a>, First International Conference on Cloud Computing (CloudComp09) Munich, Germany, 2009.</td>
</tr>
<tr class="row-odd"><td><img alt="image125" src="http://www.iterativemapreduce.org/images/bullet.GIF" /></td>
<td>Geoffrey Fox, Seung-Hee Bae, Jaliya Ekanayake, Xiaohong Qiu, and Huapeng Yuan,<a class="reference external" href="http://grids.ucs.indiana.edu/ptliupages/publications/CetraroWriteupJan09_v12.pdf">Parallel Data Mining from Multicore to Cloudy Grids</a>, High Performance Computing and Grids workshop, 2008.</td>
</tr>
<tr class="row-even"><td><img alt="image126" src="http://www.iterativemapreduce.org/images/bullet.GIF" /></td>
<td>Jaliya Ekanayake, Shrideep Pallickara, and Geoffrey Fox&nbsp;<a class="reference external" href="http://www.cs.indiana.edu/%7Ejekanaya/papers/eScience-final.pdf">MapReduce for Data Intensive Scientific Analysis</a>, Fourth IEEE International Conference on eScience, 2008, pp.277-284.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="twister-blast">
<h2>Twister Blast<a class="headerlink" href="#twister-blast" title="Permalink to this headline"></a></h2>
<dl class="docutils">
<dt>Number:</dt>
<dd>Author: Yang Ruan</dd>
</dl>
<p>Improvements:
Version: 0.1
Date: 2011-11-07</p>
<div class="section" id="id60">
<h3>Twister Blast<a class="headerlink" href="#id60" title="Permalink to this headline"></a></h3>
<p>BLAST (Basic Local Alignment Search Tool) is one of the most widely used
bioinformatics applications written in C++, and&nbsp;the version we are using
is v2.2.23. <a class="reference external" href="http://www.iterativemapreduce.org/">Twister</a>is an
iterative mapreduce framework which can be used both for iterative and
non-iterative applications. Twister Blast is an advanced Twister program
which helps Blast, a bioinformatics application, utilizes the Computing
Capability of Twister. With the flexibility of Twister run-time
environment, this application can run on a single machine, a cluster, or
Amazon EC2 cloud platform.</p>
<p>Twister-BLAST can divide original query file into small chunks, and
distribute them to all available computing nodes. Twister-BLAST manages
and schedules Map tasks to process each query chunk based on its
location. Output can also be collected by Twister-BLAST. Compared with
other parallel BLAST applications, Twister-BLAST is efficient and with
little overhead.</p>
<p>You can download the<a class="reference external" href="http://salsahpc.indiana.edu/tutorial/apps/twister-blast.tar.gz">Twister
Blast</a>
Source code and customized Blast program and Database archive
(<a class="reference external" href="http://salsahpc.indiana.edu/tutorial/apps/BlastProgramAndDB.tar.gz">BlastProgramAndDB.tar.gz</a>)
from <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/hadoopblast.html">Big Data for Science
tutorial</a>.</p>
<ul class="simple">
<li></li>
</ul>
</div>
<div class="section" id="id61">
<h3>Acknowledge<a class="headerlink" href="#id61" title="Permalink to this headline"></a></h3>
<p>This page was original designed by
<a class="reference external" href="http://salsahpc.indiana.edu/">SalsaHPC</a> group for <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/">Big Data for
Science Workshop</a>, you can see
the original pages <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/">here</a>.</p>
</div>
<div class="section" id="id62">
<h3>Requirement<a class="headerlink" href="#id62" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li>Login to FutureGrid Cluster and obtain compute nodes.
(<a class="reference external" href="../../salsahadoop-futuregrid-hpc#HPC_Nodes">HPC</a>/
<a class="reference external" href="../../salsahadoop-futuregrid-cloud-eucalyptus">Eucalyptus</a>)</li>
<li>Start Twister on compute nodes. (<a class="reference external" href="http://salsahpc.indiana.edu/tutorial/twister-intro.html">SalsaTwister
Tutorial</a>)</li>
<li>Download and unzip <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/apps/twister-blast.tar.gz">Twister
Blast</a>
Source code.</li>
<li>Download customized Blast binary and Database archive
<a class="reference external" href="http://salsahpc.indiana.edu/tutorial/apps/BlastProgramAndDB.tar.gz">BlastProgramAndDB.tar.gz</a></li>
<li>Linux command experience.</li>
</ol>
</div>
<div class="section" id="download-and-prepare-the">
<h3>1. Download and prepare the<a class="headerlink" href="#download-and-prepare-the" title="Permalink to this headline"></a></h3>
</div>
<div class="section" id="id63">
<h3>Twister-Blast<a class="headerlink" href="#id63" title="Permalink to this headline"></a></h3>
<p>First, Download and unzip the <a class="reference external" href="http://salsahpc.indiana.edu/tutorial/apps/twister-blast.tar.gz">Twister
Blast</a>
package (named as $TWISTER_BLAST_PROGRAM here), then copy&nbsp;the
unzipped $TWISTER_BLAST_PROGRAM/blast/dist/Twister-Blast.jar&nbsp;to the
$TWISTER_HOME/apps. Also, we download and unzip the blast program and
the database
<a class="reference external" href="http://salsahpc.indiana.edu/tutorial/apps/BlastProgramAndDB.tar.gz">here</a>,
and set $BLAST_HOME=/path/to/BlastProgramAndDB/. Go to
$TWISTER_BLAST_PROGRAM/blast/bin/, in <strong>twister_blast.properties</strong>,
set the BLAST+ execution command (execmd property)&nbsp; to the BLAST program
(blastx) under $BLAST_HOME/bin/. Execution options can be reset
according to users&#8217; needs. However, Input option (-query) and output
option (-out) are not set in execmd but in inop and outop in order to be
compatible with both BLAST+ and BLAST. Twister-BLAST will merge these
command options by itself when invoking BLAST+ parallel.
The execution command template inside<strong>twister_blast.properties</strong>
is given below.</p>
<div class="highlight-python"><pre>execmd = time /N/u/yangruan/Quarry/workflow/ncbi-blast-2.2.23+/bin/blastp -db /N/dc/scratch/yangruan/blast/db/cog/10k/cog.10000 -evalue 100 -max_target_seqs 1000000 -num_alignments 1000000 -outfmt 6 -seg no
inop = -query
outop = -out</pre>
</div>
</div>
<div class="section" id="prepare-twister-blast-input">
<h3>2. Prepare Twister-Blast input<a class="headerlink" href="#prepare-twister-blast-input" title="Permalink to this headline"></a></h3>
<p>Assume you have already download the input fasta file into some location
called [input file path]. Use the
$TWISTER_BLAST_PROGRAM/blast/bin/blastNewFileSpliter.sh to split the
input fasta file into multiple partitions. The parameters in as
following:</p>
<div class="highlight-python"><pre>args: [query_file] [sequence_count] [num_partition] [data_dir] [output_prefix] [output_map_file]</pre>
</div>
<ul class="simple">
<li>query_file: input fasta file</li>
<li>sequence_count: sequence count in the input fasta file</li>
<li>num_partition: number of partitions, this number should be larger or
equal to the total worker number started with twister</li>
<li>data_dir: The output folder of partitioned fasta files</li>
<li>output_prefix: The output prefix of partitioned fasta files</li>
<li>output_map_file: The file contains the information of all the
partitions width and height.</li>
</ul>
</div>
<div class="section" id="execute-twister-blast">
<h3>3. Execute Twister-Blast<a class="headerlink" href="#execute-twister-blast" title="Permalink to this headline"></a></h3>
<p>After deploying those required files onto file system, run the
twister-Blast program with the following commands:</p>
<div class="highlight-python"><pre>./blastNew.sh 128 /N/dc/scratch/yangruan/fasta/cog/10000/400/ input_ .fa 400 /N/dc/scratch/yangruan/blast/result/cog/10k/eval_100_400p/ blastOut_</pre>
</div>
<p>Here is the description of the above command:</p>
<div class="highlight-python"><pre>args:  [map number] [input folder] [input prefix] [input postfix (None for none)] [partition number] [output folder] [output prefix]</pre>
</div>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="79%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Parameter</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr class="row-even"><td>map number</td>
<td>The map task number (usually equals to the number of worker started)</td>
</tr>
<tr class="row-odd"><td>input folder</td>
<td>The folder of input fasta file partitions</td>
</tr>
<tr class="row-even"><td>input prefix</td>
<td>The prefix of input fasta file partitions</td>
</tr>
<tr class="row-odd"><td>input postfix</td>
<td>The postfix (file extension) of input fasta file partitions (default .fa)</td>
</tr>
<tr class="row-even"><td>partition number</td>
<td>The number of input fasta file partitions</td>
</tr>
<tr class="row-odd"><td>output folder</td>
<td>The folder to store output blast result</td>
</tr>
<tr class="row-even"><td>output prefix</td>
<td>The prefix of output blast result</td>
</tr>
</tbody>
</table>
<p>If Twister Blast is running correctly, it will print twister running
messages similar to the following:</p>
<div class="highlight-python"><pre>./blastNew.sh 128 /N/dc/scratch/yangruan/fasta/cog/10000/400/ input_ .fa 400 /N/dc/scratch/yangruan/blast/result/cog/10k/eval_100_400p/ blastOut_
time /N/u/yangruan/Quarry/workflow/ncbi-blast-2.2.23+/bin/blastp -db /N/dc/scratch/yangruan/blast/db/cog/10k/cog.10000 -evalue 100 -max_target_seqs 1000000 -num_alignments 1000000 -outfmt 6 -seg no
-query
-out
JobID: BlastNewac4d15a9-0997-11e1-81b4-5b7f60de01d2
Nov 7, 2011 11:24:43 PM org.apache.activemq.transport.failover.FailoverTransport doReconnect
INFO: Successfully connected to tcp://149.165.229.100:61616
0    [main] INFO  cgl.imr.client.TwisterDriver  - MapReduce computation termintated gracefully.
Total Time of BLAST : 28.12Seconds
2    [Thread-1] DEBUG cgl.imr.client.ShutdownHook  - Shutting down completed.</pre>
</div>
</div>
<div class="section" id="id64">
<h3>4.&nbsp;Finishing the Map-Reduce process<a class="headerlink" href="#id64" title="Permalink to this headline"></a></h3>
<p>After finishing the Job, please use the command to kill the Map-Reduce
daemon and broker:</p>
<div class="highlight-python"><pre>$TWISTER_HOME/bin/stop_twister.sh</pre>
</div>
</div>
</div>
<div class="section" id="eucalyptus-and-twister-on-futuregrid">
<h2>Eucalyptus and Twister on FutureGrid<a class="headerlink" href="#eucalyptus-and-twister-on-futuregrid" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="the-futuregrid-twister-tutorial">
<h2>The FutureGrid Twister Tutorial<a class="headerlink" href="#the-futuregrid-twister-tutorial" title="Permalink to this headline"></a></h2>
<p>SALSA Group
PTI&nbsp;Indiana University</p>
<p>This tutorial will show you how to use Twister under Eucalyptus on
India, FutureGrid.</p>
<p>Follow tutorial <a class="reference external" href="http://portal.futuregrid.org/tutorials/eucalyptus">Using Eucalyptus on
FutureGrid</a> to
learn how to install and use the Eucalyptus client tool to access
resources on India, FutureGrid.</p>
<p>This tool is a set of python scripts. They can provide a
pre-configured Twister environment, and also can terminate the
environment. Please
download the tool in the attachment below.</p>
<blockquote>
<div><ol class="upperroman simple" start="4">
<li>Start Twister Environment</li>
</ol>
</div></blockquote>
<hr class="docutils" />
<p>To start a Twister environment, execute the following program:</p>
<p>Here,</p>
<ul class="simple">
<li>-k is the user key name generated by the <strong>euca-add-keypair</strong> step in
the Eucalyptus tutorial.</li>
<li>-i &nbsp;is the private key .pem file path. It is also&nbsp;generated in the
<strong>euca-add-keypair</strong>&nbsp;step in the Eucalyptus tutorial.</li>
<li>-n is the number of instances for starting.</li>
<li>-t &nbsp;is the type of image.</li>
</ul>
<p>The following is an execution example:
<img alt="image127" src="https://portal.futuregrid.orghttps://portal.futuregrid.org/sites/default/files/u192/start_twister.jpg" /></p>
<p>Once the script is executed, the user can get a prepared Twister
environment.
Then, the user can follow the instructions provided by
<strong>fg_euca_start_twister.py</strong>&nbsp;to start ActiveMQ on the assigned node,
and also start the Twister environment (could be on any node just
applied).</p>
<p>To terminate a Twister environment, execute the following command:</p>
<p>Log into the node assigned for ActiveMQ broker.</p>
<div class="highlight-python"><pre>$ cd /opt/Twister/samples/kmeans
$ ant
$ cd ../../lib
$ mv Twister-Kmeans-0.9.jar ../apps/
$ cd ../bin/
$ chmod a+x twister.sh
$ ./twister.sh cpj ../apps/Twister-Kmeans-0.9.jar</pre>
</div>
<p>Open two terminals and log into the node mentioned above. One is for
starting ActiveMQ; the other is for starting Twister.</p>
<p>In Terminal 1:</p>
<div class="highlight-python"><pre>$ cd /opt/apache-activemq-5.4.2/bin/
$ activemq console</pre>
</div>
<p>In Terminal 2:</p>
<div class="highlight-python"><pre>$ cd /opt/Twister/bin
$ ./start_twister.sh</pre>
</div>
<p>Open another terminal, and create a folder for operating kmeans data:</p>
<div class="highlight-python"><pre>$ cd /opt/Twister/bin
$ ./twishter.sh mkdir kmeans</pre>
</div>
<p>^</p>
<p>^</p>
<p>Open a new terminal:</p>
<div class="highlight-python"><pre>$ cd /opt/Twister/samples/kmeans/bin/
$./gen_data.sh init_clusters.txt 2 3 /kmeans km_data 3 30000</pre>
</div>
<p>In the terminal used in Step 3, do the following:</p>
<div class="highlight-python"><pre>$ ./create_partition_file.sh kmeans km ../samples/kmeans/bin/p.pf</pre>
</div>
<p>Back in the terminal used in Step 4, do the following:</p>
<div class="highlight-python"><pre>$ ./run_kmeans.sh init_clusters.txt 3 p.pf</pre>
</div>
<p>The output is as follows:</p>
<p><img alt="image128" src="https://portal.futuregrid.orghttps://portal.futuregrid.org/sites/default/files/resize/u192/twister_kmeans-906x257.jpg" /></p>
<table border="1" class="docutils">
<colgroup>
<col width="90%" />
<col width="10%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Attachment</th>
<th class="head">Size</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference external" href="https://portal.futuregrid.org/sites/default/files/fgeucatwister.zip">fgeucatwister.zip</a></td>
<td>4.38 KB</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="using-pegasus-on-futuregrid">
<h2>Using Pegasus on FutureGrid<a class="headerlink" href="#using-pegasus-on-futuregrid" title="Permalink to this headline"></a></h2>
<div class="section" id="about-pegasus">
<h3>About Pegasus<a class="headerlink" href="#about-pegasus" title="Permalink to this headline"></a></h3>
<p><a class="reference external" href="http://pegasus.isi.edu">Pegasus WMS</a> is a configurable system for
mapping and executing abstract application workflows over a wide range
of execution environments including a laptop, a campus cluster, a Grid,
or a commercial or academic cloud. One Pegasus workflow can run on a
single system or across a heterogeneous set of resources. Pegasus can
run workflows ranging from just a few computational tasks up to 1
million.</p>
<p>Pegasus has a number of features that contribute to its usability and
effectiveness, as explained in detail in the tutorial and the <a class="reference external" href="https://pegasus.isi.edu/wms/docs/4.0/about.php#overview">Pegasus
online
documentation</a>.
Among the key feature rank abstract workflow portability between
environments, features to better control workflow performance and
resource utilization, scalability, data provenance tracking, data
management, error recovery and multiple operating environments not
limited to clouds.</p>
<p>The main mode of operation of Pegasus on FutureGrid is the usage of IaaS
clouds, as provided by Nimbus, OpenStack, Eucalyptus and bare-metal
(future).</p>
<p>If you want to find out more about Pegasus WMS, please <a class="reference external" href="http://pegasus.isi.edu">visit our web
site.</a> The liaison for FutureGrid is Jens
Vckler. He is available as &#8220;Expert&#8221; on the Portal. If you have general
questions concerning Pegasus, not related to FutureGrid, you may want to
contact</p>
<p>pegasus-support &lt;pegasus-support at isi dot edu&gt;</p>
</div>
<div class="section" id="the-pegasus-run-time-cloud-architecture">
<h3>The Pegasus Run-Time Cloud Architecture<a class="headerlink" href="#the-pegasus-run-time-cloud-architecture" title="Permalink to this headline"></a></h3>
<p>Pegasus uses the <a class="reference external" href="http://research.cs.wisc.edu/condor/">Condor high-throughput scheduling
system</a> to provide the work of
executing a plan created by Pegasus from the abstract workflow
description.</p>
<p><img alt="Cloud site sample layouts." src="https://pegasus.isi.edu/wms/docs/4.0/images/fg-pwms-prefio.3.png" />
<strong>Figure:</strong> Cloud Site Layouts example.</p>
<p>The figure above shows two cloud sites, typically two distinct IaaS
clouds, with two virtual machines (VMs) running in each site.
Effectively, it shows a sample layout for sky computing (as in: multiple
clouds) as supported by Pegasus. At this point, it is up to the user to
provision the remote resources with a proper VM image that includes a
Condor <em>startd</em> and proper Condor configuration to report back to a
Condor <em>collector</em> that the Condor <em>schedd</em> has access to.</p>
<p>The submit host is the point where a user submits Pegasus workflows for
execution. In this discussion, the <em>submit host</em> (SH) is located
logically external to the cloud provider(s) in order to bridge multiple
clouds. In contrast, the tutorial uses a submit host that is located
within a single cloud site. The site where the SH is located typically
runs a Condor <em>collector</em> to gather resource announcements, or is part
of a larger Condor pool that collects these announcement. Condor makes
the remote resources available to the submit host&#8217;s Condor installation.</p>
<p>The figure above shows the way Pegasus WMS is deployed in cloud
computing resources, ignoring how these resources were provisioned. The
provisioning request shows multiple resources per provisioning request.</p>
<p>The provisioning broker &#8211; Nimbus, Eucalyptus or EC2 &#8211; is responsible
to allocate and set up the resources. Please note that each provisioning
broker comes with its own set of client tools that may be executed
either on your laptop or desktop in front of you, or from a FutureGrid
login node. The FutureGrid tutorials for the various IaaS services have
details on their client tools and usage.</p>
<p>For a multi-node request, the worker nodes often require access to a
form of shared data storage. Concretely, either a POSIX-compliant shared
file system (e.g. NFS, PVFS) is available to the nodes, or can be
brought up for the lifetime of the application workflow. The task steps
of the application workflow facilitate shared file systems to exchange
intermediary results between tasks on the same cloud site. Pegasus also
supports an S3 data mode for the application workflow data staging. As
of Pegasus 4.0, it more easily supports non-shared file system execution
where each worker node uses only its own storage.</p>
<p>The initial stage-in and final stage-out of application data into and
out of the node set is part of any Pegasus-planned workflow. Several
configuration options exist in Pegasus to deal with the dynamics of push
and pull of data, and when to stage data. In many use-cases, some form
of external access to or from the shared file system that is visible to
the application workflow is required to facilitate successful data
staging. However, Pegasus is prepared to deal with a set of boundary
cases.</p>
<p>The data server in the figure is shown at the submit host. This is <em>not
a strict requirement</em>. The data server for consumed data (input data)
and data products (output data) may both be different and external to
the submit host.</p>
<p>Once worker nodes (resources) begin appearing in the pool managed by the
submit host&#8217;s Condor <em>collector</em>, the application workflow, as planned
by Pegasus, can be submitted to Condor. A Condor <em>DAGMan</em> will manage
the application workflow execution. Pegasus run-time tools obtain
timing-, performance and provenance information as the application
workflow is executed. In the cloud and FutureGrid scenario, it is the
user&#8217;s responsibility to de-provision the allocated resources.</p>
<p>In the figure, the cloud resources on the right side are assumed to have
uninhibited outside connectivity. This enables the Condor I/O to
communicate directly with the resources. The right side includes a setup
where the worker nodes use all private IP, but have out-going
connectivity and a NAT router to talk to the internet. The <em>Condor
connection broker</em> (CCB) facilitates this setup almost effortlessly.</p>
<p>The left side shows a more difficult setup where the connectivity is
fully firewalled without any connectivity except to in-site nodes. In
this case, a proxy server process, the <em>generic connection broker</em>
(GCB), needs to be set up in the de-militarized zone (DMZ) of the cloud
site&#8217;s firewall to facilitate Condor I/O between the submit host and
worker nodes.</p>
<p>If the cloud supports data storage servers, Pegasus 4.0 is starting to
support workflows that require staging in two steps: Consumed data is
first staged to a data server in the remote site&#8217;s DMZ, and then a
second staging task moves the data from the data server to the worker
node where the job runs. For staging out, data needs to be first staged
from the job&#8217;s worker node to the site&#8217;s data server, and possibly from
there to another data server external to the site. Pegasus is capable to
plan both steps: Normal staging to a remote site&#8217;s data server, and the
worker-node staging from and to the site&#8217;s data server as part of the
job. We are working on expanding the current code to support a more
generic set in the newly released Pegasus 4.0.</p>
</div>
<div class="section" id="id65">
<h3>Using Pegasus on FutureGrid<a class="headerlink" href="#id65" title="Permalink to this headline"></a></h3>
<p>The Pegasus tutorial for FutureGrid is forthcoming shortly. You are
welcome to <a class="reference external" href="http://pegasus.isi.edu/futuregrid/tutorials/">review the &#8220;Using Pegasus In FutureGrid&#8221;
tutorial</a> which makes
use of one of the following Virtual Machines:</p>
<table border="1" class="docutils">
<colgroup>
<col width="10%" />
<col width="11%" />
<col width="32%" />
<col width="10%" />
<col width="8%" />
<col width="10%" />
<col width="10%" />
<col width="9%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>IaaS</strong></td>
<td><strong>Resource</strong></td>
<td><strong>Image Name</strong></td>
<td><strong>CentOS</strong></td>
<td><strong>PWMS</strong></td>
<td><strong>Condor</strong></td>
<td><strong>Globus</strong></td>
<td><strong>Mntge</strong></td>
</tr>
<tr class="row-even"><td>Nimbus</td>
<td>alamo</td>
<td>pegasus-tutorial.x64.kvm.gz</td>
<td>5.7</td>
<td>4.0.0</td>
<td>7.6.6</td>
<td>5.2.0</td>
<td>3.3p2</td>
</tr>
<tr class="row-odd"><td>Nimbus</td>
<td>hotel</td>
<td>pegasus-tutorial.x64.gz</td>
<td>5.7</td>
<td>4.0.0</td>
<td>7.6.6</td>
<td>5.2.0</td>
<td>3.3p2</td>
</tr>
<tr class="row-even"><td>Nimbus</td>
<td>foxtrot</td>
<td>pegasus-tutorial.x64.gz</td>
<td>5.7</td>
<td>4.0.0</td>
<td>7.6.6</td>
<td>5.2.0</td>
<td>3.3p2</td>
</tr>
<tr class="row-odd"><td>Nimbus</td>
<td>sierra</td>
<td>pegasus-tutorial.x64.gz</td>
<td>5.7</td>
<td>4.0.0</td>
<td>7.6.6</td>
<td>5.2.0</td>
<td>3.3p2</td>
</tr>
<tr class="row-even"><td>Eucalyptus</td>
<td>sierra</td>
<td>tutorial/pegasus-tutorial.x64.manifest.xml</td>
<td>5.8</td>
<td>4.0.0</td>
<td>7.6.6</td>
<td>5.2.0</td>
<td>3.3p2</td>
</tr>
<tr class="row-odd"><td>Eucalyptus</td>
<td>india</td>
<td>pegasus/pegasus-tutorial.x64.manifest.xml</td>
<td>5.8</td>
<td>4.0.0</td>
<td>7.6.6</td>
<td>5.2.0</td>
<td>3.3p2</td>
</tr>
<tr class="row-even"><td>OpenStack</td>
<td>india</td>
<td>pegasus/pegasus-tutorial.x64.manifest.xml</td>
<td>5.8</td>
<td>4.0.0</td>
<td>7.6.6</td>
<td>5.2.0</td>
<td>3.3p2</td>
</tr>
</tbody>
</table>
<p>The table above shows the available Pegasus Virtual Machines you can use
to try out Pegasus. At this point, the Nimbus-based VMs are are
preferred.</p>
</div>
</div>
<div class="section" id="management-services">
<h2>Management Services<a class="headerlink" href="#management-services" title="Permalink to this headline"></a></h2>
<p>FutureGrid contains a number of interresting management services. This
includes image management services to deploy and provision images onto
bare metal or virtualized machines as well as experiment management that
allows the creation of easy to use workflows to run repeatable
experiments on FutureGrid. These services are curently under development
and you are welcome to join the development teams by contacting
<a class="reference external" href="mailto:laszewski&#37;&#52;&#48;gmail&#46;com">laszewski<span>&#64;</span>gmail<span>&#46;</span>com</a></p>
</div>
<div class="section" id="nimbus-phantom">
<h2>Nimbus Phantom<a class="headerlink" href="#nimbus-phantom" title="Permalink to this headline"></a></h2>
<div class="line-block">
<div class="line">Nimbus Phantom is a hosted service, running on FutureGrid, that makes</div>
</div>
<p>it easy to leverage on-demand resources provided by infrastructure
clouds. Phantom allows the user to deploy a set of virtual machines over
multiple private, community, and commercial clouds and then
automatically grows or shrinks this set based on policies defined by the
user. This elastic set of virtual machines can then be used to implement
scalable and highly available services. An example of such service is a
caching service that stands up more workers on more resources as the
number of requests to the service increases. Another example is a
scheduler that grows its set of resources as demand grows.
|
Currently Phantom works with all FutureGrid Nimbus and OpenStack
clouds as well as Amazon and the XSEDE wispy cloud (the only XSEDE cloud
for now). A user can access it via two types of clients: an easy to use
web application and a scripting client. The scripting client is the boto
autoscale client as Phantom currently implements Amazon Autscaling API 
so you can think of it as Amazon Autoscale for FutureGrid clouds that
also allows for cloudburst to XSEDE and commercial clouds and is easy to
extend with your own policies and sensors.
|
The simplest scenario for using Phantom is as a gateway for deploying
and monitoring groups of virtual machines spread over multiple
FutureGrid clouds. In a more complex scenario you can use it to
cloudburst from FutureGrid clouds to Amazon. Finally, you can use it to
explore policies that will automate cloudbursting and VM allocations
between multiple clouds.
|
For more information and/or to use the service go to
<a class="reference external" href="http://www.nimbusproject.org/phantom">www.nimbusproject.org/phantom</a>.
It should take no more than 10-15 minutes to start your own VMs.</p>
<ul class="simple">
<li><a class="reference external" href="https://phantom.nimbusproject.org/accounts/login/?next=/">Phantom web
interface</a></li>
<li><a class="reference external" href="http://www.nimbusproject.org/files/keahey_wcs_ocs_2012.pdf">Phantom
publication</a></li>
</ul>
</div>
<div class="section" id="precip-pegasus-repeatable-experiments-for-the-cloud-in-python">
<h2>Precip - Pegasus Repeatable Experiments for the Cloud in Python<a class="headerlink" href="#precip-pegasus-repeatable-experiments-for-the-cloud-in-python" title="Permalink to this headline"></a></h2>
<div class="section" id="id66">
<h3>Overview<a class="headerlink" href="#id66" title="Permalink to this headline"></a></h3>
<p>Precip is a flexible exeperiment management API for running experiments
on clouds. Precip was developed for use on FutureGrid infrastructures
such as OpenStack, Eucalyptus (&gt;=3.2), Nimbus, and at the same time
commercial clouds such as Amazon EC2. The API allows you to easily
provision resources, which you can then can run commands on and copy
files to/from subsets of instances identified by tags. The goal of the
API is to be flexible and simple to use in Python scripts to control
your experiments.</p>
<p>The API does not require any special images, which makes it easy to get
going. Any basic Linux image will work. More complex images can be used
if your experiment requires so, or you can use the experiment API to run
bootstrap scripts on the images to install/configure required software.</p>
<p>A concept which simplfies interacting with the API is instance tagging.
When you start an instance, you can add arbitrary tags to it. The
instance also gets a set of default tags. API methods such as running a
remote command, or copying files, all use tags for specify which
instances you want to target.</p>
<p>Precip also handles ssh keys and security groups automatically. This is
done to make sure the experiment management is not interfering with your
existing cloud setup. The first time you use Precip, a directory will be
created called ~/.precip. Inside this directory, a ssh keypair will be
created and used for accessing instances. On clouds which supports it,
the keypair is automatically registered as &#8216;precip&#8217;, and a &#8216;precip&#8217;
security group is created. If your experiement requires more ports to be
open you can use the cloud interface to add those ports to the precip
security group.</p>
<p>Precip is a fairly new API, and if you have questions or suggestions for
improvements, please contact
<a class="reference external" href="mailto:pegasus-support&#37;&#52;&#48;isi&#46;edu">pegasus-support<span>&#64;</span>isi<span>&#46;</span>edu</a></p>
</div>
<div class="section" id="id67">
<h3>Installation<a class="headerlink" href="#id67" title="Permalink to this headline"></a></h3>
<div class="line-block">
<div class="line">If you want to use the India or Sierra FutureGrid resources to manage</div>
</div>
<p>your experiment, Precip is available on the interactive logins nodes via
modules: module load precip/0.1
|
You can also install Precip on your own machine. Prerequisites are
the Paramiko and Boto Python modules. The Python source package and RPMs
are available at:
<a class="reference external" href="http://pegasus.isi.edu/static/precip/software/">http://pegasus.isi.edu/static/precip/software/</a></p>
</div>
<div class="section" id="api">
<h3>API<a class="headerlink" href="#api" title="Permalink to this headline"></a></h3>
<dl class="docutils">
<dt><strong>provision(image_id, instance_type=&#8217;m1.small&#8217;, count=1, tags=None)</strong></dt>
<dd><p class="first">Provision a new instance. Note that this method starts the
provisioning cycle, but does not block for the instance to finish
booting. For blocking on instance creation/booting, see wait()</p>
<p>Parameters:</p>
<ul class="last simple">
<li><strong>image_id</strong> - the id of the image to instanciate</li>
<li><strong>instance_type</strong> - the type of instance. This is infrastructure
specific, but usually follows the Amazon EC2 model with m1.small,
m1.large, and so on.</li>
<li><strong>count</strong> - number of instances to create</li>
<li><strong>tags</strong> - these are used to manipulate the instance later. Use
this to create logical groups of your instances.</li>
</ul>
</dd>
<dt><strong>wait(tags=[], timeout=600)</strong></dt>
<dd><p class="first">Barrier for all instances matching the tags argument. This method
will block until the instances have finish booting and are
accessible via their external hostnames.</p>
<p>Parameters:</p>
<ul class="last simple">
<li><strong>tags</strong> - tags specifying the subset of instances to block on.
The default value is [] which means wait for all instances.</li>
<li><strong>timeout</strong> - timeout in seconds for the instances to boot. If
the timeout is reached, an ExperimentException is raised. The
default is 600 seconds.</li>
</ul>
</dd>
<dt><strong>deprovision(tags)</strong></dt>
<dd><p class="first">Deprovisions (terminates) instances matching the tags argument</p>
<p>Parameters:</p>
<ul class="last simple">
<li><strong>tags</strong> - tags specifying the subset of instances to
deprovision.</li>
</ul>
</dd>
<dt><strong>list(tags)</strong></dt>
<dd><p class="first">Returns a list of details about the instances matching the tags. The
details include instance id, hostnames, and tags.</p>
<p>Parameters:</p>
<ul class="simple">
<li><strong>tags</strong> - tags specifying the subset of instances to give
information on. If you want details on all current instances, use
[].</li>
</ul>
<p>Returns:</p>
<ul class="last simple">
<li>List of dictionaries, one for each instance.</li>
</ul>
</dd>
<dt><strong>get_public_hostnames(tags)</strong></dt>
<dd><p class="first">Provides a list of public hostnames for the instances matching the
tags. The public hostnames can be provided to other instances in
order to let the instances know about eachother.</p>
<p>Parameters:</p>
<ul class="simple">
<li><strong>tags</strong> - tags specifying the subset of instances.</li>
</ul>
<p>Returns:</p>
<ul class="last simple">
<li>A list of public hostnames</li>
</ul>
</dd>
<dt><strong>get_private_hostnames(tags)</strong></dt>
<dd><p class="first">Provides a list of private hostnames for the instances matching the
tags. The private hostnames can be provided to other instances in
order to let the instances know about eachother.</p>
<p>Parameters:</p>
<ul class="simple">
<li><strong>tags</strong> - tags specifying the subset of instances.</li>
</ul>
<p>Returns:</p>
<ul class="last simple">
<li>A list of private hostnames</li>
</ul>
</dd>
<dt><strong>get(tags, remote_path, local_path, user=&#8221;root&#8221;)</strong></dt>
<dd><p class="first">Transfers a file from a set of remote machines matching the tags,
and stores the file locally. If more than one instance matches the
tags, an instance id will be appended to the local_path.</p>
<p>Parameters:</p>
<ul class="last simple">
<li><strong>tags</strong> - these are used to manipulate the instance later. Use
this to create logical groups of your instances.</li>
<li><strong>remote_path</strong> - the path of the file on the remote instance</li>
<li><strong>local_path</strong> - the local path to tranfer to</li>
<li><strong>user</strong> - remote user. If not specified, the default is &#8216;root&#8217;</li>
</ul>
</dd>
<dt><strong>put(tags, local_path, remote_path, user=&#8221;root&#8221;)</strong></dt>
<dd><p class="first">Transfers a local file to a set of remote machines matching the
tags.</p>
<p>Parameters:</p>
<ul class="last simple">
<li><strong>tags</strong> - these are used to manipulate the instance later. Use
this to create logical groups of your instances.</li>
<li><strong>local_path</strong> - the local path to tranfer from</li>
<li><strong>remote_path</strong> - the path on the remote instance to store the
file as</li>
<li><strong>user</strong> - remote user. If not specified, the default is &#8216;root&#8217;</li>
</ul>
</dd>
<dt><strong>run(tags, cmd, user=&#8221;root&#8221;, check_exit_code=True)</strong></dt>
<dd><p class="first">Runs a command on the instances matches the tags. The commands are
run in series, on one instance after the other.</p>
<p>Parameters:</p>
<ul class="simple">
<li><strong>tags</strong> - these are used to manipulate the instance later. Use
this to create logical groups of your instances.</li>
<li><strong>cmd</strong> - the command to run</li>
<li><strong>user</strong> - remote user. If not specified, the default is &#8216;root&#8217;.
If you need to run commands as another user, you will have to
make sure that user accepts the ssh key in ~/.precip/</li>
<li><strong>check_exit_code</strong> - If set to True (default), commands
returning non-zero exit codes will result in a
ExperimentException being raised.</li>
</ul>
<p>Returns:</p>
<ul class="last simple">
<li>A list of lists, containing exit_code[], stdout[] and stderr[]
for the commands run</li>
</ul>
</dd>
</dl>
<p><strong>copy_and_run(tags, local_script, args=[], user=&#8221;root&#8221;,
check_exit_code=True)</strong></p>
<blockquote>
<div><p>Copies a script from the local machine to the remote instances and
executes the script. The script is run in series, on one instance
after the other.</p>
<p>Parameters:</p>
<ul class="simple">
<li><strong>tags</strong> - these are used to manipulate the instance later. Use
this to create logical groups of your instances.</li>
<li><strong>local_script</strong> - the local script to run</li>
<li><strong>args</strong> - arguments for the script</li>
<li><strong>user</strong> - remote user. If not specified, the default is &#8216;root&#8217;.
If you need to run commands as another user, you will have to
make sure that user accepts the ssh key in ~/.precip/</li>
<li><strong>check_exit_code</strong> - If set to True (default), commands
returning non-zero exit codes will result in a
ExperimentException being raised.</li>
</ul>
<p>Returns:</p>
<ul class="simple">
<li>A list of lists, containing exit_code[], stdout[] and stderr[]
for the commands run</li>
</ul>
</div></blockquote>
<p>The basic methods above are standard across all the Cloud
infrastructures. What is different is the constructors as each
infrastructure handles initialization a little bit different. For
example, to create a new OpenStack using the EC2_* environment
provided automatically by FutureGrid:</p>
<p>i For Amazon EC2, you have to specify region, endpoint, and
access/secret keys. Note that it is not required to use environment
variables for your credentials, but seperating the crenditals from the
code prevents the credentials from being check in to source control
systems.</p>
</div>
<div class="section" id="id68">
<h3>Examples<a class="headerlink" href="#id68" title="Permalink to this headline"></a></h3>
<div class="section" id="hello-world">
<h4>Hello World<a class="headerlink" href="#hello-world" title="Permalink to this headline"></a></h4>
</div>
<div class="section" id="resources-from-mulitple-infrastructures">
<h4>Resources from mulitple infrastructures<a class="headerlink" href="#resources-from-mulitple-infrastructures" title="Permalink to this headline"></a></h4>
</div>
<div class="section" id="setting-up-a-condor-pool-and-run-a-pegasus-workflow">
<h4>Setting up a Condor pool and run a Pegasus workflow<a class="headerlink" href="#setting-up-a-condor-pool-and-run-a-pegasus-workflow" title="Permalink to this headline"></a></h4>
<p>This is a more complex example in which a small Condor pool is set up
and then a Pegasus workflow is run and benchmarked. The Precip script is
similar to what we have seen before, but it has two groups of instances,
one master acting as the Condor central manager, and a set of Condor
worker nodes.</p>
<p>We also need a bootstrap.sh which sets up the instances:</p>
</div>
</div>
</div>
<div class="section" id="cloudinit-d">
<h2>cloudinit.d<a class="headerlink" href="#cloudinit-d" title="Permalink to this headline"></a></h2>
<p>cloudinit.d is a tool designed for launching, controlling, and
monitoring complex environments in the cloud.</p>
<p>Its most important feature is repeatable, one-click, deployment of
sets of VMs configured with launch plans. &nbsp;These sets of VMs can be
deployed over multiple clouds (Eucalyptus, Nimbus, OpenStack, and Amazon
EC2 are currently supported) as well as include non-virtualized
resources. Like the Unix init.d process, cloudinit.d can manage
dependencies between deployed VMs. It also provides mechanisms for
testing, monitoring, and repairing a launch.</p>
<p>For more information about cloudinit.d see our&nbsp;<a class="reference external" href="http://www.nimbusproject.org/files/cloudinitd_tg11_submit3c.pdf">Teragrid 2011
paper</a>.&nbsp;For
repeatable experiment management with cloudinit.d read the&nbsp;<a class="reference external" href="http://www.nimbusproject.org/downloads/Supporting_Experimental_Computer_Science_final_draft.pdf">report
on</a>&nbsp;<a class="reference external" href="http://www.nimbusproject.org/downloads/Supporting_Experimental_Computer_Science_final_draft.pdf">support
for experimental computer
science</a>.</p>
</div>
<div class="section" id="grid-services">
<h2>Grid Services<a class="headerlink" href="#grid-services" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="unicore">
<h2>Unicore<a class="headerlink" href="#unicore" title="Permalink to this headline"></a></h2>
<p>This page is maintained by Karolina Sarnowska-Upton from the University
of Virginia.</p>
<p><img alt="image130" src="https://portal.futuregrid.org/sites/default/files/u30/fg-logo-md.gif" /></p>
<hr class="docutils" />
<dl class="docutils">
<dt>UNICORE 6 on FutureGrid</dt>
<dd>User Manual</dd>
</dl>
<hr class="docutils" />
<p>Author: Karolina Sarnowska-Upton, University of Virginia
Version: 1.0
Today&#8217;s Date: 2010-12-21
Last Revision: 2011-11-28</p>
</div>
<div class="section" id="id69">
<h2>Introduction<a class="headerlink" href="#id69" title="Permalink to this headline"></a></h2>
<p><a class="reference external" href="http://www.unicore.eu/download/unicore6%A0">UNICORE 6</a> has been
deployed at various FutureGrid sites. This tutorial explains how to
connect to the existing FutureGrid UNICORE endpoints from other grid
middleware platforms or a UNICORE Commandline Client (UCC) as well as
how runs jobs on UNICORE sites and how to deploy a new UNICORE grid.</p>
</div>
<div class="section" id="what-is-unicore">
<h2>What is UNICORE?<a class="headerlink" href="#what-is-unicore" title="Permalink to this headline"></a></h2>
<p>UNICORE (Uniform Interface to Computing Resources) is a Grid middleware
system. Listed below are the key principles of the UNICORE design. More
information about UNICORE can be found at
<a class="reference external" href="http://www.unicore.eu">http://www.unicore.eu</a>.</p>
<ul class="simple">
<li>Open source under BSD license.</li>
<li>Standards-based, conforming to the latest standards from the Open
Grid Forum (OGF), W3C, OASIS, and IETF, in particular the Open Grid
Services Architecture (OGSA) and the Web Services Resource Framework
(WS-RF 1.2).</li>
<li>Open and extensible realized with a modern Service- Oriented
Architecture (SOA), which allows easily replacement of particular
components with others.</li>
<li>Interoperable with other Grid technologies to enable a coupling of
Grid infrastructures or the users needs</li>
<li>Seamless, secure, and intuitive following a vertical, end-to-end
approach and offering components at all levels of a modern Grid
architecture from intuitive user interfaces down to the resource
level. Like previous versions UNICORE 6 seamlessly integrates in
existing environments.</li>
<li>Mature security mechanisms adequate for the use in supercomputing
environments and Grid infrastructures. X.509 certificates form the
basis for authentication and authorization, enhanced with a support
for proxy certificates and virtual organizations (VO) based access
control.</li>
<li>Workflow support tightly integrated into the stack while being
extensible in order to use different workflow languages and engines
for domain-specific usage.</li>
<li>Application integration mechanisms on the client, services and
resource level for a tight integration of various types of
applications from the scientific and industrial domain.</li>
<li>Different clients serving the needs of various scientific
communities, e.g. graphical clients to define complex workflows,
command line tool, web based access.</li>
<li>Quick and simple to install and configure to address requirements
from operational teams and to lower the barrier of adopting Grid
technologies. Similar the configuration of the various services and
components is easy to handle.</li>
<li>Various operating and batch systems are supported on all layers, i.e.
clients, services and systems; Windows, MacOS, Linux, and Unix
systems as well as different batch systems are supported such as
LoadLeveler, Torque, SLURM, LSF, OpenCCS, etc.</li>
<li>Implemented in Java to achieve platform independence.</li>
</ul>
</div>
<div class="section" id="connecting-to-the-unicore-bes-endpoints-from-other-grid-middleware-clients">
<h2>Connecting to the UNICORE BES Endpoints From Other Grid Middleware Clients<a class="headerlink" href="#connecting-to-the-unicore-bes-endpoints-from-other-grid-middleware-clients" title="Permalink to this headline"></a></h2>
<p>Two UNICORE BES endpoints have been deployed on FutureGrid for
interoperability testing. One endpoint is located on Sierra and the
other is located on India. This section contains the information needed
for other grid middleware platforms to connect to the UNICORE BES
endpoints.</p>
<div class="section" id="india-endpoint-info-currently-unavailable">
<h3>India Endpoint Info &lt;currently unavailable&gt;<a class="headerlink" href="#india-endpoint-info-currently-unavailable" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><strong>Endpoint URL</strong>:
<a class="reference external" href="https://149.165.146.134:8081/DEMO-SITE/services/">https://149.165.146.134:8081/DEMO-SITE/services/</a>BESFactory?res=default_bes_factory</li>
<li><strong>Security</strong>: configured for username/password authentication;
<a class="reference external" href="mailto:karolina&#37;&#52;&#48;virginia&#46;edu">Email</a> for username token and CA
cert.</li>
<li><strong>OS</strong>: Red Hat Enterprise Linux Server release 5.5</li>
<li><strong>Arch</strong>: x86_64</li>
<li><strong>Cores</strong>: 8 (Jobs submitted directly to machine - i.e. not through
PBS queue)</li>
</ul>
</div>
<div class="section" id="sierra-endpoint-info">
<h3>Sierra Endpoint Info<a class="headerlink" href="#sierra-endpoint-info" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><strong>Endpoint URL</strong>:
<a class="reference external" href="https://198.202.120.85:8081/DEMO-SITE/services/">https://198.202.120.85:8081/DEMO-SITE/services/</a>BESFactory?res=default_bes_factory</li>
<li><strong>Security</strong>: configured for X-509 based mutual client
authentication; <a class="reference external" href="mailto:karolina&#37;&#52;&#48;virginia&#46;edu">Email</a> with X-509
cert and for CA cert.</li>
<li><strong>OS</strong>: Red Hat Enterprise Linux Server release 5.5</li>
<li><strong>Arch</strong>: x86_64</li>
<li><strong>Cores</strong>: 320 (Jobs submitted to PBS queue)</li>
</ul>
</div>
</div>
<hr class="docutils" />
<div class="section" id="connecting-to-the-unicore-bes-endpoints-using-a-unicore-commandline-client">
<h2>Connecting to the UNICORE BES Endpoints Using a UNICORE Commandline Client<a class="headerlink" href="#connecting-to-the-unicore-bes-endpoints-using-a-unicore-commandline-client" title="Permalink to this headline"></a></h2>
<p>A UNICORE client can be used to connect to the FutureGrid UNICORE6
endpoints. This section describes how to install a UCC ( UNICORE
Commandline Client), configure it to connect to a FutureGrid U6 endpoint
via X-509 based mutual client authentication, and then submit jobs via
BES.</p>
<div class="section" id="installing-the-unicore6-commandline-client-ucc">
<h3>Installing the UNICORE6 Commandline Client (UCC)<a class="headerlink" href="#installing-the-unicore6-commandline-client-ucc" title="Permalink to this headline"></a></h3>
<div class="section" id="acquire-client-bundle">
<h4>Acquire Client Bundle<a class="headerlink" href="#acquire-client-bundle" title="Permalink to this headline"></a></h4>
<ol class="arabic simple">
<li>Navigate to the UNICORE website:
<a class="reference external" href="http://www.unicore.eu/">http://www.unicore.eu/</a></li>
<li>Select Download link in left hand tool bar</li>
<li>Under Clients section, select Download link for Commandline Client</li>
<li>Click on folder for desired version&nbsp; (i.e. 6.4.1)</li>
<li>Click on desired distribution bundle to download&nbsp; (i.e.
ucc-6.4.1-all.tar.gz)</li>
</ol>
</div>
<div class="section" id="unpack-ucc">
<h4>Unpack UCC<a class="headerlink" href="#unpack-ucc" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li>Unpack files from downloaded distribution bundle&nbsp; (i.e. tar -xvzf
ucc-6.4.1-all.tar.gz)</li>
<li>On completion, there should be a directory containing the UNICORE6
commandline client&nbsp; (i.e. ucc-6.4.1)</li>
<li>You can add the bin directory to your path for easier client
execution</li>
</ul>
</div>
<div class="section" id="examine-ucc-files">
<h4>Examine UCC Files<a class="headerlink" href="#examine-ucc-files" title="Permalink to this headline"></a></h4>
<p>Directory Structure</p>
<ul class="simple">
<li>bin  contains executable ucc</li>
<li>certs</li>
<li>conf  contains <em>preferences</em>file to be configured with security
and registry settings</li>
<li>doc</li>
<li>extras</li>
<li>lib</li>
<li>samples</li>
</ul>
</div>
<div class="section" id="run-commandline">
<h4>Run Commandline<a class="headerlink" href="#run-commandline" title="Permalink to this headline"></a></h4>
<p>~</p>
</div>
<div class="section" id="client">
<h4>Client<a class="headerlink" href="#client" title="Permalink to this headline"></a></h4>
<p>Run ucc to get list and description of available commands</p>
<dl class="docutils">
<dt>Usage: ucc &lt;command&gt; [OPTIONS] &lt;args&gt;</dt>
<dd>The following commands are available:
DATA MANAGEMENT
&nbsp;ls&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - list a storage
&nbsp;rm&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - remove a remote file or directory
&nbsp;copy-file-status&nbsp;&nbsp;&nbsp; &nbsp; - check status of a copy-file
&nbsp;get-file&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - get remote files
&nbsp;find&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - find files on storages
&nbsp;resolve&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - resolve remote location
&nbsp;mkdir&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - create a directory remotely
&nbsp;copy-file&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - copy remote files
&nbsp;put-file&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - puts a local file to a remote server
GENERAL
&nbsp;create-storage&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - create a storage service instance
&nbsp;connect&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - connect to UNICORE
&nbsp;list-storages&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - list the available remote storages
&nbsp;list-applications&nbsp;&nbsp;&nbsp; - lists applications on target systems
&nbsp;list-jobs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - list your jobs
&nbsp;list-sites&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - list remote sites
&nbsp;system-info&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - checks the availability of services
JOB EXECUTION
&nbsp;run&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp;- run a job through UNICORE 6
&nbsp;get-status&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp;- get job status
&nbsp;abort-job&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - abort a job
&nbsp;batch&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - run ucc on a set of files
&nbsp;get-output &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - get output files
OGSA-BES
&nbsp;bes-list-att&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - provides information about U6 BES Interface
&nbsp;bes-terminate-job&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - terminate bes activity
&nbsp;bes-submit-job&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - run a job through UNICORE 6 BES Interface
&nbsp;bes-list-jobs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - list jobs running on BES.
&nbsp;bes-job-status&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - get bes activity status
OTHER
&nbsp;shell&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - starts an interactive UCC session
&nbsp;issue-delegation &nbsp;&nbsp;&nbsp;&nbsp; - allows to issue a trust delegation assertion
&nbsp;connect-to-testgrid&nbsp;&nbsp; - get credentials for the public testgrid
&nbsp;wsrf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - perform a WSRF operation
&nbsp;cip-query&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - query a CIS Infoprovider at a UNICORE site
&nbsp;run-groovy&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - run a Groovy script
WORKFLOW
&nbsp;workflow-trace &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - trace info on a workflow in Chemomentum
&nbsp;workflow-control &nbsp;&nbsp;&nbsp;&nbsp; - offers workflow control functions
&nbsp;workflow-submit &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - submit a workflow
&nbsp;workflow-info&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; - lists info on workflows.
&nbsp;broker-run&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - submit work assignment to service</dd>
<dt>orchestrator</dt>
<dd>Enter &#8216;ucc &lt;command&gt; -h&#8217; for help on a particular</dd>
</dl>
</div>
<div class="section" id="installation-conclusion">
<h4>Installation Conclusion<a class="headerlink" href="#installation-conclusion" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li>At this point, the UNICORE Commandline Client has been installed</li>
<li>However, the client is currently not connected to any UNICORE sites</li>
<li>The next section will explain how to configure the client so that the
client will connect to one of the FutureGrid U6 endpoints</li>
</ul>
</div>
</div>
<div class="section" id="configuring-client-to-connect-to-futuregrid-u6-endpoints">
<h3>Configuring Client to Connect to FutureGrid U6 Endpoints<a class="headerlink" href="#configuring-client-to-connect-to-futuregrid-u6-endpoints" title="Permalink to this headline"></a></h3>
<div class="section" id="configuration-overview">
<h4>Configuration Overview<a class="headerlink" href="#configuration-overview" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li>To connect to a FutureGrid U6 endpoint, you need to<ul>
<li>setup security information so that your client will trust the
FutureGrid U6 endpoint and vice-versa</li>
<li>specify the connection address that the client should use (aka
registry address)</li>
</ul>
</li>
<li>This configuration process consists of<ul>
<li>setting up keystore and truststore files with security info</li>
<li>specifying a registry address for the FutureGrid endpoint</li>
</ul>
</li>
<li>This information is stored in a preferences file (starter at
$UCC_HOME/conf/preferences)</li>
</ul>
</div>
</div>
<div class="section" id="setting-up-security-in-unicore">
<h3>Setting Up Security in UNICORE<a class="headerlink" href="#setting-up-security-in-unicore" title="Permalink to this headline"></a></h3>
<p>To setup security, you will need to inform the UNICORE software of your
identity and who you trust via keystore and truststore files:</p>
<ul class="simple">
<li><strong>Keystore</strong>- a file from which UNICORE software reads your identity,
i.e. your private key and your certificate. As your private key is
very sensitive, the keystore is encrypted and you will need a
password to &#8220;unlock it before usage.</li>
<li><strong>Truststore</strong>- a file from which UNICORE software reads certificates
of the Certificate Authorities you trust. It is not as sensitive as a
keystore, but it is also encrypted.</li>
</ul>
<p>For an overview of the security mechanism found in the UNICORE grid
middleware, please consult the <a class="reference external" href="http://unicore.svn.sourceforge.net/svnroot/unicore/documentation/old/securityGuide/Main-UCCOnly.pdf">Users&#8217; UNICORE Security Guide UCC
version</a></p>
<blockquote>
<div>This guide also discusses common security configuration problems and</div></blockquote>
<p>details how to create keystores/truststores</p>
</div>
<div class="section" id="setting-up-a">
<h3>Setting Up a<a class="headerlink" href="#setting-up-a" title="Permalink to this headline"></a></h3>
</div>
<div class="section" id="keystore">
<h3>Keystore<a class="headerlink" href="#keystore" title="Permalink to this headline"></a></h3>
<p>Assumption: you want to use a preexisting X.509 certificate for security
validation</p>
<ul class="simple">
<li>If your key and certificate are in a keystore (in PKCS12 or JKS
format), you can directly use this keystore</li>
<li>If you have PEM files, you will need to wrap your key and certificate
files into a PKCS12 keystore:</li>
</ul>
<div class="section" id="creating-a-truststore">
<h4>Creating a Truststore<a class="headerlink" href="#creating-a-truststore" title="Permalink to this headline"></a></h4>
<p>Assumption: you have certificates for CAs that you trust and want to put
into a truststore file. This should include:</p>
<p>CA cert for CA that issued your certificate</p>
<p>CA cert for CA that issued FutureGrid U6 endpoint certs (Email
<a class="reference external" href="mailto:uvacse&#37;&#52;&#48;virginia&#46;edu">uvacse<span>&#64;</span>virginia<span>&#46;</span>edu</a> for cert)</p>
<p>To create a truststore with keytool</p>
<p>Repeat command for every CA certificate file (set a unique alias for
each certificate)</p>
</div>
<div class="section" id="acquiring-the-registry">
<h4>Acquiring the Registry<a class="headerlink" href="#acquiring-the-registry" title="Permalink to this headline"></a></h4>
</div>
<div class="section" id="address">
<h4>Address<a class="headerlink" href="#address" title="Permalink to this headline"></a></h4>
<p>The UNICORE Registry server provides information about available
services to clients and other services</p>
<p>Registry address for FutureGrid U6 Endpoint on Sierra (as of 08/2011)</p>
</div>
<div class="section" id="connecting-to-endpoints-w-o-registries">
<h4>Connecting to Endpoints w/o Registries<a class="headerlink" href="#connecting-to-endpoints-w-o-registries" title="Permalink to this headline"></a></h4>
<p>A Registry URL is expected to be provided in the preferences file. If a
BES endpoint is not advertised via a UNICORE Registry, the configuration
options can be modified to allow this behavior.</p>
<ol class="arabic simple">
<li>In the preferences file, set the contact-registry to false:</li>
</ol>
<div class="highlight-python"><div class="highlight"><pre><span class="n">contact</span><span class="o">-</span><span class="n">registry</span><span class="o">=</span><span class="n">false</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li>Instead, provide at least one BESFactory URL using the following
format.</li>
</ol>
<div class="highlight-python"><pre>bes.1=https://site1.com/services/BESFactory
bes.4=file:///tmp/bes.xml
bes.5=/tmp/bes.xml</pre>
</div>
<p>If the &#8220;contact-registry&#8221; option is set to false and no OGSA-BES URL is
specified, UCC will report an error.</p>
<p>To use an XML endpoint reference (EPR) read from a file for contacting a
BESFactory service, the contents of a EPR file must validate against the
WS-Addressings endpoint reference schema.</p>
</div>
<div class="section" id="preferences-file-modifications">
<h4>Preferences File Modifications<a class="headerlink" href="#preferences-file-modifications" title="Permalink to this headline"></a></h4>
<p>Once you have keystore and truststore files, configure the client
preferences file to use this security information</p>
<p>$UCC_HOME/conf/preferences</p>
<p><strong>Keystore Settings</strong></p>
<ul class="simple">
<li>Set keystore path to the full path to where your keystore file is
saved locally</li>
<li>Provide the password that unlocks your keystore file</li>
<li>If you do not want specify the password, you will be asked for it on
the commandline</li>
<li>Provide keystores alias</li>
<li>Provide keystores storetype (PKCS12 or JKS)</li>
</ul>
<p><strong>Truststore Settings</strong></p>
<ul class="simple">
<li>Set truststore path to the full path to where your truststore file is
saved locally</li>
<li>Provide the password that unlocks your truststore file</li>
</ul>
<p><strong>Registry Address or BESFactory URL Information</strong></p>
<ul class="simple">
<li>Provide registry address/BESFactory URL of FutureGrid U6 endpoint</li>
</ul>
</div>
<div class="section" id="example-preferences-file">
<h4>Example Preferences File<a class="headerlink" href="#example-preferences-file" title="Permalink to this headline"></a></h4>
<div class="highlight-python"><pre>keystore=/home/gridcerts/keystore.p12
password=YOUR-KEYSTORE-PASSWORD
storetype=pkcs12
alias=myKeystore

truststore=/home/gridcerts/truststore.jks
truststorePassword=YOUR-TRUSTSTORE-PASSWORD

# The address(es) of the registries to contact (space separated list)
registry=https://198.202.120.85:8081/DEMO-SITE/services/Registry?res=default_registry

# ... other properties can follow</pre>
</div>
</div>
<div class="section" id="locating-the-preferences">
<h4>Locating the Preferences<a class="headerlink" href="#locating-the-preferences" title="Permalink to this headline"></a></h4>
</div>
<div class="section" id="file">
<h4>File<a class="headerlink" href="#file" title="Permalink to this headline"></a></h4>
<p>By default, UCC checks for the existence of a file at
$USER_HOME/.ucc/preferences and reads default settings from there</p>
<p>Copy your preferences file to this location or specify its a location
every time you issue a UCC command</p>
<div class="highlight-python"><pre>-c $UCC_HOME/conf/preferences</pre>
</div>
</div>
<div class="section" id="validate-client">
<h4>Validate Client<a class="headerlink" href="#validate-client" title="Permalink to this headline"></a></h4>
</div>
<div class="section" id="setup">
<h4>Setup<a class="headerlink" href="#setup" title="Permalink to this headline"></a></h4>
<p>To check access to FutureGrid endpoints, try a command while specifying
the path to your preferences file</p>
<div class="highlight-python"><pre>$UCC_HOME/bin/ucc system-info -v -l -c $UCC_HOME/conf/preferences
$UCC-HOME/bin/ucc list-sites -c $UCC_HOME/conf/</pre>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="n">preferences</span>
</pre></div>
</div>
<p>If everything is alright you should see information about the FutureGrid
endpoint.</p>
</div>
<div class="section" id="id70">
<h4>Configuration<a class="headerlink" href="#id70" title="Permalink to this headline"></a></h4>
</div>
<div class="section" id="conclusion">
<h4>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline"></a></h4>
<p>Your UCC has now been configured to access the FutureGrid U6 endpoint
via X-509 based mutual client authentication</p>
<blockquote>
<div>Consult the next section to learn about getting started and submit</div></blockquote>
<p>jobs</p>
</div>
</div>
<div class="section" id="submitting-jobs-to-futuregrid-u6-endpoints">
<h3>Submitting Jobs to FutureGrid U6 Endpoints<a class="headerlink" href="#submitting-jobs-to-futuregrid-u6-endpoints" title="Permalink to this headline"></a></h3>
<div class="section" id="getting">
<h4>Getting<a class="headerlink" href="#getting" title="Permalink to this headline"></a></h4>
</div>
<div class="section" id="started">
<h4>Started<a class="headerlink" href="#started" title="Permalink to this headline"></a></h4>
<p>Make sure you have access to some target system</p>
<div class="highlight-python"><pre>$UCC_HOME/bin/ucc connect -c $UCC_HOME/conf/preferences</pre>
</div>
<p>List the sites available to you using</p>
<div class="highlight-python"><pre>$UCC_HOME/bin/ucc list-sites -c $UCC_HOME/conf/</pre>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="n">preferences</span>
</pre></div>
</div>
<p>Run a sample job</p>
<div class="highlight-python"><pre>$UCC_HOME/bin/ucc run -v $UCC_HOME/samples/date.u -c $UCC_HOME/conf/preferences</pre>
</div>
</div>
<div class="section" id="submit-jobs-to">
<h4>Submit Jobs to<a class="headerlink" href="#submit-jobs-to" title="Permalink to this headline"></a></h4>
</div>
<div class="section" id="bes">
<h4>BES<a class="headerlink" href="#bes" title="Permalink to this headline"></a></h4>
<p>To send a job read from a JSDL file</p>
<ul class="simple">
<li>to a site listed in the preferences file:</li>
</ul>
<div class="highlight-python"><pre>$UCC_HOME/bin/ucc bes-submit-job -j hellompi.xml -s bes.3 -v</pre>
</div>
<ul class="simple">
<li>using a BESFactory URL:</li>
</ul>
<div class="highlight-python"><pre>ucc bes-submit-job -j hellompi.xml -s https://example.com/services/BESFactory -v</pre>
</div>
<ul class="simple">
<li>using an endpoint reference file path:</li>
</ul>
<div class="highlight-python"><pre>ucc bes-submit-job -j hellompi.xml -s file:///tmp/bes.xml -v</pre>
</div>
</div>
<div class="section" id="other-bes-related">
<h4>Other BES Related<a class="headerlink" href="#other-bes-related" title="Permalink to this headline"></a></h4>
</div>
<div class="section" id="commands">
<h4>Commands<a class="headerlink" href="#commands" title="Permalink to this headline"></a></h4>
<p>Check job status</p>
<div class="highlight-python"><pre>ucc bes-job-status jobid.job</pre>
</div>
<p>Terminate job</p>
<div class="highlight-python"><pre>ucc bes-terminate-job jobid.job</pre>
</div>
<p>List users jobs on a BESFactory</p>
<div class="highlight-python"><pre>ucc bes-list-job-s bes.1</pre>
</div>
<p>List BESFactory properties:</p>
<div class="highlight-python"><pre>ucc bes-list-att -s bes.1</pre>
</div>
<p>*Descriptor (.job) file is automatically generated after a successful
execution of &#8220;bes-submit-job&#8221; command</p>
</div>
<div class="section" id="job-submission-conclusion">
<h4>Job Submission Conclusion<a class="headerlink" href="#job-submission-conclusion" title="Permalink to this headline"></a></h4>
<p>You should now be able to submit jobs to a BES service on a FutureGrid
U6 endpoint</p>
<ul class="simple">
<li></li>
<li></li>
</ul>
</div>
</div>
<div class="section" id="references">
<h3>References<a class="headerlink" href="#references" title="Permalink to this headline"></a></h3>
<p>The information provided in this section has been extracted from the
following sources. Each contains much more detail about the various
topics discussed here.</p>
<ul class="simple">
<li><a class="reference external" href="https://unicore.svn.sourceforge.net/svnroot/unicore/documentation/old/securityGuide/Main-UCCOnly.pdf">UNICORE6
Manuals</a></li>
<li><a class="reference external" href="http://www.unicore.eu/documentation/manuals/unicore6/files/ucc/ucc-manual.html">UNICORE Commandline Client: User
Manual</a></li>
<li><a class="reference external" href="https://unicore.svn.sourceforge.net/svnroot/unicore/documentation/old/securityGuide/Main-UCCOnly.pdf">UNICORE Security Guide:
UCC</a></li>
</ul>
</div>
<div class="section" id="questions-comments">
<h3>&nbsp; Questions/Comments<a class="headerlink" href="#questions-comments" title="Permalink to this headline"></a></h3>
<p>Please email <a class="reference external" href="mailto:uvacse&#37;&#52;&#48;virginia&#46;edu">uvacse<span>&#64;</span>virginia<span>&#46;</span>edu</a></p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="running-jobs-on-unicore-sites">
<h2>Running Jobs on UNICORE Sites<a class="headerlink" href="#running-jobs-on-unicore-sites" title="Permalink to this headline"></a></h2>
<p>This section provides a general overview of running jobs on UNICORE
sites.</p>
<p>First, you can check whether there is an available target system for
execution:</p>
<p>You can also list the available target sites:</p>
<p>To run a job, you can specify the job using the UNICORE <a class="reference external" href="http://www.unicore.eu/documentation/manuals/unicore6/ucc/jobdescription.html">job description
format</a>.</p>
<p>Or you can specify a job to run using a Job Submission Description
Language (JSDL) file.</p>
<p>For example, a simple job to run the date, date.u, could be described
using the UNICORE description format as follows:</p>
<p>This job can then be run with the command:</p>
<p>Alternately, this job could be described using a JSDL file, date.jsdl,
as follows:</p>
<p>This JSDL job can then be run with the command:</p>
</div>
<hr class="docutils" />
<div class="section" id="deploying-a-new-unicore-6-grid">
<h2>Deploying a New UNICORE 6 Grid<a class="headerlink" href="#deploying-a-new-unicore-6-grid" title="Permalink to this headline"></a></h2>
<p>This section details how to install and setup a UNICORE 6 grid. This
will create a new grid infrastructure separate from the existing
deployments mentioned above.</p>
<div class="section" id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline"></a></h3>
<p>Prerequisites listed in UNICORE 6 README.txt file:</p>
<ul class="simple">
<li>Java 5 JRE or SDK (or later). We recommend using SUN Java 6.</li>
</ul>
<p>You can check what version of java is installed by running:</p>
<p>If the java version is too old or the command is not found, download the
latest JRE version. After unpacking, update JAVA_HOME and PATH.</p>
<ul class="simple">
<li>Perl 5 (only for the classic TSI)</li>
</ul>
<p>You will need Perl if you will be running jobs via a queue management
system. By default, jobs run directly on the local machine. Extra
configuration is needed to setup a TSI and run jobs via a queue
management system.</p>
</div>
<div class="section" id="installing-the-core-server-bundle">
<h3>Installing the Core Server Bundle<a class="headerlink" href="#installing-the-core-server-bundle" title="Permalink to this headline"></a></h3>
<p>The UNICORE 6 Core Server Bundle can be downloaded from the <a class="reference external" href="http://www.unicore.eu/download/unicore6/">UNICORE
website</a>. This bundle will
provide you with the minimal set of UNICORE components (Gateway,
UNICORE/X, TSI, XUUDB) needed to get a UNICORE 6 grid up and running
quickly.</p>
</div>
<div class="section" id="starting-stopping-the-unicore-servers">
<h3>Starting/Stopping the UNICORE Servers<a class="headerlink" href="#starting-stopping-the-unicore-servers" title="Permalink to this headline"></a></h3>
<p>The servers are started with the start.sh script and stopped with the
stop.sh script. Both of these scripts are located under the UNICORE
install directory.</p>
</div>
</div>
<div class="section" id="genesis-ii">
<h2>Genesis II<a class="headerlink" href="#genesis-ii" title="Permalink to this headline"></a></h2>
<dl class="docutils">
<dt><img alt="image131" src="https://portal.futuregrid.org/sites/default/files/u30/fg-logo-md.gif" /></dt>
<dd>Genesis II on FutureGrid
User Manual</dd>
</dl>
<hr class="docutils" />
<p>Author: University of Virginia
&nbsp;Version: 1.0
&nbsp; Last Revision: 2011-11-02</p>
<blockquote>
<div>Introduction</div></blockquote>
<hr class="docutils" />
<p><a class="reference external" href="http://www.genesis2.virginia.edu/wiki">GenesisII</a> compute endpoints
are currently deployed on three FutureGrid HPC resources: XRay, Sierra
and India.&nbsp; Additional endpoints are planned for Hotel and Alamo in the
near future.&nbsp; This tutorial explains:</p>
<ul class="simple">
<li>How to connect to these GenesisII endpoints from other grid
middleware platforms;</li>
<li>How to download GenesisII client software and use the UVa Corss
Campus Grid (XCG) which includes the FutureGrid endpoints.</li>
</ul>
</div>
<div class="section" id="what-is-genesisii">
<h2>What is GenesisII<a class="headerlink" href="#what-is-genesisii" title="Permalink to this headline"></a></h2>
<p>GenesisII is a Grid middleware system. Listed below are the key
principles/features of the GenesisII design. More information about
GenesisII can be found <a class="reference external" href="http://www.genesis2.virginia.edu/wiki">here</a>.</p>
<ul class="simple">
<li>Open source.</li>
<li>Standards-based. GenesisII follows grid standards from the Open Grid
Forum (OGF), W3C, and OASIS, including many from the Open Grid
Services Architecture (OGSA) and the Web Services Resource Framework
(WSRF).&nbsp; In particular, GenesisII adheres to parts or all of OGSA
Basic Execution Service (BES), Resource Naming Service (RNS), OGSA
ByteIO, WS-Security, WS-Naming, WS-Trust, and Job Submission
Description Language (JSDL).</li>
<li>Open and extensible realized with a modern Service- Oriented
Architecture (SOA), which allows to easily replace particular
components with others.</li>
<li>Interoperable with other Grid technologies to enable a coupling of
Grid infrastructures or the users needs</li>
<li>End user focused. Whenever possible GenesisII was designed to use
concepts already familiar to users (such as hierarchical directory
structures) and to provide easy to use commands and GUIs to simplify
the user&#8217;s experience.&nbsp; GenesisII provides a number of commands based
on familiar UNIX tools and provides GUIs for to browse the grid
directory structure, monitor jobs, create JSDL job descriptions, etc.</li>
<li>Strong security mechanisms built in from the ground up.&nbsp; Flexible
access control for all grid resources.&nbsp; Support for X.509
certificates and username/password (based on WS-Security and OGF
Basic Security Profile).&nbsp; Support for virtual organizations via
user-defined groups.</li>
<li>Quick and simple to install and configure.</li>
<li>Client and server software supported on a number of platforms
(Windows XP, Linux, and MacOS).&nbsp; Support for various batch systems
such as PBS, Sun Grid Engine, etc.</li>
<li>Implemented in Java to achieve platform independence.</li>
</ul>
</div>
<div class="section" id="connecting-to-the-genesisii-bes-endpoints">
<h2>Connecting to the GenesisII BES Endpoints<a class="headerlink" href="#connecting-to-the-genesisii-bes-endpoints" title="Permalink to this headline"></a></h2>
<p>To date, three GenesisII BES endpoints have been deployed on FutureGrid
for interoperability testing as well as grid client usage. Endpoints are
located on Sierra, India, and Alamo. This section contains the
information needed for other grid middleware platforms to connect to the
GenesisII BES endpoints.</p>
<div class="section" id="supported-data-staging">
<h3>Supported Data Staging<a class="headerlink" href="#supported-data-staging" title="Permalink to this headline"></a></h3>
</div>
<div class="section" id="protocols">
<h3>Protocols<a class="headerlink" href="#protocols" title="Permalink to this headline"></a></h3>
<p>Each of the GenesisII endpoints supports a number of protocols for
staging data into and out of jobs as supported by the JSDL
specification&#8217;s Data Staging elements.&nbsp; The following are the protocols
currently supported by the GenesisII endpoints:</p>
<div class="section" id="stage-in">
<h4>Stage in:<a class="headerlink" href="#stage-in" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li>HTTP</li>
<li>RNS/ByteIO</li>
<li>ftp</li>
<li>scp</li>
<li>sftp</li>
</ul>
</div>
<div class="section" id="stage-out">
<h4>Stage out:<a class="headerlink" href="#stage-out" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li>mailto</li>
<li>RNS/ByteIO</li>
<li>ftp</li>
<li>scp</li>
<li>sftp</li>
</ul>
<p>** NOTE: ftp, scp, sftp is supported as per the HPC FSE standard and
only for the username/password security token version.</p>
</div>
</div>
<div class="section" id="endpoint-connection-information">
<h3>Endpoint Connection Information<a class="headerlink" href="#endpoint-connection-information" title="Permalink to this headline"></a></h3>
<div class="section" id="id71">
<h4>India<a class="headerlink" href="#id71" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><strong>Endpoint EPR</strong>: <a class="reference external" href="https://portal.futuregrid.org/sites/default/files/india-epr-Nov-01-2011.txt">click
here</a></li>
<li><strong>Security</strong>: configured for username/password authentication;
<a class="reference external" href="mailto:uvacse&#37;&#52;&#48;virginia&#46;edu">Email</a> for username token and CA
cert.</li>
<li><strong>OS</strong>: Red Hat Enterprise Linux Server release 5.7</li>
<li><strong>Arch</strong>: x86_64</li>
<li><strong>Cores</strong>: Approximately 400. Jobs submitted to HPC (i.e. batch)
queue via PBS</li>
<li><strong>Grid Path</strong>:
/bes-containers/FutureGrid/IU/pbs-long.from-daemon.india.futuregrid.org</li>
</ul>
</div>
<div class="section" id="id72">
<h4>Sierra<a class="headerlink" href="#id72" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><strong>Endpoint EPR</strong>: <a class="reference external" href="https://portal.futuregrid.org/sites/default/files/sierra-epr-Mar-22-2012_0.txt">click
here</a></li>
<li><strong>Security</strong>: configured for username/password authentication;
<a class="reference external" href="mailto:uvacse&#37;&#52;&#48;virginia&#46;edu">Email</a> for username token and CA
cert.</li>
<li><strong>OS</strong>: Red Hat Enterprise Linux Server release 6.1</li>
<li><strong>Arch</strong>: x86_64</li>
<li><strong>Cores</strong>: Approximately 300. Jobs submitted to HPC (i.e. batch)
queue via PBS</li>
<li><strong>Grid Path</strong>:
/bes-containers/FutureGrid/SDSC/pbs-long.from-daemon.sierra.futuregrid.org</li>
</ul>
</div>
<div class="section" id="id73">
<h4>Alamo<a class="headerlink" href="#id73" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><strong>Endpoint EPR</strong>: <a class="reference external" href="https://portal.futuregrid.org/sites/default/files/alamo-epr-Mar-22-2012_0.txt">click
here</a></li>
<li><strong>Security</strong>: configured for username/password authentication;
<a class="reference external" href="mailto:uvacse&#37;&#52;&#48;virginia&#46;edu">Email</a> for username token and CA
cert.</li>
<li><strong>OS</strong>: CentOS release 5.6</li>
<li><strong>Arch</strong>: x86_64</li>
<li><strong>Cores</strong>: Approximately 200. Jobs submitted to HPC (i.e. batch)
queue via PBS</li>
<li><strong>Grid Path</strong>: /bes-containers/FutureGrid/TACC/pbs-long.from-alamo1</li>
</ul>
</div>
<div class="section" id="id74">
<h4>Hotel<a class="headerlink" href="#id74" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><strong>Endpoint EPR</strong>: <a class="reference external" href="https://portal.futuregrid.org/sites/default/files/hotel-epr-Apr-24-2012.txt">click
here</a></li>
<li><strong>Security</strong>: <a class="reference external" href="mailto:uvacse&#37;&#52;&#48;virginia&#46;edu">Email</a> for
username/password authentication token and CA cert.</li>
<li><strong>OS</strong>: Red Hat Enterprise Linux Server release 5.8 (Tikanga)</li>
<li><strong>Arch</strong>: x86_64</li>
<li><strong>Cores</strong>: Approximately 300. Jobs submitted to HPC (i.e. batch)
queue via PBS</li>
<li><strong>Grid Path</strong>:
/bes-containers/FutureGrid/UC/pbs-long.from-hotel1.futuregrid.org</li>
</ul>
</div>
</div>
</div>
<div class="section" id="using-the-futuregrid-genesisii-endpoints-as-a-client">
<h2>Using the Futuregrid GenesisII Endpoints as a Client<a class="headerlink" href="#using-the-futuregrid-genesisii-endpoints-as-a-client" title="Permalink to this headline"></a></h2>
<p>There are two ways a client can use the GenesisII endpoints deployed
within Futuregrid: using a standards-compliant non-GenesisII middleware
client or using the GenesisII client.</p>
<div class="section" id="non-genesisiiusing-a-standards-compliant-client">
<h3>Non-GenesisIIUsing a Standards-Compliant Client<a class="headerlink" href="#non-genesisiiusing-a-standards-compliant-client" title="Permalink to this headline"></a></h3>
<p>If you wish to use a standards-based middleware client, you may be
able to use that client to access the GenesisII BES endpoints within
Futuregrid.&nbsp; The first step is to determine whether the grid client
software you wish to use is properly compliant with the GenesisII BES
implementation.&nbsp; GenesisII BES endpoints have been tested for
interoperability against several grid software systems.&nbsp; You will need
to contact the developer/vendor of your system to determine if their
software is compatible with GenesisII.&nbsp; Depending on your system works,
you will either need to contact your grid system administrator to have
him/her add the Futuregrid GenesisII BES endpoints or you will need to
provide a reference to the endpoints to the client tooling.&nbsp; In either
case, the information you or your grid administrator needs is included
in the section above (<a class="reference external" href="#Connecting%20To%20GenesisII%20BES%20Endpoints">Connecting to the GenesisII BES
Endpoints</a>).</p>
</div>
<div class="section" id="using-the-genesisii-client">
<h3>Using The GenesisII Client<a class="headerlink" href="#using-the-genesisii-client" title="Permalink to this headline"></a></h3>
<p>GenesisII has a rich client package available for Windows, MacOS and
LINUX platforms that includes UNIX-style command line tools as well as
several graphical user interface tools.&nbsp; The University of Virginia
maintains a grid called the Cross Campus Grid (XCG) that already
includes the FurtureGrid GenesisII BES endpoints in it - already
configured and ready to go.&nbsp; To get started, the first step is to
download and install the GenesisII/XCG installation package.</p>
<div class="section" id="acquiring-genesisii-client-package">
<h4>Acquiring GenesisII Client Package<a class="headerlink" href="#acquiring-genesisii-client-package" title="Permalink to this headline"></a></h4>
<p>The GenesisII/XCG installers are available at
<a class="reference external" href="http://www.genesis2.virginia.edu/wiki/Main/Downloads">http://www.cs.virginia.edu/~vcgr/wiki/index.php/Genesis_II_Downloads</a>.
Choose the platform that matches the machine/OS where you will be
running your client and click the corresponding &#8220;Download&#8221; link to
download the installer.</p>
<div class="section" id="linux">
<h5>Linux<a class="headerlink" href="#linux" title="Permalink to this headline"></a></h5>
<p>The Linux installer is a shell script named XCG-Installer.sh encoded
with the entire GenesisII package inside.&nbsp; Simply execute the shell
script (./XCG-Installer in proper directory) to begin installation.
Follow the steps here in answering the installation questions.</p>
</div>
<div class="section" id="windows">
<h5>Windows<a class="headerlink" href="#windows" title="Permalink to this headline"></a></h5>
<p>The Windows installer is an executable named XCG-Installer.exe.
Simply run it like any other executable (e.g. double-clicking) and
follow the steps here in answering the installation questions.</p>
<p>Note that currently, the GenesisII software is only tested for
Windows XP.&nbsp; However, our early experience with Windows 7 (and Vista)
indicates that the client installation will work fine as long as you
install GenesisII in a folder that does not have special security
meaning to Windows (such as your Documents and Settings directory or
C:\).</p>
</div>
<div class="section" id="macos">
<h5>MacOS<a class="headerlink" href="#macos" title="Permalink to this headline"></a></h5>
<p>The MacOS installer is a dmg file named XCG-Installer.dmg encoded with
the entire GenesisII package inside.&nbsp; Simply execute the dmg file (e.g.
by double clicking) to begin installation.&nbsp; Follow the steps here in
answering the installation questions.</p>
</div>
</div>
<div class="section" id="installing-the-genesisii-xcg-client-package">
<h4>Installing the GenesisII/XCG Client Package<a class="headerlink" href="#installing-the-genesisii-xcg-client-package" title="Permalink to this headline"></a></h4>
<p>The installation process requires answering a few questions about
license agreement and configuration options.</p>
<ul class="simple">
<li><strong>Agree to license terms</strong>.&nbsp; The GenesisII license follows the Apache
License model.&nbsp; For command line versions, this may require hitting
enter a number of time to scroll the license text on the screen.</li>
<li><strong>Select the directory for the installation</strong>.&nbsp; In most cases the
default is sufficient, though you can feel free to change it.&nbsp; As
noted earlier, Windows Vista and 7 users should choose a directory
that is not given special protection from those operating systems.
For example, the default C:\Program Files is given special treatment
and blocks writes made there by the software.&nbsp; This causes GenesisII
problems because GenesisII has a patch mechanism built which needs to
overwrite package files in the installation directory.&nbsp; For Windows
7/Vista, we recommend a directory within your user folder.</li>
<li><strong>Client v Full Container Install.</strong>&nbsp; The entire GenesisII software
package is included in the installer.&nbsp; This includes both client-side
command line tools and GUIs as well as server-side programs for
installing grid servers.&nbsp; We recommend installing the client only
version.&nbsp; Select XCG Client Install option.</li>
<li><strong>Shortcuts/Start Menu options.</strong>&nbsp; For Windows users, you can select
whether to create a shortcut for all users (default yes), create a
start menu item for GenesisII (default: yes) and pick a name for the
start menu folder (default: GenesisII).&nbsp; The defaults are
recommended, but you can change if you like.&nbsp; For Linux and MacOS,
you can choose whether to create shortcuts in the standard binary
directory (Linux: /usr/bin; MacOS: /Applications/?).&nbsp; Unless you are
installing with root privileges, we recommend choosing &#8220;No&#8221; to this
question.</li>
</ul>
<p>That&#8217;s it.&nbsp; Since the installer is specific to the UVA Cross Campus
Grid, you not only have GenesisII successfully installed, but you have
also configured your system to connect to the XCG.</p>
</div>
</div>
<div class="section" id="id75">
<h3>Getting Started<a class="headerlink" href="#id75" title="Permalink to this headline"></a></h3>
<p>To get started using the XCG, you will need an XCG account - which is
<strong>not</strong> the same as your local machine account or your FutureGrid
resource account. To request an XCG account, fill out the XCG user
application form located at
<a class="reference external" href="https://www.cs.virginia.edu/~vcgr/userrequest/">http://www.cs.virginia.edu/~vcgr/userrequest</a>.</p>
<p>Once you have an account and have the appropriate GenesisII software
installed, you are ready to start a GenesisII grid shell and login.</p>
<div class="section" id="start-grid-shell">
<h4>Start Grid Shell<a class="headerlink" href="#start-grid-shell" title="Permalink to this headline"></a></h4>
<p>To start a grid shell start the &#8220;grid&#8221; executable from the installation
directory.</p>
<div class="section" id="id76">
<h5>Windows<a class="headerlink" href="#id76" title="Permalink to this headline"></a></h5>
<p>Double click the &#8220;grid&#8221; file in the installation directory;
or
Open a Windows command line window, cd to the installation directory,
and enter the grid.exe command.</p>
</div>
<div class="section" id="linux-or-macos">
<h5>Linux or MacOS<a class="headerlink" href="#linux-or-macos" title="Permalink to this headline"></a></h5>
<p>If you have a Window manager running, double click on the grid binary
in the installation directory
or
Open a shell, cd to the installation directory and enter the grid
command</p>
<p><strong>LogIn</strong>
Once you have a grid shell open, you need to be logged into your XCG
grid account in order to perform most useful commands. First check to
see if you are already logged in - the &#8220;whoami&#8221; command prints out your
current credentials. If you are not logged in, it should look like this
(the Client Tool Identity is an automatically generated certificate used
by GenesisII client commands and does not carry any authentication
information):</p>
<p>To login, use the &#8220;login&#8221; command.&nbsp; The syntax is &#8220;login
&#8211;username=&lt;grid user name&gt;&#8221;.&nbsp; After running this command, a popup
window will prompt you for your password</p>
<p>The output from the post-login whoami command indicates that I have 2
new certificates - one that asserts that I am user jfk3w, and another
that asserts that I have the permissions of the group
&#8220;uva-idp-group.2010&#8221;.&nbsp; The XCG uses the uva-idp-group.2010 to set
permissions for all approved XCG users.&nbsp; Your membership in this group
is done by XCG administrators when you account is created.&nbsp; If you later
create new groups or are added to other existing groups, your login will
automatically acquire the extra credentials to assert that you are a
member of that group (assuming the group allows you access).</p>
</div>
</div>
<div class="section" id="running-jobs">
<h4>Running Jobs<a class="headerlink" href="#running-jobs" title="Permalink to this headline"></a></h4>
<p>To learn how the basics about executing and monitoring jobs using
GenesisII and the XCG, please refer to <a class="reference external" href="https://portal.futuregrid.org/sites/default/files/XCG%20Tutorial.pdf">XCG
Tutorial</a>
document.</p>
</div>
</div>
<div class="section" id="getting-help">
<h3>Getting Help<a class="headerlink" href="#getting-help" title="Permalink to this headline"></a></h3>
<p>To get help using the XCG send email to the UVA Computational Science
and Engineering (UVACSE) group at <a class="reference external" href="mailto:uvacse&#37;&#52;&#48;virginia&#46;edu">uvacse<span>&#64;</span>virginia<span>&#46;</span>edu</a>.&nbsp; You can also use
the <a class="reference external" href="http://www.genesis2.virginia.edu/wiki/Main/HomePage">GenesisII web
site</a> and the
<a class="reference external" href="http://www.cs.virginia.edu/~xcgshare/wiki/index.php/Homepage">XCG web
site</a>
to look at further documentation and FAQs</p>
<table border="1" class="docutils">
<colgroup>
<col width="91%" />
<col width="9%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Attachment</th>
<th class="head">Size</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference external" href="https://portal.futuregrid.org/sites/default/files/india-epr-Nov-01-2011.txt">india-epr-Nov-01-2011.txt</a></td>
<td>7.8 KB</td>
</tr>
<tr class="row-odd"><td><a class="reference external" href="https://portal.futuregrid.org/sites/default/files/sierra-epr-Mar-22-2012_0.txt">sierra-epr-Mar-22-2012.txt</a></td>
<td>7.8 KB</td>
</tr>
<tr class="row-even"><td><a class="reference external" href="https://portal.futuregrid.org/sites/default/files/alamo-epr-Mar-22-2012_0.txt">alamo-epr-Mar-22-2012.txt</a></td>
<td>7.82 KB</td>
</tr>
<tr class="row-odd"><td><a class="reference external" href="https://portal.futuregrid.org/sites/default/files/hotel-epr-Apr-24-2012.txt">hotel-epr-Apr-24-2012.txt</a></td>
<td>7.86 KB</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="saga-supporting-distributed-applications-on-grids-clouds-on-futuregrid">
<h2>SAGA supporting Distributed Applications on Grids, Clouds on FutureGrid<a class="headerlink" href="#saga-supporting-distributed-applications-on-grids-clouds-on-futuregrid" title="Permalink to this headline"></a></h2>
<p><strong>*SAGA on FutureGrid*</strong></p>
<p><strong>*Activity*</strong>:</p>
<p>The Simple API for Grid Applications (SAGA) is an OGF standard
(<a class="reference external" href="http://www.ogf.org/">http://www.ogf.org/</a>), and defines a high
level, application-driven API for developing first-principle distributed
applications, and for distributed application frameworks and tools. Our
SAGA project (see
<a class="reference external" href="http://www.saga-project.org/">http://www.saga-project.org/</a>)
provides SAGA API implementations in C++ and Python, which interface to
a variety of middleware backends, as well as higher level application
frameworks, such as Master-Worker, MapReduce, AllPairs, and BigJob.&nbsp; For
all those components, we use FutureGrid and the different software
environments available on FG for extensive portability and
interoperability testing, but also for scale-up and scale-out
experiments. These activities allow to harden the SAGA components
described above, and support CS and Science experiments based on SAGA.</p>
<p><strong>*Achievements:*</strong></p>
<p>FG has provided a persistent, production-grade experimental
infrastructure with the ability to perform controlled experiments,
without violating production policies and disrupting production
infrastructure priorities.&nbsp; These attributes, coupled with FutureGrid&#8217;s
technical support, have resulted in the following specific advances in
the short period of under a year:</p>
<p><em>1: Use of FG for Standards-based development and interoperability
tests:</em></p>
<p>We have, in particular, been able to prepare SAGA for future
deployments on XSEDE; this has occurred by testing the SAGA-BES adaptor
in a variety of configurations: against Unicore and Genesis-II backends,
with UserPass and Certificate-based authentication, with POSIX and HPC
application types, with and without file staging support.&nbsp; While those
tests are still ongoing, it allows us to be confident about the expected
XSEDE middleware evolution; in the vast majority of cases, the
standards-based approach seems to work without a hitch.</p>
<p>Furthermore, we are continuously using FG-based job submission
endpoints for GIN-driven interoperation tests with a variety of other
production Grid infrastructures, including DEISA, PRACE, Teragrid and
EGI (see
<a class="reference external" href="http://forge.gridforum.org/sf/projects/gin/">http://forge.gridforum.org/sf/projects/gin/</a>
and
<a class="reference external" href="http://www.saga-project.org/interop-demos/">http://www.saga-project.org/interop-demos/</a>).</p>
<p>In order to simplify the deployment and to improve end user support
for SAGA, we have been using FG hosts to develop, test and harden our
deployment procedures by mimicking the CSA approach we currently use on
TeraGrid and XSEDE.&nbsp; At the same time, that deployment procedure makes
SAGA and SAGA-based components available and maintained on all FG
endpoints.</p>
<p><em>2: Use of FG for Analysing &amp; Comparing Programming Models and
Run-time tools for Computation and Data-Intensive Science.</em></p>
<p><em>2.a: Development of Tools and Frameworks:</em></p>
<ul>
<li><dl class="first docutils">
<dt>P* experiments</dt>
<dd><p class="first last">&#8216;P*&#8217; is a conceptual model of pilot-based abstractions, in</p>
</dd>
</dl>
<p>particular for pilot jobs.&nbsp; Our work on P* includes comparison
between different PilotJob frameworks (BigJob, Condor GlideIn, Diane,
Swift), and between different coordination models within those
frameworks.&nbsp; We used FG for a number of those experiments, as it
allowed us to compare a range of characteristics in a controlled
environment.</p>
</li>
<li><p class="first">Advanced dynamic partitioning and distribution of data-intensive
distributed applications</p>
<blockquote>
<div><p>Futuregrid resources have been crucial in carrying out a first set</p>
</div></blockquote>
<p>of scoping experiments for O.W.&#8217;s Ph.D thesis: &#8220;Towards a Reasoning
Framework and Software System to Aid the Dynamic Partitioning,
Distribution and Execution of Data-Intensive Applications&#8221;. In these
scoping experiments, three distinct FutureGrid resources (india,
hotel, alamo) were used to coordinately execute a data-intensive
genome matching workload (HTC). The partitioning and distribution
decisions were dynamically made by an experimental software system
based on autonomic computing concepts, and which is capable of
monitoring FG HPC resources as well as jobs during workload
execution.</p>
</li>
<li><dl class="first docutils">
<dt>Bliss (Bliss is SAGA)</dt>
<dd><p class="first last">Bliss</p>
</dd>
</dl>
<p>(<a class="reference external" href="http://oweidner.github.com/bliss/">http://oweidner.github.com/bliss/</a>)
is an experimental implementation of SAGA written in pure Python.
Bliss does not rely on any distributed Grid middleware; however, it
allows distributed access to all FutureGrid HPC resources by
providing an SFTP plugin for file transfer as well as &#8216;PBS over SSH&#8217;
for SAGA&#8217;s job submission and resource information capabilities.
Bliss has been developed specifically with FutureGrid in mind and has
been used in several cross-site experiments as the primary access
mechanism to computing and storage resources.&nbsp; While &#8216;PBS over SSH&#8217;
probably won&#8217;t be a replacement for &#8216;real&#8217; Grid middleware (like,
e.g., Globus), its exposure through the standardized SAGA API
presents an attractive and lightweight alternative to traditionally
large Grid middleware stacks.</p>
</li>
<li><dl class="first docutils">
<dt>High-performance dynamic applications</dt>
<dd><p class="first last">In extreme-scale computational science, there is a growing</p>
</dd>
</dl>
<p>importance and need for specialized architectures and multi-model
simulations. In this emerging environment, different simulation
components will have different computational requirements.&nbsp; Instead
of coarsely assigning resources to all simulation components for
their lifetime, we research methodologies whereby simulations can be
split into their constituent components, and distributed
computational resources are allocated according to the needs of these
individual components.&nbsp; Each simulation component is transferred
along with the data and parameters needed to execute the simulation
component on the target hardware.&nbsp; This approach enables
multi-component applications to more easily benefit from
heterogeneous and distributed computing environments, in which
multiple types of processing elements and storage may be available.</p>
<blockquote>
<div><p>In cases where software is developed with a static execution mode</p>
</div></blockquote>
<p>and only one resource in mind, the choice to distribute may not be
available. By creating a dynamic method of execution and developing
software which can package, transmit, and execute sub-applications
remotely, existing simulations may be extended to make use of
distributed resources. Through specially-designed modules that are
compatible with pre-existing Cactus framework applications, we
demonstrated means of improving task-level parallelism and extended
the range of computing resources used with a minimal amount of change
needed to existing applications.&nbsp; Experiments were conducted using
production cyberinfrastructures on FutureGrid and XSEDE, with up to
128 cores.</p>
</li>
<li><dl class="first docutils">
<dt>Grid/Cloud interop (with Andre Luckow) [finished]</dt>
<dd><p class="first last">We demonstrated for the first time the use of Pilot-Jobs</p>
</dd>
</dl>
<p>concurrently on different types of infrastructures; specifically, we
use BigJob both on FutureGrid HPC and Cloud resources as well as on
other resources such as the XSEDE and OSG Condor resources.</p>
</li>
</ul>
<p><em>2.b: Data Intensive Apps:</em></p>
<ul>
<li><dl class="first docutils">
<dt>MapReduce [with Andre Luckow]</dt>
<dd><p class="first last">In Ref. [1], published in Future Generation Computing Systems, we</p>
</dd>
</dl>
<p>compare implementations of the word-count application to not only use
multiple, heterogeneous infrastructure (Sector versus DFS), but also
to use different programming models (Sphere versus MapReduce).</p>
</li>
<li><dl class="first docutils">
<dt>Grid/Cloud NGS analysis experiments</dt>
<dd><p class="first last">Building upon SAGA-based MapReduce, we have constructed an efficient</p>
</dd>
</dl>
<p>pipeline for gene sequencing. This pipeline is capable of dynamic
resource utilization and task/worker placement.</p>
</li>
<li><p class="first">Hybrid cloud-Grid scientific applications and tools (autonomic
schedulers) [with Manish Parashar, finished]</p>
<blockquote>
<div><p>Policy-based (objective driven) Autonomic Scheduler provides a</p>
</div></blockquote>
<p>system-level approach to hybrid grid-cloud usage.&nbsp; FG has been used
for the development and extension of such Autonomic Scheduling and
application requirements.&nbsp; We have integrated the distributed and
heterogeneous resources of FG as a pool of resources which can be
allocated by the policy-based Autonomic Scheduler (Comet). The
Autonomic Scheduler dynamically determines and allocates instances to
meet specific objectives, such as lowest time-to-completion, lowest
cost etc. We also used FG supplement objective-driven pilot jobs on
TeraGrid (ranger).</p>
</li>
<li><dl class="first docutils">
<dt>Investigate run time fluctuations of application kernels</dt>
<dd><p class="first last">We attempt to explore and characterize run-time fluctuations for a</p>
</dd>
</dl>
<p>given application kernel representative of both a large number of
MPI/parallel workloads and workflows.&nbsp; Fluctuation appears to be
independent of the system load and a consequence of the complex
interaction of the MPI library specifics and virtualization layer, as
well as operating environment.&nbsp; Thus we have been investigating
fluctuations in application performance due to the cloud operational
environment.&nbsp; An explicit aim is to correlate these fluctuations to
details of the infrastructure.&nbsp; As it is difficult to discern or
reverse engineer the specific infrastructure details on EC2 or other
commercial infrastructure, FG has provided us a controlled and well
understood environment at infrastructure scales that are not possible
at the individual PI/resource level.</p>
</li>
</ul>
<p><em>3. SAGA has also produced the following papers (selection):</em></p>
<p>See Refs:
<a class="reference external" href="#ref1">[1]</a>,<a class="reference external" href="#ref2">[2]</a>,<a class="reference external" href="#ref3">[3]</a>,<a class="reference external" href="#ref4">[4]</a>,<a class="reference external" href="#ref5">[5]</a></p>
<p><strong>*FuturePlans:*</strong></p>
<p>We will be continuing to use FG as a resource for SAGA development.
Among other goals, we intend the following: to move the testing
infrastructure to other SAGA based components, like our PilotJob and
PilotData frameworks; to widen the set of middlewares used for testing
(again, keeping XSEDE and other PGIs in mind); to enhance the scope and
scale of our scalability testing; and to test and harden our deployment
and packaging procedures.</p>
<hr class="docutils" />
<ol class="arabic simple">
<li>[fg-1975] <a class="reference external" href="/biblio/author/175">Sehgal, S.</a>, <a class="reference external" href="/biblio/author/176">M.
Erdelyi</a>, <a class="reference external" href="/biblio/author/177">A. Merzky</a>,
and <a class="reference external" href="/biblio/author/115">S. Jha</a>, &#8220;<a class="reference external" href="/references/understanding-application-level-interoperability-scaling-out-mapreduce-over-high-performa">Understanding
application-level interoperability: Scaling-out MapReduce over
high-performance grids and
clouds</a>&#8221;,
Future Generation Computer Systems, vol. 27, issue 5, 2011.</li>
<li>[fg-1976] <a class="reference external" href="/biblio/author/178">Luckow, A.</a>, <a class="reference external" href="/biblio/author/179">L.
Lacinski</a>, and <a class="reference external" href="/biblio/author/115">S.
Jha</a>, &#8220;<a class="reference external" href="/references/saga-bigjob-extensible-and-interoperable-pilot-job-abstraction-distributed-applications-a">SAGA BigJob: An Extensible and
Interoperable Pilot-Job Abstraction for Distributed Applications and
Systems</a>&#8221;,
10th IEEE/ACM International Symposium on Cluster, Cloud and Grid
Computing, 2010.</li>
<li>[fg-1977] <a class="reference external" href="/biblio/author/178">Luckow, A.</a>, and <a class="reference external" href="/biblio/author/115">S.
Jha</a>, &#8220;<a class="reference external" href="/references/abstractions-loosely-coupled-and-ensemble-based-simulations-azure">Abstractions for Loosely-Coupled and
Ensemble-Based Simulations on
Azure</a>&#8221;,
IEEE International Conference on Cloud Computing Technology and
Science, 2010.</li>
<li>[fg-1978] <a class="reference external" href="/biblio/author/180">Kim, J.</a>, <a class="reference external" href="/biblio/author/181">S.
Maddineni</a>, and <a class="reference external" href="/biblio/author/115">S.
Jha</a>, &#8220;<a class="reference external" href="/references/building-gateways-life-science-applications-using-dynamic-application-runtime-environment">Building Gateways for Life-Science
Applications using the Dynamic Application Runtime Environment (DARE)
Framework</a>&#8221;,
The 2011 TeraGrid Conference: Extreme Digital Discovery, 2011.</li>
<li>[fg-1979] <a class="reference external" href="/biblio/author/180">Kim, J.</a>, <a class="reference external" href="/biblio/author/181">S.
Maddineni</a>, and <a class="reference external" href="/biblio/author/115">S.
Jha</a>, &#8220;<a class="reference external" href="/references/characterizing-deep-sequencing-analytics-using-bfast-towards-scalable-distributed-archite">Characterizing Deep Sequencing
Analytics using BFAST: Towards a Scalable Distributed Architecture
for Next-Generation Sequencing
Data</a>&#8221;,
The Second International Workshop on Emerging Computational Methods
for the Life Sciences, 06/2011.</li>
</ol>
<p>Average: Select ratingPoorOkayGoodGreatAwesome</p>
<p>Your rating: 5 Average: 4.5 (2 votes)</p>
</div>
<div class="section" id="emi-unicore-tutorial">
<h2>EMI Unicore Tutorial<a class="headerlink" href="#emi-unicore-tutorial" title="Permalink to this headline"></a></h2>
<div class="section" id="what-is-emi">
<h3>What is EMI?<a class="headerlink" href="#what-is-emi" title="Permalink to this headline"></a></h3>
<p><a class="reference external" href="http://www.eu-emi.eu/">European Middleware Initiative</a>&nbsp;(EMI) is a
software platform for high performance distributed computing.</p>
<p>It is at the core of grid middleware distributions used by scientific
research communities and distributed computing infrastructures all over
the world including WLCG&#8211;the Worldwide LHC Computing Grid&#8211;which
supports, for example, the search for the Higgs boson and new types of
matter searches of the physicists at LHC, together with other large
scientific challenges in astronomy, biology, computational chemistry and
other sciences.</p>
<p>Being a close collaboration among well-established grid middleware
providers and other specialized software providers, EMI proposes itself
as a leading platform for scientific grid computing and looks at
expanding outside of its natural environment.</p>
</div>
<div class="section" id="emi-on-futuregrid">
<h3>EMI on FutureGrid<a class="headerlink" href="#emi-on-futuregrid" title="Permalink to this headline"></a></h3>
<p>EMI have created a number of Virtual Appliances which run on FutureGrid.
This tutorial describes how to use the the EMI UNICORE server tools.
Users can connect an EMI user interface node to the UNICORE instance
running on the VM in order to submit jobs, monitor them and retrieve
results.</p>
<p>The emi-unicore virtual appliance is available on the following
resources</p>
<ul class="simple">
<li>Sierra</li>
<li>Hotel</li>
<li>Foxtrot</li>
</ul>
</div>
<div class="section" id="launching-the-unicore-appliance">
<h3>Launching the UNICORE appliance<a class="headerlink" href="#launching-the-unicore-appliance" title="Permalink to this headline"></a></h3>
<p>To launch the emi-unicore VM you must use the Nimbus cloud-client tool.
For instructions on how to install and configure Nimbus please see
the&nbsp;<a class="reference external" href="https://portal.futuregrid.org/tutorials/nimbus">Nimbus
tutorial</a>.</p>
<p>Once you have a working Nimbus client you can search for the emi-unicore
appliance using the following command</p>
<div class="highlight-python"><pre>$ bin/cloud-client.sh --list</pre>
</div>
<p>You should see a the emi-unicore appliance in the list. To instantiate
an emi-unicore appliance use the following command</p>
<div class="highlight-python"><pre>$ bin/cloud-client.sh --run --name &lt;VM appliance name&gt; --hours &lt;number of hours&gt;</pre>
</div>
<p>For example, if the emi-unicore appliance is called
emi1-unicore-centos-5.3-x64-p1.gz and you want to run it for three
hours, use the following command</p>
<div class="highlight-python"><pre>$ bin/cloud-client.sh --run --name emi1-unicore-centos-5.3-x64-p1.gz --hours 3</pre>
</div>
<p>Once the VM is running you should have a working UNICORE server. You can
log in as root using your ssh key and test the UNICORE services with the
command</p>
<div class="highlight-python"><pre>$ unicore-unicorex-status.sh</pre>
</div>
</div>
<div class="section" id="using-your-unicore-server">
<h3>Using your UNICORE Server<a class="headerlink" href="#using-your-unicore-server" title="Permalink to this headline"></a></h3>
<p>The emi-unicore appliance contains the UNICORE server packages. In order
to submit jobs to this server you must have access to a user interface
machine with the emi-ui package installed.</p>
<p>This can be an external machine, or another VM, but it must run
Scientific Linux versions 5 or 6.</p>
<p>The emi-ui package can be obtained from the&nbsp;<a class="reference external" href="http://emisoft.web.cern.ch/emisoft/index.html">EMI
repository</a>.&nbsp;EMI
packages are signed with the EMI GPG key, which can be obtained
from&nbsp;<a class="reference external" href="http://emisoft.web.cern.ch/emisoft/dist/EMI/2/RPM-GPG-KEY-emi">http://emisoft.web.cern.ch/emisoft/dist/EMI/2/RPM-GPG-KEY-emi</a>.
Download the key and run&nbsp;rpm &#8211;import &lt;keyfilename&gt;&nbsp;in order to allow
packages signed with this key to be verified.</p>
<p>First configure the repository by installing the relevant yum repository
file:</p>
<ul class="simple">
<li><a class="reference external" href="http://emisoft.web.cern.ch/emisoft/dist/EMI/2/sl5/x86_64/base/emi-release-2.0.0-1.sl5.noarch.rpm">SL5</a></li>
<li><a class="reference external" href="http://emisoft.web.cern.ch/emisoft/dist/EMI/2/sl6/x86_64/base/emi-release-2.0.0-1.sl6.noarch.rpm">SL6</a></li>
</ul>
<p>You should now be able to install packages from the EMI repository. To
install the emi-ui type</p>
<div class="highlight-python"><pre>$ yum install emi-ui</pre>
</div>
<p>To configure your emi user interface to use the UNICORE server running
in your emi-unicore virtual appliance you must edit the file
.ucc/preferences. If this file does not exist first run the command ucc
to create it.</p>
<p>Set the registry line to the uri of the EMI UNICORE instance, you will
need to ensure that the hostname matches that of the emi-unicore VM
which you previously instantiated. The hostname was given in the output
of the Nimbus cloud-client.sh &#8211;run command. For example, if your
UNICORE appliance was instantiated on host vm-7.sdsc.futuregrid.org then
you would set the registry line as follows in the .ucc/preferences file:</p>
<div class="highlight-python"><pre>registry=https://vm-7.sdsc.futuregrid.org:8080/DEFAULT-SITE/services/Registry?res=default_registry</pre>
</div>
<p>You will also need to configure the UNICORE authentication. Your UNICORE
server is configured to allow access to the UNICORE &#8220;demo user&#8221;. You can
find the keystore for this
user&nbsp;<a class="reference external" href="http://www.eu-emi.eu/documents/10147/45270/user-keystore.jks">here</a>,
download it and put it in the .ucc/certs/ directory. Next set the
following parameters in the .ucc/preferences file:</p>
<div class="highlight-python"><pre>keystore=certs/user-keystore.jks password=the!user</pre>
</div>
<p>You are now ready to begin using your EMI UNICORE installation. To test
it use the command</p>
<div class="highlight-python"><pre>$ ucc connect</pre>
</div>
</div>
<div class="section" id="querying-resources-on-emi-unicore">
<h3>Querying Resources on EMI UNICORE<a class="headerlink" href="#querying-resources-on-emi-unicore" title="Permalink to this headline"></a></h3>
<p>Before you run any jobs, you should investigate what resources are
available. You can query the UNICORE server using the following commands</p>
<div class="highlight-python"><pre>$ ucc list-sites $ ucc list-storages $ ucc list-applications</pre>
</div>
<p>The first two commands will tell you about the computing and storage
resources available to you. The last command tells you the supported
applications which you are permitted to use. If you look in more detail
at output of the list-applications command you will see that it provides
a list of common applications along with their version number.</p>
<div class="highlight-python"><pre>$ ucc list-applications Applications on target system &lt;DEFAULT-SITE&gt; Korn shell Version M 1993-12-28 q C shell 6.14.00 Bash shell 3.1.16 POVRay 3.5 Python Script 2.4.2 Perl 5.8.8 Date 1.0 Custom executable 1.0</pre>
</div>
<p>When you submit a job to a UNICORE server, you must specify which of
these applications the job will use. You can also upload data files or
scripts for execution by any of these commands. The details of which
application to use and any input and output files must be stored in a
job description file which is submitted to UNICORE for execution.</p>
</div>
<div class="section" id="runing-a-job-and-the-job-description-file">
<h3>Runing a job and the job description file<a class="headerlink" href="#runing-a-job-and-the-job-description-file" title="Permalink to this headline"></a></h3>
<p>Let&#8217;s get started with a very simple example. Create a file on your user
interface machine called date.u with the follwoing content:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># simple job: run Date { ApplicationName: Date, ApplicationVersion: 1.0, }</span>
</pre></div>
</div>
<p>This file tells UNICORE to execute the application Date version 1.0.</p>
<p>To submit this job we use the command</p>
<div class="highlight-python"><pre>$ ucc run date.u -v</pre>
</div>
<p>In the output from this command you should see a line specifying where
the output file was downloaded to, for example:</p>
<div class="highlight-python"><pre>[ucc run] Downloading remote file 'https://vm-7.sdsc.futuregrid.org:8080/DEFAULT-SITE/services/StorageManagement?res=f884b431-4660-4b7a-b91c-260b647604db#//stdout' -&gt; /root/./205d109d-42a6-4cea-8cd3-c85ecc201e4d/stdout</pre>
</div>
<p>This line indicates that the standard output of your job has been
downloaded to the file
/root/./205d109d-42a6-4cea-8cd3-c85ecc201e4d/stdout</p>
<p>You can view your job&#8217;s output by typing</p>
<div class="highlight-python"><pre>$ cat /root/./205d109d-42a6-4cea-8cd3-c85ecc201e4d/stdout Thu Jul 12 10:32:29 EDT 2012</pre>
</div>
<p>Congratulations, you have now succesfully run your first EMI UNICORE
grid job! Of course, this was a very simple job, so next we will go on
to look at how you can submit your own data and scripts for execution by
UNICORE.</p>
</div>
<div class="section" id="running-your-own-scripts">
<h3>Running your own scripts<a class="headerlink" href="#running-your-own-scripts" title="Permalink to this headline"></a></h3>
<p>The job that you ran above didn&#8217;t require any input or output files, and
used a native command on the server that does not require any input.
This is not the case with most useful jobs.</p>
<p>If you want to run your own script you will need to upload the script to
the UNICORE storage device and specify it in the job description file to
tell UNICORE which of your input files to pass to the interpreter.</p>
<p>Let&#8217;s start with a simple bash script. The emi-unicore appliance comes
with a version of the Bash shell, run ucc list-appliances to check the
version number.</p>
<p>Next create a job description file to run this application, for example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># a job to run a bash script { ApplicationName: Bash shell, ApplicationVersion: 3.1.16, }</span>
</pre></div>
</div>
<p>Create a script that you want to run and copy it to the UI machine. A
&#8220;Hello world&#8221; example is given below.</p>
<div class="highlight-python"><pre>echo "Hello World!" &gt; output.txt</pre>
</div>
<p>This script should be set as the Source in your job description file as
follows</p>
<div class="highlight-python"><pre>Environment: [ "SOURCE=remoteScript.sh", ],</pre>
</div>
<p>Next you must tell ucc to upload the script with your job, and to
download the output file using the Imports and Exports</p>
<div class="highlight-python"><pre>Imports: [ { From: "./script.sh", To: "remoteScript.sh" }, ], Exports: [ { From: "output.txt", To:"./output.txt" }, ]</pre>
</div>
<p>Your full job description file will now look something like this</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># a job to run a bash script { ApplicationName: Bash shell, ApplicationVersion: 3.1.16,  Environment: [ &quot;SOURCE=remoteScript.sh&quot;, ],  Imports: [ { From: &quot;./script.sh&quot;, To: &quot;remoteScript.sh&quot; }, ], Exports: [ { From: &quot;output.txt&quot;, To:&quot;./output.txt&quot; }, ] }</span>
</pre></div>
</div>
<p>Now run this job with the ucc run command. You can do the same with
Perl, Python or other scripting languages. Use ucc list-applications to
see a full list of available scripting languages.</p>
</div>
<div class="section" id="handling-data">
<h3>Handling Data<a class="headerlink" href="#handling-data" title="Permalink to this headline"></a></h3>
<p>If you have data files that you want to use with multiple jobs it can be
inefficient to stage them in with each job. In this case it may be
useful to copy the files to the UNICORE storage in advance. You can do
this using the ucc command line tool. Once done, your job can tell
UNICORE to copy the files to your local directory by specifying them in
your job description file.</p>
<p>To copy a file to a storage element first list the available storages:</p>
<div class="highlight-python"><pre>$ ucc list-storages Home https://vm-143.uc.futuregrid.org:8080/DEFAULT-SITE/services/StorageManagement?res=demo-Home</pre>
</div>
<p>This tells you that your Home directory is available addressable via
this URI. You can use this URI to access your Home directory as follows</p>
<div class="highlight-python"><pre>$ ucc ls https://vm-143.uc.futuregrid.org:8080/DEFAULT-SITE/services/StorageManagement?res=demo-Home</pre>
</div>
<div class="highlight-python"><pre>$ ucc put-file -s localfile.data -t https://vm-143.uc.futuregrid.org:8080/DEFAULT-SITE/services/StorageManagement?res=demo-Home#/remotefile.data</pre>
</div>
<p>Note that the -s parameter for ucc put-file specifies the source file
while -t specifies the target file.</p>
<p>Using the full URI is sometimes inconvenient, so you can use a shorter,
more intuitive format. This is also a URI, but you need to know only the
site (target system) name, and the storage or job id. For example</p>
<div class="highlight-python"><pre>unicore6://DEFAULT-SITE/Home/file</pre>
</div>
<p>will resolve the &#8220;Home&#8221; storage at the target system named
&#8220;DEFAULT-SITE&#8221;.</p>
<div class="highlight-python"><pre>$ ucc put-file -s localfile.data -t u6://DEFAULT-SITE/Home/testdata  $ ucc get-file -s u6://DEFAULT-SITE/Home/testdata -t newlocalfile.data</pre>
</div>
<p>Remember that you will need to copy the file to the local working
directory as part of your job in order to use it. To do this, include it
in the job description file as an import as follows:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">{</span> <span class="n">From</span><span class="p">:</span> <span class="s">&quot;u6://DEFAULT-SITE/Home/testdata&quot;</span><span class="p">,</span> <span class="n">To</span><span class="p">:</span> <span class="s">&quot;testdata&quot;</span><span class="p">}</span>
</pre></div>
</div>
<p>You can also refer to a job Uspace (the job&#8217;s working directory) on a
given site. For this, you will need the unique ID of that job, which you
can get for example using the &#8216;list-jobs&#8217; command. For example,</p>
<div class="highlight-python"><pre>unicore6://DEFAULT-SITE/1f3bc2e2-d814-406e-811d-e533f8f7a93b/outfile</pre>
</div>
<p>refers to the file &#8220;outfile&#8221; in the working directory of the given job
on the &#8220;DEFAULT-SITE&#8221; target system. This may be useful for checking the
content of files during the job&#8217;s execution, allowing you to steer the
analysis on the basis of partial results.</p>
</div>
<div class="section" id="resources">
<h3>Resources<a class="headerlink" href="#resources" title="Permalink to this headline"></a></h3>
<p>In the Resources section of the .u script user can specify resources to
run the job on the remote system. The section may look as follows:</p>
<div class="highlight-python"><pre>Resources: { Memory: 128000000, Nodes: 1, CPUs: 8 , }</pre>
</div>
</div>
<div class="section" id="running-job-on-a-set-of-files">
<h3>Running job on a set of files<a class="headerlink" href="#running-job-on-a-set-of-files" title="Permalink to this headline"></a></h3>
<p>To run UNICORE on a set of files user can put jobs descriptions in one
directory (e.g. indir/) and use batch command: batch. -i argument
indicate source directory (with .u scripts), -o - directory for output
files:</p>
<div class="highlight-python"><pre>$ ucc batch -i indir -o outdir</pre>
</div>
<p>Average: Select ratingPoorOkayGoodGreatAwesome</p>
<p>No votes yet</p>
</div>
</div>
<div class="section" id="tutorials">
<h2>Tutorials<a class="headerlink" href="#tutorials" title="Permalink to this headline"></a></h2>
<p>If you&#8217;re looking for the Cloud Summer School 2012 (along with
excellent tutorial material), click on the icon below:</p>
<p><a class="reference external" href="https://portal.futuregrid.org/projects/241"><img alt="image132" src="https://portal.futuregrid.org/sites/default/files/u23/summerschool2012.png" /></a></p>
<p><strong>Note:</strong>This page is maintained by Renato Figueiredo from UF.</p>
<p>The following tutorials are broadly organized into topics, with each
tutorial classified by the user&#8217;s target level of expertise with
FutureGrid (novice, intermediate, advanced). (If you are a tutorial
developer, for instructions on&nbsp;how to add a tutorial to this list,
please&nbsp;<a class="reference external" href="https://portal.futuregrid.org/outreach">refer to the TEOS
page</a>).</p>
<p>If you have corrections or suggestions related to our tutorial
content, please <a class="reference external" href="https://portal.futuregrid.org/help">fill out a help
request</a>.</p>
<div class="section" id="tutorial-topic-0-accessing-futuregrid-resources">
<h3>Tutorial Topic 0: Accessing FutureGrid Resources<a class="headerlink" href="#tutorial-topic-0-accessing-futuregrid-resources" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/accessing-futuregrid-resources-ssh">https://portal.futuregrid.org/accessing-futuregrid-resources-ssh</a>&nbsp;[novice]</li>
</ul>
</div>
<div class="section" id="tutorial-topic-1-cloud-provisioning-platforms">
<h3>Tutorial Topic 1: Cloud Provisioning Platforms<a class="headerlink" href="#tutorial-topic-1-cloud-provisioning-platforms" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/nimbus">Using Nimbus on
FutureGrid</a>
[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/nm2">Nimbus One-click Cluster
Guide</a> [intermediate]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/manual/openstack/grizzly">Using OpenStack Grizzly on
FutureGrid</a>
[novice] <strong>** NEW **</strong></li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/openstack">Using OpenStack Essex
on&nbsp;FutureGrid</a>&nbsp;[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/eucalyptus3">Using Eucalyptus on
FutureGrid</a>
[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/contrib/simple-vine-tutorial">Connecting private network VMs&nbsp;across Nimbus clusters using
ViNe</a>&nbsp;[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/ipop1">IPOP1: (IP-over-P2P) Virtual Network Introductory
Tutorial</a> [novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/ipop1">IPOP2: Deploying your Own P2P Overlay for IPOP
VPNs</a>&nbsp;[intermediate]</li>
</ul>
</div>
<div class="section" id="tutorial-topic-2-cloud-run-time-map-reduce-platforms">
<h3>Tutorial Topic 2: Cloud Run-time Map/Reduce Platforms<a class="headerlink" href="#tutorial-topic-2-cloud-run-time-map-reduce-platforms" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/running-hadoop-batch-job-using-myhadoop">Running Hadoop as a batch job using
MyHadoop</a>&nbsp;[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-hpc">Running SalsaHadoop&nbsp;(one-click Hadoop) on HPC
environment</a>&nbsp;[beginner]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/twister-futuregrid-hpc">Running Twister on HPC
environment</a>&nbsp;[beginner]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/salsahadoop-futuregrid-cloud-eucalyptus">Running SalsaHadoop&nbsp;on
Eucalyptus</a>&nbsp;[intermediate]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/eucalyptus-and-twister-futuregrid">Running</a><a class="reference external" href="https://portal.futuregrid.org/tutorials/eucalyptus-and-twister-futuregrid">FG-Twister
on
Eucalyptus</a>&nbsp;[intermediate]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/one-click-hadoop-wordcount-eucalyptus-futuregrid">Running One-click Hadoop WordCount on
Eucalyptus</a>
[beginner]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/one-click-twister-k-means-eucalyptus-futuregrid">Running One-click Twister K-means on
Eucalyptus</a>
[beginner]</li>
</ul>
</div>
<div class="section" id="tutorial-topic-3-grid-appliances-for-training-education-and-outreach">
<h3>Tutorial Topic 3: Grid Appliances for Training, Education, and Outreach<a class="headerlink" href="#tutorial-topic-3-grid-appliances-for-training-education-and-outreach" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/ga1">Running a Grid Appliance on your
desktop</a>&nbsp;&nbsp;[novice]</li>
<li><a class="reference external" href="http://portal.futuregrid.org/tutorials/ga9">Running a Grid Appliance on
FutureGrid</a> [novice]</li>
<li><a class="reference external" href="http://portal.futuregrid.org/tutorials/os1">Running an OpenStack virtual appliance on
FutureGrid</a> [novice]</li>
<li><a class="reference external" href="http://portal.futuregrid.org/tutorials/ga8">Running Condor tasks on the Grid
Appliance</a> [novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/mp1">Running MPI tasks on the Grid
Appliance</a>&nbsp;[novice]</li>
<li><a class="reference external" href="http://portal.futuregrid.org/tutorials/ga10">Running Hadoop&nbsp;tasks on the Grid
Appliance</a> [novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/ga4">Deploying virtual private Grid Appliance clusters using
Nimbus</a>
[intermediate]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/ga3">Building an educational appliance from Ubuntu
10.04</a> [intermediate]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/ga7">Customizing and registering Grid Appliance images using
Eucalyptus</a>
[intermediate]</li>
</ul>
</div>
<div class="section" id="tutorial-topic-4-high-performance-computing">
<h3>Tutorial Topic 4: High Performance Computing<a class="headerlink" href="#tutorial-topic-4-high-performance-computing" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><a class="reference external" href="/tutorials/hpc">Basic High Performance Computing</a> [novice]</li>
<li><a class="reference external" href="/tutorials/running-hadoop-batch-job-using-myhadoop">Running Hadoop as a batch job using
MyHadoop</a>
[novice]</li>
<li><a class="reference external" href="/manual/performance/vampir">Performance Analysis with Vampir</a>
[advanced]</li>
<li><a class="reference external" href="/manual/vampir/trace">Instrumentation and tracing with
VampirTrace</a> [advanced]</li>
</ul>
</div>
<div class="section" id="tutorial-topic-5-experiment-management">
<h3>Tutorial Topic 5: Experiment Management<a class="headerlink" href="#tutorial-topic-5-experiment-management" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/interactive-experiment-management">Running interactive
experiments</a>
[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/workflow-experiment-management">Running workflow experiments using
Pegasus</a>&nbsp;[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/pegasus-on-futuregrid-tutorial">Pegasus 4.1 on FutureGrid
Tutorial</a>&nbsp;[novice]</li>
</ul>
</div>
<div class="section" id="tutorial-topic-6-image-management-and-rain">
<h3>Tutorial Topic 6: Image Management and Rain<a class="headerlink" href="#tutorial-topic-6-image-management-and-rain" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><a class="reference external" href="http://futuregrid.github.com/rain/quickstart.html">Using Image Management and
Rain</a>&nbsp;[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/generate-and-register-os-image-futuregrid-using-fg-shell">Easy steps to generate and register an
Image</a>
[novice]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/manually-customize-image">Manual Image
Customization</a>
[advanced]</li>
<li><a class="reference external" href="https://portal.futuregrid.org/register-virtual-box-image-openstack">Register your VirtualBox image in
OpenStack</a>
[intermediate]</li>
</ul>
</div>
<div class="section" id="tutorial-topic-7-storage">
<h3>Tutorial Topic 7: &nbsp;Storage<a class="headerlink" href="#tutorial-topic-7-storage" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/tutorials/hpss">Using HPSS from
FutureGrid</a>&nbsp;[novice]</li>
</ul>
</div>
<div class="section" id="other-tutorials-and-educational-materials">
<h3>Other Tutorials and Educational Materials<a class="headerlink" href="#other-tutorials-and-educational-materials" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><a class="reference external" href="https://portal.futuregrid.org/additional_tutorials">Additional tutorials on FutureGrid-related
technologies</a></li>
<li><a class="reference external" href="https://portal.futuregrid.org/community_edu_materials">FutureGrid community educational
materials</a></li>
<li><a class="reference external" href="http://www.citutor.org/browse.php?access=&amp;category=-1&amp;search=performance&amp;include=all&amp;filter=Filter">CI Tutor performance
tutorials</a>
(requires brief registration to view content)</li>
</ul>
</div>
</div>
<div class="section" id="futuregrid-grid-appliance-for-nimbus-and-eucalyptus">
<h2>FutureGrid Grid Appliance for Nimbus and Eucalyptus<a class="headerlink" href="#futuregrid-grid-appliance-for-nimbus-and-eucalyptus" title="Permalink to this headline"></a></h2>
<p><strong>Summary:</strong></p>
<p>This tutorial provides step-by-step instructions on how to install
clients to access Eucalyptus and Nimbus clouds on FutureGrid using the
Grid appliance.</p>
<p><strong>Pre-requisites:</strong></p>
<p><a class="reference external" href="http://portal.futuregrid.org/tutorials/ga1">FutureGrid tutorial GA1 - Introduction to the Grid
Appliance</a></p>
<p><strong>Hands-on tutorial</strong>:</p>
<p>This tutorial is maintained at the Grid Appliance portal. See
<a class="reference external" href="http://www.grid-appliance.org/wiki/index.php/FutureGrid:clientappliance">FutureGrid:clientappliance</a>.</p>
</div>
<div class="section" id="one-click-hadoop-wordcount-on-eucalyptus-futuregrid">
<h2>One-click Hadoop WordCount on Eucalyptus FutureGrid<a class="headerlink" href="#one-click-hadoop-wordcount-on-eucalyptus-futuregrid" title="Permalink to this headline"></a></h2>
<p>This tutorial shows how to run a one-click Hadoop WordCount job on the
Eucalyptus platform of FutureGrid.</p>
<p>1. FutureGrid&nbsp;HPC account: please apply via&nbsp;<a class="reference external" href="../../user/register">FutureGrid
portal</a> and&nbsp;<a class="reference external" href="../../request-hpc-account">request a HPC
account</a>.
2. FutureGrid Eucalyptus account: please see <a class="reference external" href="../../tutorials/eucalyptus3">FutureGrid Eucalyptus
Tutorial</a> for detailed instructions.
3. FutureGrid&nbsp;Eucalyptus credentials zip file
(euca2-[username]-x509.zip) stored under user&#8217;s home directory
4. Key pair created and added for use with Eucalyptus virtual
machines</p>
<p>The following sections assume a user has created both an HPC account
and a Eucalpytus account under the username of <em>gaoxm</em>.</p>
<p>[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;129-79-49-98">gaoxm<span>&#64;</span>129-79-49-98</a> ~]$ ssh -i .ssh/id_rsa_fg
india.futuregrid.org
Enter passphrase for key &#8216;.ssh/id_rsa_fg&#8217;:
Last login: Sat May&nbsp; 5 02:17:33 2012 from
c-71-194-153-252.hsd1.in.comcast.net
...
torque/2.5.5 version 2.5.5 loaded
moab version 5.4.0 loaded
euca2ools version 1.2 loaded
[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> ~]$ cd eucalyptus/
[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> eucalyptus]$ ls
cloud-cert.pem&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; euca2-gaoxm-d108375b-pk.pem
eucarc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hosts&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nodes
euca2-gaoxm-d108375b-cert.pem&nbsp; euca2-gaoxm-x509.zip
gaoxm.private&nbsp; jssecacerts&nbsp; tmp.out</p>
<p>[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> test]$ wget
<a class="reference external" href="http://mypage.iu.edu/~gao4/data/hadoopOneClick.zip">http://mypage.iu.edu/~gao4/data/hadoopOneClick.zip</a>
...
[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> test]$ ls
hadoopOneClick.zip
[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> test]$ unzip hadoopOneClick.zip</p>
<p>[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> test]$ cd hadoopOneClick
[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> hadoopOneClick]$ ls
deploy-hadoop.sh&nbsp;&nbsp;&nbsp;&nbsp; instanceIds.txt&nbsp; publicIps.txt
stop-hadoop.sh
hadoop-one-click.sh&nbsp; ipHosts.txt&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; run-hadoop-wordcount.sh
terminate-instances.sh
hosts&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nodes.txt&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; start-instances.sh
[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> hadoopOneClick]$ chmod +x *.sh
[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> hadoopOneClick]$ ./hadoop-one-click.sh -n 2 -t m1.small
-i emi-D778156D -k gaoxm -p ~/eucalyptus/gaoxm.private -l
<a class="reference external" href="http://mypage.iu.edu/~gao4/data/grexp10.txt">http://mypage.iu.edu/~gao4/data/grexp10.txt</a>
-s
<a class="reference external" href="http://salsahpc.indiana.edu/tutorial/apps/hadoop-0.20.203.0-for-EucaVm.tar.gz">http://salsahpc.indiana.edu/tutorial/apps/hadoop-0.20.203.0-for-EucaVm.tar.gz</a></p>
<p>This will run a MapReduce word-count job on a dynamically created
virtual Hadoop cluster on FutureGrid. The user needs to replace the <em>k</em>
and <em>p</em> parameter values with his/her key-pair name and private key
path. For detailed usage information, try</p>
<p>[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> hadoopOneClick]$ ./hadoop-one-click.sh -h</p>
<p>[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> hadoopOneClick]$ ls outputs/
_logs&nbsp; part-r-00000&nbsp; _SUCCESS
[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> hadoopOneClick]$ vim outputs/part-r-00000</p>
<p>To run other MapReduce jobs, replace <em>run-hadoop-wordcount.sh</em> with
new scripts, and change <em>hadoop-one-click.sh</em> to call the corresponding
scripts.</p>
</div>
<div class="section" id="one-click-twister-k-means-on-eucalyptus-futuregrid">
<h2>One-click Twister K-means on Eucalyptus FutureGrid<a class="headerlink" href="#one-click-twister-k-means-on-eucalyptus-futuregrid" title="Permalink to this headline"></a></h2>
<p>This tutorial shows how to run a one-click Twister K-means job on the
Eucalyptus platform of FutureGrid.</p>
<p>1. FutureGrid&nbsp;HPC account, please apply via&nbsp;<a class="reference external" href="https://portal.futuregrid.org/user/register">FutureGrid
portal</a>&nbsp;and&nbsp;<a class="reference external" href="https://portal.futuregrid.org/request-hpc-account">request a
HPC account</a>.
2. FutureGrid Eucalyptus account, please see&nbsp;<a class="reference external" href="https://portal.futuregrid.org/tutorials/eucalyptus">FutureGrid Eucalyptus
Tutorial</a>&nbsp;for
detailed instructions.
3. FutureGrid&nbsp;Eucalyptus credentials zip file
(euca2-[username]-x509.zip) stored under user&#8217;s home directory.
4. Key pair created and added for use with Eucalyptus virtual
machines.</p>
<p>The following sections assume a user has created both HPC account and
Eucalpytus account under the username of gaoxm.</p>
<p>[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;129-79-49-98">gaoxm<span>&#64;</span>129-79-49-98</a> ~]$ ssh -i .ssh/id_rsa_fg
india.futuregrid.org
Enter passphrase for key &#8216;.ssh/id_rsa_fg&#8217;:
Last login: Sat May&nbsp; 5 02:17:33 2012 from
c-71-194-153-252.hsd1.in.comcast.net
...
torque/2.5.5 version 2.5.5 loaded
moab version 5.4.0 loaded
euca2ools version 1.2 loaded
[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> ~]$ cd eucalyptus/
[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> eucalyptus]$ ls
cloud-cert.pem&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; euca2-gaoxm-d108375b-pk.pem
eucarc&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; hosts&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nodes
euca2-gaoxm-d108375b-cert.pem&nbsp; euca2-gaoxm-x509.zip
gaoxm.private&nbsp; jssecacerts&nbsp; tmp.out</p>
<p>[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> test]$
wget&nbsp;<a class="reference external" href="http://mypage.iu.edu/~gao4/data/hadoopOneClick.zip">http://mypage.iu.edu/~gao4/data/twisterOneClick.zip</a>
...
[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> test]$ ls
hadoopOneClick&nbsp; hadoopOneClick.zip&nbsp; twisterOneClick.zip
[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> test]$ unzip twisterOneClick.zip</p>
<p>[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> test]$ cd twisterOneClick
[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> twisterOneClick]$ ls
deploy-twister.sh&nbsp; instanceIds.txt&nbsp; publicIps.txt
stop-twister.sh
hostnames.txt&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ipHosts.txt&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; run-twister-kmeans.sh
terminate-instances.sh
hosts&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nodes.txt&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; start-instances.sh
twister-one-click.sh
[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> twisterOneClick]$ chmod +x *.sh
[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> twisterOneClick]$ ./twister-one-click.sh -n 2 -t m1.small
-i emi-D778156D -k gaoxm -p ~/eucalyptus/gaoxm.private
-l<a class="reference external" href="http://salsahpc.indiana.edu/tutorial/apps/Twister-0.9.tar.gz">http</a><a class="reference external" href="http://salsahpc.indiana.edu/tutorial/apps/Twister-0.9.tar.gz">://salsahpc.indiana.edu/tutorial/apps/Twister-0.9.tar.gz</a>&nbsp;-a
<a class="reference external" href="http://www.iterativemapreduce.org/apache-activemq-5.4.2-bin.tar.gz">http://www.iterativemapreduce.org/apache-activemq-5.4.2-bin.tar.gz</a></p>
<p>This will run a MapReduce K-means job on a dynamically created
virtual Twister cluster on&nbsp; FutureGrid. The user needs to replace the
k and p parameter values with his/her key-pair name and private
key path. For detailed usage information, try</p>
<p>[<a class="reference external" href="mailto:gaoxm&#37;&#52;&#48;i136">gaoxm<span>&#64;</span>i136</a> twisterOneClick]$ ./twister-one-click.sh -h</p>
<p>Calling run_kmeans.sh on 149.165.159.140...
JobID: kmeans-map-reduce9ec9eaa2-9731-11e1-80d7-156f25bd362a
May 6, 2012 4:11:57 AM
org.apache.activemq.transport.failover.FailoverTransport doReconnect
INFO: Successfully connected
to&nbsp;<a class="reference external" href="https://master:61616/">tcp://master:61616</a>
0&nbsp;&nbsp;&nbsp; [main] INFO&nbsp; cgl.imr.client.TwisterDriver&nbsp; - Configure Mappers
through the partition file, please wait....
1975 [main] INFO&nbsp; cgl.imr.client.TwisterDriver&nbsp; - Configuring Mappers
through the partition file is completed.
250.77056136584878 , 125.15021341387315 , 249.21561041359857 ,
246.74715176402833 , 375.350251646343 , 249.17570173022511 ,
Total Time for kemeans : 6.808
Total loop count : 15
6260 [main] INFO&nbsp; cgl.imr.client.TwisterDriver&nbsp; - MapReduce
computation termintated gracefully.
&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;
Kmeans clustering took 6.841 seconds.
&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;</p>
<p>To run other iterative MapReduce jobs, replace run-twister-kmeans.sh
with new scripts, and change twister-one-click.sh to call the
corresponding scripts.</p>
</div>
<div class="section" id="register-virtual-box-image-on-openstack">
<h2>Register Virtual Box Image on OpenStack<a class="headerlink" href="#register-virtual-box-image-on-openstack" title="Permalink to this headline"></a></h2>
<p>In this tutorial we explain how to convert a Virtual Box image to kvm
format and then register it on OpenStack.</p>
<div class="section" id="id77">
<h3>Prerequisites<a class="headerlink" href="#id77" title="Permalink to this headline"></a></h3>
<p>There is two main prerequisites for your images to work with OpenStack</p>
<div class="section" id="disable-selinux-only-for-redhat-based-linux-like-centos">
<h4>Disable SELinux (Only for RedHat-based Linux like CentOS)<a class="headerlink" href="#disable-selinux-only-for-redhat-based-linux-like-centos" title="Permalink to this headline"></a></h4>
<p>Edit the file&nbsp;/etc/selinux/config and set the SELINUX option to disabled</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">SELINUX</span><span class="o">=</span><span class="n">disabled</span>
</pre></div>
</div>
</div>
<div class="section" id="configuring-the-image-network-interface-eth0-for-dhcp">
<h4>Configuring the image network interface (eth0) for DHCP<a class="headerlink" href="#configuring-the-image-network-interface-eth0-for-dhcp" title="Permalink to this headline"></a></h4>
<p>In Ubuntu, you edit the file /etc/network/interfaces and configure eth0
to:</p>
<div class="highlight-python"><pre>auto eth0
iface eth0 inet dhcp</pre>
</div>
<p>In CentOS, you edit the file /etc/sysconfig/network-scripts/ifcfg-eth0
and make sure it contains:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">DEVICE</span><span class="o">=</span><span class="n">eth0</span>
<span class="n">BOOTPROTO</span><span class="o">=</span><span class="n">dhcp</span>
<span class="n">ONBOOT</span><span class="o">=</span><span class="n">yes</span>
</pre></div>
</div>
</div>
<div class="section" id="configure-the-image-to-allow-openstack-to-inject-the-ssh-key">
<h4>Configure the image to allow OpenStack to inject the ssh key<a class="headerlink" href="#configure-the-image-to-allow-openstack-to-inject-the-ssh-key" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li>Ubuntu&nbsp;(it may not be needed for Ubuntu, but it is recomended)</li>
</ul>
<ul class="simple">
<li>CentOS.&nbsp;Edit the file /etc/rc.local</li>
</ul>
</div>
<div class="section" id="configure-udev-persistent-rules-only-centos">
<h4>Configure udev persistent rules (only CentOS)<a class="headerlink" href="#configure-udev-persistent-rules-only-centos" title="Permalink to this headline"></a></h4>
<p>Edit the file&nbsp;/etc/udev/rules.d/70-persistent-net.rules, delete
everything and add:</p>
<div class="highlight-python"><pre>ACTION=="add",SUBSYSTEM=="net", IMPORT{program}="/lib/udev/rename_device"
SUBSYSTEM=="net", RUN+="/etc/sysconfig/network-scripts/net.hotplug"</pre>
</div>
</div>
</div>
<div class="section" id="convert-your-virtual-box-image-to-raw-format">
<h3>Convert your virtual box image to raw format<a class="headerlink" href="#convert-your-virtual-box-image-to-raw-format" title="Permalink to this headline"></a></h3>
<div class="highlight-python"><pre>VBoxManage clonehd /path/to/imagefile/vboximage.vdi /path/to/rawimage.img --format raw</pre>
</div>
<p>Make sure that you specify the full path. Otherwise, you may experience
errors and the new image will be placed in&nbsp;&nbsp;~/.VirtualBox/HardDisks/</p>
</div>
<div class="section" id="convert-the-image-to-qcow2-format-optional">
<h3>Convert the image to qcow2 format (optional)<a class="headerlink" href="#convert-the-image-to-qcow2-format-optional" title="Permalink to this headline"></a></h3>
<p>This step is optional, but it is recommended if your image as a
considerable size because the qcow2 format will compress your image.
Nevertheless, it will takes a while to finish this command. As example
my 8GB image took a couple of hours to get ready and was compressed to
less than 6 GB.</p>
<div class="highlight-python"><pre>qemu-img convert -f raw rawimage.img -O qcow2 qcow2image.img</pre>
</div>
</div>
<div class="section" id="test-your-image">
<h3>Test your image<a class="headerlink" href="#test-your-image" title="Permalink to this headline"></a></h3>
<p>If you are in a computer with graphical interface, the easiest way to
test it is by executing the following command. This command will open a
window and you will see if your image boots correctly.</p>
<div class="highlight-python"><pre>kvm -hda rawimage.img -m 1024</pre>
</div>
<p>or</p>
<div class="highlight-python"><pre>kvm -hda qcow2image.img -m 1024</pre>
</div>
<p>From now on, we refer only to the rawimage.img, but it works in the same
way with the qcow2image.img.</p>
</div>
<div class="section" id="transfer-your-image-to-india">
<h3>Transfer your Image to India<a class="headerlink" href="#transfer-your-image-to-india" title="Permalink to this headline"></a></h3>
<div class="highlight-python"><pre>$ scp rawimage.img &lt;username&gt;@india.futuregrid.org:/N/u/&lt;username&gt;/</pre>
</div>
</div>
<div class="section" id="id78">
<h3>Log into India<a class="headerlink" href="#id78" title="Permalink to this headline"></a></h3>
<div class="highlight-python"><pre>$ ssh &lt;username&gt;@india.futuregrid.org</pre>
</div>
</div>
<div class="section" id="upload-your-image-to-openstack">
<h3>Upload your image to OpenStack<a class="headerlink" href="#upload-your-image-to-openstack" title="Permalink to this headline"></a></h3>
<p>First, we need to load the euca2ools module that contains the command
line interface to interact with OpenStack. Then we need to load our own
credentials that are typically in a novarc file. Finally you update and
register the image. Although, we are going to briefly explain these
steps here, this is part of the OpenStack tutorial that can be found
in&nbsp;<a class="reference external" href="https://portal.futuregrid.org/tutorials/openstack">https://portal.futuregrid.org/tutorials/openstack</a>.</p>
<div class="highlight-python"><pre>$ module load euca2ools
$ source ~/novarc</pre>
</div>
<p>Upload the image</p>
<div class="highlight-python"><pre>$ euca-bundle-image -i rawimage.img

Checking image
Encrypting image
Splitting image...
Part: rawimage.img.part.00
Part: rawimage.img.part.01
Part: rawimage.img.part.02
.....
Generating manifest /tmp/rawimage.img.manifest.xml</pre>
</div>
<p>At the end you get a manifest file that you use in the next step. You
also need to specify a bucket name (option -b). We can use our username
(jdiazz in my case), but it can be any other string.</p>
<div class="highlight-python"><pre>$ euca-upload-image -m /tmp/rawimage.img.manifest.xml -b jdiazz

Checking bucket: jdiaz
Uploading manifest file
Uploading part: rawimage.img.part.00
Uploading part: rawimage.img.part.01
Uploading part: rawimage.img.part.02
.....
Uploaded image as jdiazz/rawimage.img.manifest.xml</pre>
</div>
<p>Finally we register the image.</p>
<div class="highlight-python"><pre>$ euca-register jdiazz/rawimage.img.manifest.xml

IMAGE ami-00000058</pre>
</div>
<p>From this last command we get the ami-ID that identifies the image in
OpenStack (marked in yellow). You will need this to start instances.</p>
</div>
<div class="section" id="checking-status-image">
<h3>Checking Status Image<a class="headerlink" href="#checking-status-image" title="Permalink to this headline"></a></h3>
<p>You cannot run instances until your image is in available status. You
can check the status of your image with the euca-describe-images
command. This command can take some time to respond because the system
will be busy processing your image.</p>
<div class="highlight-python"><pre>$ euca-describe-images ami-00000058</pre>
</div>
</div>
<div class="section" id="test-image-in-openstack">
<h3>Test Image in OpenStack<a class="headerlink" href="#test-image-in-openstack" title="Permalink to this headline"></a></h3>
<p>For this step we recomend to go to the OpenStack tutorial where we
explain how to create a key-pair and run an instance with our image.
Please
see&nbsp;<a class="reference external" href="https://portal.futuregrid.org/using-openstack-futuregrid#key_management">https://portal.futuregrid.org/using-openstack-futuregrid#key_management</a></p>
<p>Running the instance can be something like this:</p>
<div class="highlight-python"><pre>$ euca-run-instances -k jdiaznova ami-00000058 -t m1.large</pre>
</div>
<p>where jdiaznova is the key name of my openstack key pairs. This key will
allow us to ssh into the image. Please refer to the OpenStack tutorial
for more information.</p>
</div>
<div class="section" id="id79">
<h3>Troubleshooting<a class="headerlink" href="#id79" title="Permalink to this headline"></a></h3>
<p>One problem of this way of using our images is that we cannot use
euca-get-console-output command to debug the boot process of the images.
Therefore it makes more complicated solving runtime problems. However,
if your image boots properly when doing the &#8220;Test your Image&#8221; section,
it should work also on OpenStack and the only problem could be wrong
configuration of network interface or SELinux enabled.</p>
</div>
<div class="section" id="notes">
<h3>Notes:<a class="headerlink" href="#notes" title="Permalink to this headline"></a></h3>
<p>This tutorial has been tested with Ubuntu 12 and CentOS 6 using
OpenStack Essex.</p>
</div>
</div>
<div class="section" id="virtual-appliances">
<h2>Virtual Appliances<a class="headerlink" href="#virtual-appliances" title="Permalink to this headline"></a></h2>
<p>Virtual appliances are virtual machine images encapsulating
pre-installed, pre-configured software that can be easily deployed on
cloud resources.&nbsp;Users of FutureGrid can use public appliance images
posted by other users, as well as contribute to the repository of
images.</p>
<p>This page provides a summary of community-provided virtual appliances
that are available for use on FutureGrid. If you have created a virtual
appliance and you would like to advertise its availability and features
with the community, feel free to edit this page and include information
about your appliance.</p>
<blockquote>
<div><strong>Creating Your Own Appliance:</strong></div></blockquote>
<hr class="docutils" />
<p>Any FutureGrid user with Eucalyptus or Nimbus accounts can create and
register an appliance. The approach is similar in both cases: you can
upload an existing image into FutureGrid (e.g. from another Eucalyptus
or Nimbus cloud, or an image you create on your own computer), or you
can customize an instance interactively in FutureGrid and save it in a
FutureGrid resource.</p>
<p>You can upload a &#8220;common&#8221; image so it becomes available to others in a
FutureGrid cloud resource - to do this, add the flag &#8211;common to the
cloud_client.sh command line when you transfer an image (ensure you are
using cloud client version 020 or above).&nbsp; You can also save a Nimbus
instance that you are using interactively (through ssh) as an image.
Please refer to the <a class="reference external" href="http://www.nimbusproject.org/docs/current/clouds/cloudquickstart.html">manual for
cloud_client.sh</a>&nbsp;and
the &nbsp;<a class="reference external" href="http://portal.futuregrid.org/tutorials/nimbus">Nimbus tutorial</a>
for instructions.</p>
<p>Please refer to <a class="reference external" href="https://portal.futuregrid.org/tutorials/ga7">FutureGrid tutorial
GA7</a>.</p>
<p>There are several appliances already available on FutureGrid. The table
below summarizes a list of appliances, where they are available, and
their image names. If you have an appliance that you would like to add
to this list, please add it to the table.</p>
<blockquote>
<div><em>Editing tips for the table:</em></div></blockquote>
<hr class="docutils" />
<ul class="simple">
<li><em>You can add rows to the table by right-clicking the last row of the
table and selecting Row-&gt;Insert Row After</em></li>
<li><em>If you make a mistake (e.g., deleting a row), you can undo it by
pressing CTRL-Z</em></li>
<li><em>You need to click on the Submit button so your changes take effect</em></li>
<li><em>You can `create a community
page &lt;https://portal.futuregrid.org/node/add/page-community&gt;`__
explaining your appliance and its usage in more depth, and can link
to it from this page</em></li>
</ul>
<table border="1" class="docutils">
<colgroup>
<col width="30%" />
<col width="30%" />
<col width="12%" />
<col width="18%" />
<col width="9%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Appliance name</strong></td>
<td><strong>Appliance description</strong></td>
<td><strong>FutureGrid system(s) available</strong></td>
<td><strong>Appliance ID</strong></td>
<td><dl class="first last docutils">
<dt><a href="#id80"><span class="problematic" id="id81">**</span></a>Installed</dt>
<dd>by (FutureGrid user ID)**</dd>
</dl>
</td>
</tr>
<tr class="row-even"><td><a class="reference external" href="https://portal.futuregrid.org/tutorials/ga9">Grid Appliance</a></td>
<td>Self-configures Condor/MPI/Hadoop virtual clusters for training/education</td>
<td>alamo, india</td>
<td>grid-appliance-2.05.03.gz (alamo), emi-E4ED1880 (india)</td>
<td>panoat</td>
</tr>
<tr class="row-odd"><td><dl class="first last docutils">
<dt><a href="#id82"><span class="problematic" id="id83">`</span></a>OpenStack</dt>
<dd>Appliance &lt;<a class="reference external" href="https://portal.futuregrid.org/tutorials/os1">https://portal.futuregrid.org/tutorials/os1</a>&gt;`__</dd>
</dl>
</td>
<td>Deploys a single-node OpenStack compute virtual cloud</td>
<td>alamo</td>
<td>openstack-ubuntu-10.10-amd64.img</td>
<td>menghan</td>
</tr>
<tr class="row-even"><td><a class="reference external" href="https://portal.futuregrid.org/contrib/simple-vine-tutorial">ViNe Appliance</a></td>
<td>Deploys the ViNe virtual network overlay to connect private-address VMs in sierra and foxtrot</td>
<td>sierra, foxtrot</td>
<td>centos-5.5-x64-vine.gz</td>
<td>menghan</td>
</tr>
<tr class="row-odd"><td><a class="reference external" href="https://portal.futuregrid.org/contrib/fg-twister-appliance-tutorial">Twister Appliance</a></td>
<td>Deploys a virtual private cluster running the Twister iterative MapReduce system</td>
<td>india</td>
<td>emi-F0B8194D</td>
<td>jemitche</td>
</tr>
<tr class="row-even"><td>(Add your appliance here)</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
<p>Average: Select ratingPoorOkayGoodGreatAwesome</p>
<p>No votes yet</p>
</div>
<div class="section" id="development-projects">
<h2>Development Projects<a class="headerlink" href="#development-projects" title="Permalink to this headline"></a></h2>
<p>FutureGrid&nbsp;provides a number of software development projects. Much
(but not all) of our work we have just begun to move to github.</p>
<blockquote>
<div>The documentation for these projects can be found</div></blockquote>
<p>at&nbsp;<a class="reference external" href="http://futuregrid.github.com/doc/">http://futuregrid.github.com/doc/</a>
.</p>
<p>&nbsp;As a part of FutureGrid, many other projects perform development as
well. These include Inca, Nimbus, Pegasus, PAPI, Vampir, as well as a
project called hostlists. These projects either maintain their own
repositories or use the FutureGrid sourceforge&nbsp;repository.</p>
<p>&nbsp;Actvities on github&nbsp;include:</p>
<p>Rain</p>
<blockquote>
<div><ul class="simple">
<li>A project to do bare metal and VM-based dynamic provisioning</li>
<li>Documentation:&nbsp;<a class="reference external" href="http://futuregrid.github.com/rain">http://futuregrid.github.com/rain</a></li>
<li>Source:&nbsp;<a class="reference external" href="https://github.com/futuregrid/rain">https://github.com/futuregrid/rain</a></li>
</ul>
</div></blockquote>
<p>Cloud Shift</p>
<blockquote>
<div><ul class="simple">
<li>A project to move resources between different cloud and HPC
services</li>
<li>Documentation: todo</li>
<li>Source: todo</li>
<li>Issues: todo</li>
</ul>
</div></blockquote>
<p>Cloud Metric</p>
<blockquote>
<div><ul class="simple">
<li>A project to measure and display metric information about usage
and utilization of your cloud</li>
<li>Documentation:<a class="reference external" href="https://portal.futuregrid.org/doc/metric/index.html">https://portal.futuregrid.org/doc/metric/index.html</a></li>
<li>Source:&nbsp;<a class="reference external" href="https://github.com/futuregrid/futuregrid-cloud-metrics">https://github.com/futuregrid/futuregrid-cloud-metrics</a></li>
<li>Issues:&nbsp;<a class="reference external" href="https://github.com/futuregrid/futuregrid-cloud-metrics/issues">https://github.com/futuregrid/futuregrid-cloud-metrics/issues</a></li>
</ul>
</div></blockquote>
<p>Virtual Cluster</p>
<blockquote>
<div><ul class="simple">
<li>A project to create a SLURM-based cluster in your cloud and run
MPI jobs on it</li>
<li>Documentation:&nbsp;<a class="reference external" href="http://futuregrid.github.com/virtual-cluster">http://futuregrid.github.com/virtual-cluster</a></li>
<li>Source:&nbsp;<a class="reference external" href="https://github.com/futuregrid/virtual-cluster">https://github.com/futuregrid/virtual-cluster</a></li>
<li>Issues:&nbsp;<a class="reference external" href="https://github.com/futuregrid/virtual-cluster/issues">https://github.com/futuregrid/virtual-cluster/issues</a></li>
</ul>
</div></blockquote>
<p>Authentication</p>
<blockquote>
<div><ul class="simple">
<li>A project to unify authentication between Eucalyptus, OpenStack,
and Nimbus</li>
<li>Documentation: todo</li>
<li>Source: todo</li>
<li>Issues: todo</li>
</ul>
</div></blockquote>
<p>Mediawiki Jira Issues</p>
<blockquote>
<div><ul class="simple">
<li>A project that can be used to automatically create reports based
on comments submitted to jira, returning jira issues in
mediawiki, and executing arbitrary queries from mediawiki to jira
that are rendered in mediawiki</li>
<li>Documentation: todo</li>
<li>Source: todo</li>
<li>Issues: todo</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="appendix">
<h2>Appendix<a class="headerlink" href="#appendix" title="Permalink to this headline"></a></h2>
</div>
<div class="section" id="upgrading-nimbus-on-fg-clusters">
<h2>Upgrading Nimbus On FG clusters<a class="headerlink" href="#upgrading-nimbus-on-fg-clusters" title="Permalink to this headline"></a></h2>
<div class="section" id="disclaimer">
<h3>Disclaimer<a class="headerlink" href="#disclaimer" title="Permalink to this headline"></a></h3>
<p>This document provides guidelines on how to upgrade Nimbus on
FutureGrid sites.&nbsp; It is not a definitive guide to upgrading Nimbus
anywhere.&nbsp; An admin&#8217;s own skills and intuition should be in full force
while following this guide.&nbsp; This page should be considered little more
than the notes from one effective upgrade from 2.7 to 2.8.</p>
</div>
<div class="section" id="upgrading-workspace-service-and-cumulus">
<h3>Upgrading Workspace Service and Cumulus<a class="headerlink" href="#upgrading-workspace-service-and-cumulus" title="Permalink to this headline"></a></h3>
<p>First create a source directory and download the latest Nimbus release
from
<a class="reference external" href="http://www.nimbusproject.org/downloads/">http://www.nimbusproject.org/downloads/</a>,
and untar the distributions:</p>
<div class="highlight-python"><pre>mkdir ~/src
wget http://www.nimbusproject.org/downloads/nimbus-iaas-2.8-src.tar.gz
tar -xzf nimbus-iaas-2.8-src.tar.gz
wget http://www.nimbusproject.org/downloads/nimbus-iaas-controls-2.8.tar.gz
tar -zxf nimbus-iaas-controls-2.8.tar.gz</pre>
</div>
<p>For convenience set the following variables:</p>
<div class="highlight-python"><pre>export OLD_NIMBUS=&lt;path to old nimbus install&gt;
export NEW_NIMBUS=&lt;path to new nimbus install&gt;</pre>
</div>
<p>Make sure that the old install is not running:</p>
<div class="highlight-python"><pre>$OLD_NIMBUS/bin/nimbusctl stop</pre>
</div>
<p>Install the new Nimbus. If you are on an old distribution such as CentOS
5, you might need to set the OLD_OPENSSL_VERSION environment variable.
If you run into Java OOM issues, export the following variable: export
ANT_OPTS=-Xmx1024M.&nbsp;When asked any questions just hit enter:</p>
<div class="highlight-python"><pre>./install $NEW_NIMBUS</pre>
</div>
<p>Run the following commands to transfer over the env.&nbsp;<strong>*NOTE*</strong> please
check that you are ok with these commands first, do not blindly copy and
paste them.&nbsp;<strong>*These commands DELETE things!*</strong></p>
<div class="highlight-python"><pre>rm -rf $NEW_NIMBUS/var/ca
rm $NEW_NIMBUS/var/hostcert.pem
rm $NEW_NIMBUS/var/hostkey.pem
rm $NEW_NIMBUS/var/keystore.jks

cp $OLD_NIMBUS/nimbus-setup.conf $NEW_NIMBUS/nimbus-setup.conf

export PREVIOUS_NIMBUS_HOME_VALIDATED=$OLD_NIMBUS
$NEW_NIMBUS/bin/nimbus-configure --conf $NEW_NIMBUS/nimbus-setup.conf  --import-cumulusdb
$NEW_NIMBUS/bin/nimbus-configure --conf $NEW_NIMBUS/nimbus-setup.conf --import-prev

cp $OLD_NIMBUS/services/etc/nimbus/workspace-service/metadata.conf $NEW_NIMBUS/services/etc/nimbus/workspace-service/metadata.conf</pre>
</div>
<p>Import SSH keys from the Elastic interface:</p>
<div class="highlight-python"><pre>sqlite3 $OLD_NIMBUS/services/var/nimbus/elastic.db
sqlite&gt; .output /N/u/nimbus/ssh_keypairs
sqlite&gt; .dump ssh_keypairs
sqlite&gt; .quit

sqlite3 $NEW_NIMBUS/services/var/nimbus/elastic.db
sqlite&gt; .read /N/u/nimbus/ssh_keypairs
sqlite&gt; .quit</pre>
</div>
<p>Manually check the remaining differences in the configuration files.
The differences will be very site dependent so put on your thinking
cap.&nbsp; The following script will diff out all the conf files.&nbsp; Go through
the output and make sure you merge over all the needed changes for your
site</p>
<div class="highlight-python"><pre>cd $OLD_NIMBUS
confs=`find . -name '*.conf'`
for c in $confs
do
    diff -q -u $OLD_NIMBUS/$c $NEW_NIMBUS/$c</pre>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="n">done</span>
</pre></div>
</div>
<p>Now verify that services/etc/nimbus-context-broker/jndi-config.xml has
the right caCertPath and caKeyPath (in almost every case it will not
have the right values).
Also verify that&nbsp;services/etc/globus_wsrf_core/server-config.wsdd
has the right&nbsp;logicalHost.
Also verify the customizations
in&nbsp;services/etc/nimbus/workspace-service/other/authz-callout-ACTIVE.xml
are still enabled (for example cp propagation).</p>
<p>Also check $NEW_NIMBUS/cumulus/etc/cumulus.ini for the backend
data_dir if you are using type &#8216;posix&#8217; (which you likely are).
And check
$NEW_NIMBUS/services/etc/nimbus/workspace-service/cumulus.conf for the
cumulus.authz.db and cumulus.repo.dir settings.</p>
<p>Now you must load up the new Nimbus install with the backends from
the old install.&nbsp; The following python script can help with that task.
Simply give it two arguments: $OLD_NIMBUS $NEW_NIMBUS</p>
<p><a class="reference external" href="https://raw.github.com/nimbusproject/nimbus/master/docs/src/admin/reload.py">https://raw.github.com/nimbusproject/nimbus/master/docs/src/admin/reload.py</a></p>
</div>
<div class="section" id="upgrading-workspace-control">
<h3>Upgrading Workspace Control<a class="headerlink" href="#upgrading-workspace-control" title="Permalink to this headline"></a></h3>
<p>Set up the workspace control nodes by doing the following:</p>
<ol class="arabic simple">
<li>Move the old install from /opt/nimbus to /opt/nimbus2.7</li>
<li>Create the directory /opt/nimbus2.8 and have it owned by the nimbus
user and group</li>
<li>Create a symlink from /opt/nimbus to /opt/nimbus2.8</li>
</ol>
<p>Now copy the old working workspace control code from a working VMM
node to the source directory:</p>
<div class="highlight-python"><pre>scp -r working_node:/opt/nimbus2.7 ~/src/</pre>
</div>
<p>Just like in the above step, run the following diff script to find
configuration differences, give it the full path to nimbus2.7 as the
first argument and the full path to the new distribution at the second:</p>
<div class="highlight-python"><pre>cd $1
confs=`find . -name '*.conf'`
for c in $confs
do
    diff -u $1/$c $2/$c</pre>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="n">done</span>
</pre></div>
</div>
<p>Merge all of the differences.&nbsp; Check out the following files and merge
the differences:</p>
<p>libexec/workspace-control/mount-alter.sh
lantorrent/etc/*
etc/workspace-control/libvirt_template.xml</p>
<p>Make sure that everything in nimbus2.7/var/workspace-control/kernels
is also in
nimbus-iaas-controls-2.8/workspace-control/var/workspace-control/kernels</p>
<p>scp everything under nimbus-iaas-controls-2.8/workspace-control to
all VMM nodes:/opt/nimbus/</p>
</div>
</div>
<div class="section" id="faq">
<h2>FAQ<a class="headerlink" href="#faq" title="Permalink to this headline"></a></h2>
<p><a class="reference external" href="https://portal.futuregrid.org/faq">https://portal.futuregrid.org/faq</a></p>
</div>
</div>


</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
    </p>
  </div>
</footer>
  </body>
</html>